
training huabei40x20
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x20  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x20 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
9s - loss: 1821.4221 - val_loss: 9246.3262
Epoch 00000: val_loss improved from inf to 9246.32616, saving model to huabei40x20_weights.hdf5
         huabei40x20  1676.5      0.62  0.32  0.48      0.60  0.19  0.53      0.61  0.13  0.56
forget = 0.955541
delta_x = 4.239
delta_h = 2.02318
delta mean, abs_mean, abs_mean+, abs_mean-: 0.595902 4.239 3.48425 5.94933
U_c = [[-0.08598424]] U_f = [[-0.01141071]] b_f = [ 1.01084268]
         huabei40x20  9246.3      0.85  0.22  0.68      0.83  0.14  0.73      0.83  0.09  0.76
forget = 0.960687
delta_x = 4.92593
delta_h = 3.03356
delta mean, abs_mean, abs_mean+, abs_mean-: 0.672737 4.92593 3.90014 7.5345
U_c = [[-0.08598424]] U_f = [[-0.01141071]] b_f = [ 1.01084268]
Epoch 2/300
9s - loss: 1655.1847 - val_loss: 8878.5087
Epoch 00001: val_loss improved from 9246.32616 to 8878.50868, saving model to huabei40x20_weights.hdf5
         huabei40x20  1638.1      0.68  0.34  0.50      0.66  0.21  0.56      0.67  0.14  0.60
forget = 0.959604
delta_x = 4.68348
delta_h = 2.17788
delta mean, abs_mean, abs_mean+, abs_mean-: 0.925612 4.68348 3.81595 7.08906
U_c = [[-0.08407843]] U_f = [[-0.01280636]] b_f = [ 0.9577018]
         huabei40x20  8878.5      0.87  0.23  0.69      0.85  0.15  0.74      0.85  0.10  0.77
forget = 0.964239
delta_x = 5.44207
delta_h = 3.34102
delta mean, abs_mean, abs_mean+, abs_mean-: 1.21826 5.44207 4.31146 9.27895
U_c = [[-0.08407843]] U_f = [[-0.01280636]] b_f = [ 0.9577018]
Epoch 3/300
9s - loss: 1624.5857 - val_loss: 9045.9847
Epoch 00002: val_loss did not improve
Epoch 4/300
9s - loss: 1596.5469 - val_loss: 9100.9001
Epoch 00003: val_loss did not improve
Epoch 5/300
9s - loss: 1564.6538 - val_loss: 8919.9227
Epoch 00004: val_loss did not improve
Epoch 6/300
9s - loss: 1524.4288 - val_loss: 9084.2980
Epoch 00005: val_loss did not improve
Epoch 7/300
9s - loss: 1476.6705 - val_loss: 9606.2356
Epoch 00006: val_loss did not improve
Epoch 8/300
9s - loss: 1425.4217 - val_loss: 9897.2821
Epoch 00007: val_loss did not improve
Epoch 9/300
9s - loss: 1381.6029 - val_loss: 10028.2438
Epoch 00008: val_loss did not improve
Epoch 10/300
9s - loss: 1349.9449 - val_loss: 10372.4153
Epoch 00009: val_loss did not improve
Epoch 11/300
9s - loss: 1320.8214 - val_loss: 10459.2747
Epoch 00010: val_loss did not improve
Epoch 12/300
9s - loss: 1297.1278 - val_loss: 10225.9217
Epoch 00011: val_loss did not improve
Epoch 13/300
9s - loss: 1276.8192 - val_loss: 10986.6199
Epoch 00012: val_loss did not improve
Epoch 14/300
9s - loss: 1258.6956 - val_loss: 10418.7626
Epoch 00013: val_loss did not improve
Epoch 15/300
9s - loss: 1241.2253 - val_loss: 11065.8776
Epoch 00014: val_loss did not improve
Epoch 16/300
9s - loss: 1226.5647 - val_loss: 10764.6452
Epoch 00015: val_loss did not improve
Epoch 17/300
9s - loss: 1213.3522 - val_loss: 10769.2042
Epoch 00016: val_loss did not improve
Epoch 18/300
9s - loss: 1200.7631 - val_loss: 10610.3329
Epoch 00017: val_loss did not improve
Epoch 19/300
9s - loss: 1189.9584 - val_loss: 10624.8473
Epoch 00018: val_loss did not improve
Epoch 20/300
9s - loss: 1178.6200 - val_loss: 10697.5459
Epoch 00019: val_loss did not improve
Epoch 21/300
9s - loss: 1168.0583 - val_loss: 10913.4576
Epoch 00020: val_loss did not improve
Epoch 22/300
9s - loss: 1157.3272 - val_loss: 10470.8158
Epoch 00021: val_loss did not improve
Epoch 23/300
9s - loss: 1146.0333 - val_loss: 10738.9826
Epoch 00022: val_loss did not improve

training huabei40x21
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x21  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x21 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
10s - loss: 1820.6863 - val_loss: 8792.5535
Epoch 00000: val_loss improved from inf to 8792.55355, saving model to huabei40x21_weights.hdf5
         huabei40x21  1674.8      0.68  0.35  0.50      0.66  0.22  0.56      0.67  0.15  0.60
forget = 0.960642
delta_x = 4.34834
delta_h = 2.15596
delta mean, abs_mean, abs_mean+, abs_mean-: 1.0446 4.34834 3.74355 5.90579
U_c = [[-0.08534577]] U_f = [[-0.00988846]] b_f = [ 1.01874816]
         huabei40x21  8792.5      0.88  0.24  0.68      0.86  0.16  0.73      0.86  0.11  0.78
forget = 0.967287
delta_x = 5.22425
delta_h = 3.41206
delta mean, abs_mean, abs_mean+, abs_mean-: 1.61314 5.22425 4.4717 7.66747
U_c = [[-0.08534577]] U_f = [[-0.00988846]] b_f = [ 1.01874816]
Epoch 2/300
10s - loss: 1652.6864 - val_loss: 9061.2294
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1620.3086 - val_loss: 8824.0357
Epoch 00002: val_loss did not improve
Epoch 4/300
10s - loss: 1588.9445 - val_loss: 8903.2077
Epoch 00003: val_loss did not improve
Epoch 5/300
10s - loss: 1557.8828 - val_loss: 8939.4330
Epoch 00004: val_loss did not improve
Epoch 6/300
10s - loss: 1521.2043 - val_loss: 9122.5363
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1484.0783 - val_loss: 9041.2975
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1449.0588 - val_loss: 9662.1906
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1411.3269 - val_loss: 10515.9523
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1381.7729 - val_loss: 10143.5248
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1357.8217 - val_loss: 10655.3980
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1329.8459 - val_loss: 10668.4435
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1303.8094 - val_loss: 11152.6361
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1274.9402 - val_loss: 11070.8192
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1246.9023 - val_loss: 10601.1366
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1225.0713 - val_loss: 10928.9055
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1208.9382 - val_loss: 10932.1530
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1196.7808 - val_loss: 11463.6106
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1187.5025 - val_loss: 10798.5970
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1178.7712 - val_loss: 11114.3957
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1171.4591 - val_loss: 11358.3162
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1163.5279 - val_loss: 11688.9266
Epoch 00021: val_loss did not improve

training huabei40x22
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x22  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x22 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
10s - loss: 1818.6776 - val_loss: 8784.9882
Epoch 00000: val_loss improved from inf to 8784.98819, saving model to huabei40x22_weights.hdf5
         huabei40x22  1665.0      0.67  0.35  0.49      0.65  0.21  0.56      0.67  0.15  0.60
forget = 0.956982
delta_x = 4.56148
delta_h = 2.08402
delta mean, abs_mean, abs_mean+, abs_mean-: 0.897937 4.56148 3.82667 6.38997
U_c = [[-0.08144662]] U_f = [[-0.01230952]] b_f = [ 0.99972874]
         huabei40x22  8785.0      0.87  0.23  0.68      0.85  0.15  0.73      0.86  0.10  0.77
forget = 0.963534
delta_x = 5.38208
delta_h = 3.20787
delta mean, abs_mean, abs_mean+, abs_mean-: 1.23509 5.38208 4.37151 8.52775
U_c = [[-0.08144662]] U_f = [[-0.01230952]] b_f = [ 0.99972874]
Epoch 2/300
10s - loss: 1649.1714 - val_loss: 8827.3720
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1620.0663 - val_loss: 8916.7364
Epoch 00002: val_loss did not improve
Epoch 4/300
10s - loss: 1595.4828 - val_loss: 9173.3965
Epoch 00003: val_loss did not improve
Epoch 5/300
10s - loss: 1572.1205 - val_loss: 8928.6384
Epoch 00004: val_loss did not improve
Epoch 6/300
10s - loss: 1549.2668 - val_loss: 8863.3237
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1518.6039 - val_loss: 8701.9068
Epoch 00006: val_loss improved from 8784.98819 to 8701.90675, saving model to huabei40x22_weights.hdf5
         huabei40x22  1501.2      0.73  0.33  0.54      0.71  0.21  0.60      0.73  0.15  0.64
forget = 0.957404
delta_x = 5.39078
delta_h = 1.69639
delta mean, abs_mean, abs_mean+, abs_mean-: 0.776995 5.39078 4.36189 7.87352
U_c = [[-0.05972482]] U_f = [[-0.01612431]] b_f = [ 0.61541533]
         huabei40x22  8701.9      0.81  0.19  0.67      0.79  0.12  0.71      0.80  0.09  0.74
forget = 0.960711
delta_x = 5.70764
delta_h = 2.58591
delta mean, abs_mean, abs_mean+, abs_mean-: 0.377635 5.70764 4.01708 10.9864
U_c = [[-0.05972482]] U_f = [[-0.01612431]] b_f = [ 0.61541533]
Epoch 8/300
10s - loss: 1476.9213 - val_loss: 9077.2674
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1432.0823 - val_loss: 9782.3730
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1392.1980 - val_loss: 10053.3283
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1358.8479 - val_loss: 10253.3191
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1330.0253 - val_loss: 10859.6256
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1304.1383 - val_loss: 10511.2479
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1278.1238 - val_loss: 11500.9684
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1253.2340 - val_loss: 11139.4471
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1228.3353 - val_loss: 11324.5116
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1209.3812 - val_loss: 11691.0726
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1194.3558 - val_loss: 11252.6161
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1181.1582 - val_loss: 11931.4848
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1169.7816 - val_loss: 11338.5723
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1159.3748 - val_loss: 11411.6366
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1148.2275 - val_loss: 11069.0619
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1139.1645 - val_loss: 11342.7245
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1128.0586 - val_loss: 11767.7795
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1119.4124 - val_loss: 11796.6505
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1110.3195 - val_loss: 11316.0460
Epoch 00025: val_loss did not improve
Epoch 27/300
10s - loss: 1101.7127 - val_loss: 11542.1180
Epoch 00026: val_loss did not improve
Epoch 28/300
10s - loss: 1093.4847 - val_loss: 12231.4710
Epoch 00027: val_loss did not improve

training huabei40x23
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x23  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x23 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
10s - loss: 1821.5286 - val_loss: 9025.2634
Epoch 00000: val_loss improved from inf to 9025.26344, saving model to huabei40x23_weights.hdf5
         huabei40x23  1669.3      0.63  0.33  0.48      0.61  0.20  0.53      0.63  0.13  0.57
forget = 0.956118
delta_x = 4.28942
delta_h = 2.03207
delta mean, abs_mean, abs_mean+, abs_mean-: 0.668457 4.28942 3.54729 6.01141
U_c = [[-0.08458102]] U_f = [[-0.01183053]] b_f = [ 1.00429094]
         huabei40x23  9025.3      0.85  0.22  0.68      0.84  0.14  0.73      0.84  0.10  0.77
forget = 0.961765
delta_x = 5.09184
delta_h = 3.1267
delta mean, abs_mean, abs_mean+, abs_mean-: 0.879636 5.09184 4.06613 7.92644
U_c = [[-0.08458102]] U_f = [[-0.01183053]] b_f = [ 1.00429094]
Epoch 2/300
10s - loss: 1651.2519 - val_loss: 9065.8970
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1618.9165 - val_loss: 8937.8390
Epoch 00002: val_loss improved from 9025.26344 to 8937.83900, saving model to huabei40x23_weights.hdf5
         huabei40x23  1605.4      0.69  0.34  0.51      0.67  0.21  0.57      0.69  0.14  0.62
forget = 0.959628
delta_x = 4.81769
delta_h = 1.9782
delta mean, abs_mean, abs_mean+, abs_mean-: 0.855107 4.81769 3.86627 7.43799
U_c = [[-0.07515018]] U_f = [[-0.01380887]] b_f = [ 0.87996942]
         huabei40x23  8937.8      0.86  0.23  0.68      0.84  0.15  0.73      0.84  0.10  0.76
forget = 0.967403
delta_x = 5.19744
delta_h = 2.90535
delta mean, abs_mean, abs_mean+, abs_mean-: 1.00172 5.19744 4.01173 9.22663
U_c = [[-0.07515018]] U_f = [[-0.01380887]] b_f = [ 0.87996942]
Epoch 4/300
10s - loss: 1591.3204 - val_loss: 8794.5168
Epoch 00003: val_loss improved from 8937.83900 to 8794.51680, saving model to huabei40x23_weights.hdf5
         huabei40x23  1585.9      0.65  0.31  0.50      0.63  0.19  0.55      0.64  0.13  0.59
forget = 0.957415
delta_x = 4.89571
delta_h = 1.82099
delta mean, abs_mean, abs_mean+, abs_mean-: 0.586186 4.89571 3.84656 7.49665
U_c = [[-0.07579555]] U_f = [[-0.01554252]] b_f = [ 0.82087284]
         huabei40x23  8794.5      0.83  0.20  0.69      0.82  0.13  0.73      0.83  0.09  0.76
forget = 0.962572
delta_x = 5.4532
delta_h = 2.92256
delta mean, abs_mean, abs_mean+, abs_mean-: 0.511341 5.4532 3.95554 10.0423
U_c = [[-0.07579555]] U_f = [[-0.01554252]] b_f = [ 0.82087284]
Epoch 5/300
10s - loss: 1563.3814 - val_loss: 8656.6215
Epoch 00004: val_loss improved from 8794.51680 to 8656.62152, saving model to huabei40x23_weights.hdf5
         huabei40x23  1541.5      0.71  0.33  0.53      0.69  0.21  0.58      0.71  0.15  0.63
forget = 0.958944
delta_x = 5.18697
delta_h = 1.76922
delta mean, abs_mean, abs_mean+, abs_mean-: 0.80446 5.18697 4.13999 7.92798
U_c = [[-0.06741975]] U_f = [[-0.01503043]] b_f = [ 0.77249187]
         huabei40x23  8656.6      0.84  0.21  0.68      0.83  0.14  0.72      0.83  0.10  0.76
forget = 0.965663
delta_x = 5.47042
delta_h = 2.70524
delta mean, abs_mean, abs_mean+, abs_mean-: 0.706329 5.47042 4.0583 9.96678
U_c = [[-0.06741975]] U_f = [[-0.01503043]] b_f = [ 0.77249187]
Epoch 6/300
10s - loss: 1516.6207 - val_loss: 8569.9959
Epoch 00005: val_loss improved from 8656.62152 to 8569.99589, saving model to huabei40x23_weights.hdf5
         huabei40x23  1488.7      0.73  0.32  0.54      0.71  0.20  0.60      0.73  0.14  0.65
forget = 0.955463
delta_x = 5.55008
delta_h = 1.67609
delta mean, abs_mean, abs_mean+, abs_mean-: 0.718848 5.55008 4.31914 8.80695
U_c = [[-0.06115561]] U_f = [[-0.01663407]] b_f = [ 0.71781307]
         huabei40x23  8570.0      0.83  0.21  0.68      0.82  0.14  0.72      0.83  0.09  0.76
forget = 0.964361
delta_x = 5.44651
delta_h = 2.51341
delta mean, abs_mean, abs_mean+, abs_mean-: 0.469225 5.44651 3.90569 10.255
U_c = [[-0.06115561]] U_f = [[-0.01663407]] b_f = [ 0.71781307]
Epoch 7/300
10s - loss: 1469.0329 - val_loss: 9088.6131
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1424.9915 - val_loss: 9635.3043
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1387.0885 - val_loss: 9974.7216
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1355.6202 - val_loss: 9667.1697
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1328.0306 - val_loss: 10113.0225
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1305.6214 - val_loss: 10108.5876
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1287.8609 - val_loss: 10165.5901
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1270.1562 - val_loss: 9846.7472
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1255.9879 - val_loss: 9839.2435
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1242.5634 - val_loss: 10148.2807
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1229.3818 - val_loss: 10106.4469
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1217.8379 - val_loss: 10099.2046
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1203.8129 - val_loss: 10544.5816
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1191.6095 - val_loss: 10171.5378
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1180.3907 - val_loss: 10780.2616
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1169.6026 - val_loss: 10487.5136
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1160.5843 - val_loss: 10448.4032
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1150.0457 - val_loss: 10880.6699
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1142.0799 - val_loss: 10916.4789
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1134.3030 - val_loss: 11080.4219
Epoch 00025: val_loss did not improve
Epoch 27/300
10s - loss: 1125.7086 - val_loss: 11043.0400
Epoch 00026: val_loss did not improve

training huabei40x24
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x24  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x24 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
10s - loss: 1820.9274 - val_loss: 9082.0410
Epoch 00000: val_loss improved from inf to 9082.04096, saving model to huabei40x24_weights.hdf5
         huabei40x24  1668.8      0.67  0.35  0.49      0.65  0.21  0.55      0.66  0.14  0.60
forget = 0.959516
delta_x = 4.28061
delta_h = 2.03978
delta mean, abs_mean, abs_mean+, abs_mean-: 0.938678 4.28061 3.65879 5.82733
U_c = [[-0.08155234]] U_f = [[-0.00891417]] b_f = [ 1.008641]
         huabei40x24  9082.0      0.86  0.24  0.68      0.85  0.15  0.73      0.85  0.11  0.77
forget = 0.965443
delta_x = 4.90742
delta_h = 3.03518
delta mean, abs_mean, abs_mean+, abs_mean-: 1.15938 4.90742 4.10472 7.18023
U_c = [[-0.08155234]] U_f = [[-0.00891417]] b_f = [ 1.008641]
Epoch 2/300
10s - loss: 1651.6738 - val_loss: 9163.9137
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1623.6615 - val_loss: 9073.9242
Epoch 00002: val_loss improved from 9082.04096 to 9073.92424, saving model to huabei40x24_weights.hdf5
         huabei40x24  1611.8      0.65  0.32  0.50      0.63  0.19  0.55      0.64  0.13  0.59
forget = 0.957652
delta_x = 4.60901
delta_h = 2.03499
delta mean, abs_mean, abs_mean+, abs_mean-: 0.655154 4.60901 3.65631 7.05727
U_c = [[-0.08275717]] U_f = [[-0.01248604]] b_f = [ 0.88455677]
         huabei40x24  9073.9      0.84  0.21  0.68      0.83  0.13  0.73      0.83  0.09  0.76
forget = 0.961317
delta_x = 5.22633
delta_h = 3.12101
delta mean, abs_mean, abs_mean+, abs_mean-: 0.652039 5.22633 3.91823 9.15337
U_c = [[-0.08275717]] U_f = [[-0.01248604]] b_f = [ 0.88455677]
Epoch 4/300
10s - loss: 1601.2705 - val_loss: 8930.5581
Epoch 00003: val_loss improved from 9073.92424 to 8930.55806, saving model to huabei40x24_weights.hdf5
         huabei40x24  1590.7      0.70  0.34  0.51      0.68  0.21  0.58      0.70  0.15  0.62
forget = 0.959967
delta_x = 4.82144
delta_h = 1.92694
delta mean, abs_mean, abs_mean+, abs_mean-: 0.834839 4.82144 3.88182 7.34345
U_c = [[-0.07232808]] U_f = [[-0.01248948]] b_f = [ 0.84114796]
         huabei40x24  8930.5      0.84  0.21  0.68      0.83  0.14  0.73      0.83  0.09  0.76
forget = 0.963795
delta_x = 5.22432
delta_h = 2.87231
delta mean, abs_mean, abs_mean+, abs_mean-: 0.709102 5.22432 3.91192 9.34348
U_c = [[-0.07232808]] U_f = [[-0.01248948]] b_f = [ 0.84114796]
Epoch 5/300
10s - loss: 1580.9567 - val_loss: 8982.7461
Epoch 00004: val_loss did not improve
Epoch 6/300
10s - loss: 1562.0076 - val_loss: 8960.4492
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1540.8434 - val_loss: 9017.2290
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1512.7334 - val_loss: 8893.1476
Epoch 00007: val_loss improved from 8930.55806 to 8893.14758, saving model to huabei40x24_weights.hdf5
         huabei40x24  1493.5      0.71  0.32  0.53      0.70  0.20  0.59      0.71  0.14  0.64
forget = 0.955515
delta_x = 5.48767
delta_h = 1.72939
delta mean, abs_mean, abs_mean+, abs_mean-: 0.686512 5.48767 4.31556 8.43309
U_c = [[-0.06285198]] U_f = [[-0.01886629]] b_f = [ 0.66508567]
         huabei40x24  8893.1      0.80  0.20  0.67      0.79  0.12  0.71      0.80  0.08  0.74
forget = 0.962206
delta_x = 5.36932
delta_h = 2.54404
delta mean, abs_mean, abs_mean+, abs_mean-: 0.281125 5.36932 3.66676 11.0852
U_c = [[-0.06285198]] U_f = [[-0.01886629]] b_f = [ 0.66508567]
Epoch 9/300
10s - loss: 1483.7884 - val_loss: 8882.3574
Epoch 00008: val_loss improved from 8893.14758 to 8882.35744, saving model to huabei40x24_weights.hdf5
         huabei40x24  1470.3      0.72  0.31  0.54      0.70  0.19  0.60      0.72  0.14  0.64
forget = 0.954898
delta_x = 5.51575
delta_h = 1.5726
delta mean, abs_mean, abs_mean+, abs_mean-: 0.651039 5.51575 4.54619 7.55943
U_c = [[-0.05598969]] U_f = [[-0.01883998]] b_f = [ 0.62508595]
         huabei40x24  8882.4      0.80  0.20  0.66      0.78  0.12  0.70      0.79  0.08  0.73
forget = 0.962804
delta_x = 5.27815
delta_h = 2.35532
delta mean, abs_mean, abs_mean+, abs_mean-: 0.280403 5.27815 3.66769 10.3163
U_c = [[-0.05598969]] U_f = [[-0.01883998]] b_f = [ 0.62508595]
Epoch 10/300
10s - loss: 1459.7262 - val_loss: 9402.8032
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1429.1913 - val_loss: 9713.1100
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1394.8757 - val_loss: 10026.4458
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1364.8655 - val_loss: 10307.2535
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1338.6108 - val_loss: 10033.6856
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1315.6555 - val_loss: 10849.7719
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1293.7016 - val_loss: 11250.0941
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1266.3735 - val_loss: 10823.4482
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1243.0936 - val_loss: 10264.3038
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1224.2652 - val_loss: 9994.6720
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1208.8230 - val_loss: 11065.2908
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1196.2078 - val_loss: 11020.6186
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1183.5395 - val_loss: 11569.9048
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1172.2497 - val_loss: 10659.4210
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1163.3245 - val_loss: 11077.8023
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1155.4237 - val_loss: 10417.4751
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1147.9486 - val_loss: 10627.3227
Epoch 00025: val_loss did not improve
Epoch 27/300
10s - loss: 1140.5243 - val_loss: 10902.3888
Epoch 00026: val_loss did not improve
Epoch 28/300
10s - loss: 1133.8626 - val_loss: 11156.2840
Epoch 00027: val_loss did not improve
Epoch 29/300
10s - loss: 1128.0820 - val_loss: 11005.3769
Epoch 00028: val_loss did not improve
Epoch 30/300
10s - loss: 1120.1953 - val_loss: 10837.5497
Epoch 00029: val_loss did not improve

training huabei40x25
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x25  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x25 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
9s - loss: 1818.9274 - val_loss: 9121.0922
Epoch 00000: val_loss improved from inf to 9121.09219, saving model to huabei40x25_weights.hdf5
         huabei40x25  1665.1      0.66  0.34  0.49      0.64  0.21  0.55      0.65  0.14  0.59
forget = 0.958782
delta_x = 4.25201
delta_h = 2.0295
delta mean, abs_mean, abs_mean+, abs_mean-: 0.854361 4.25201 3.62534 5.74434
U_c = [[-0.08223873]] U_f = [[-0.00933632]] b_f = [ 0.99561626]
         huabei40x25  9121.1      0.86  0.23  0.68      0.84  0.15  0.73      0.85  0.10  0.77
forget = 0.964384
delta_x = 4.85997
delta_h = 3.01683
delta mean, abs_mean, abs_mean+, abs_mean-: 1.00536 4.85997 3.98618 7.29232
U_c = [[-0.08223873]] U_f = [[-0.00933632]] b_f = [ 0.99561626]
Epoch 2/300
10s - loss: 1650.5454 - val_loss: 9137.3620
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1622.5751 - val_loss: 9046.4113
Epoch 00002: val_loss improved from 9121.09219 to 9046.41128, saving model to huabei40x25_weights.hdf5
         huabei40x25  1608.3      0.65  0.32  0.50      0.64  0.20  0.55      0.65  0.13  0.59
forget = 0.957932
delta_x = 4.66958
delta_h = 1.93141
delta mean, abs_mean, abs_mean+, abs_mean-: 0.670708 4.66958 3.71292 7.11917
U_c = [[-0.07839055]] U_f = [[-0.01351544]] b_f = [ 0.84255546]
         huabei40x25  9046.4      0.84  0.21  0.68      0.83  0.14  0.73      0.83  0.09  0.76
forget = 0.963666
delta_x = 5.1273
delta_h = 3.00553
delta mean, abs_mean, abs_mean+, abs_mean-: 0.766818 5.1273 3.88198 9.05279
U_c = [[-0.07839055]] U_f = [[-0.01351544]] b_f = [ 0.84255546]
Epoch 4/300
10s - loss: 1598.1675 - val_loss: 8684.1379
Epoch 00003: val_loss improved from 9046.41128 to 8684.13789, saving model to huabei40x25_weights.hdf5
         huabei40x25  1584.2      0.70  0.34  0.52      0.69  0.21  0.58      0.71  0.15  0.63
forget = 0.957512
delta_x = 5.23085
delta_h = 1.93744
delta mean, abs_mean, abs_mean+, abs_mean-: 0.867692 5.23085 4.16869 8.1242
U_c = [[-0.07167213]] U_f = [[-0.01525439]] b_f = [ 0.77527297]
         huabei40x25  8684.1      0.85  0.21  0.69      0.83  0.14  0.73      0.84  0.10  0.76
forget = 0.962367
delta_x = 5.643
delta_h = 2.92249
delta mean, abs_mean, abs_mean+, abs_mean-: 0.798726 5.643 4.20442 10.3539
U_c = [[-0.07167213]] U_f = [[-0.01525439]] b_f = [ 0.77527297]
Epoch 5/300
10s - loss: 1572.1791 - val_loss: 8606.0129
Epoch 00004: val_loss improved from 8684.13789 to 8606.01293, saving model to huabei40x25_weights.hdf5
         huabei40x25  1552.4      0.69  0.33  0.52      0.68  0.21  0.58      0.70  0.15  0.63
forget = 0.953543
delta_x = 5.46438
delta_h = 1.83121
delta mean, abs_mean, abs_mean+, abs_mean-: 0.684186 5.46438 4.21275 8.84426
U_c = [[-0.06677809]] U_f = [[-0.0162639]] b_f = [ 0.71401328]
         huabei40x25  8606.0      0.84  0.20  0.69      0.82  0.13  0.73      0.83  0.09  0.76
forget = 0.95857
delta_x = 5.85958
delta_h = 2.81043
delta mean, abs_mean, abs_mean+, abs_mean-: 0.523607 5.85958 4.20616 11.0609
U_c = [[-0.06677809]] U_f = [[-0.0162639]] b_f = [ 0.71401328]
Epoch 6/300
10s - loss: 1527.1528 - val_loss: 8995.2024
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1492.0402 - val_loss: 9545.3639
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1449.6984 - val_loss: 9697.6881
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1408.7133 - val_loss: 9825.9400
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1373.9289 - val_loss: 9470.0295
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1347.7828 - val_loss: 9570.5251
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1327.0488 - val_loss: 9531.2511
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1307.9959 - val_loss: 9708.1675
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1289.4561 - val_loss: 9571.2514
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1272.2592 - val_loss: 9554.4203
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1257.3262 - val_loss: 9540.0940
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1239.4080 - val_loss: 10354.7063
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1222.8218 - val_loss: 10042.9072
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1208.4975 - val_loss: 9887.4893
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1193.7896 - val_loss: 9978.4170
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1182.7659 - val_loss: 9877.9100
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1170.1436 - val_loss: 9801.2240
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1160.9629 - val_loss: 10820.6913
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1150.8391 - val_loss: 10219.9038
Epoch 00023: val_loss did not improve
Epoch 25/300
9s - loss: 1140.9604 - val_loss: 9915.5720
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1133.7844 - val_loss: 9929.3218
Epoch 00025: val_loss did not improve

training huabei40x26
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x26  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x26 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
9s - loss: 1822.8173 - val_loss: 8833.1384
Epoch 00000: val_loss improved from inf to 8833.13842, saving model to huabei40x26_weights.hdf5
         huabei40x26  1667.4      0.66  0.34  0.49      0.64  0.21  0.55      0.65  0.14  0.59
forget = 0.956951
delta_x = 4.47633
delta_h = 2.09955
delta mean, abs_mean, abs_mean+, abs_mean-: 0.825853 4.47633 3.72268 6.34088
U_c = [[-0.08298919]] U_f = [[-0.01228061]] b_f = [ 1.00422561]
         huabei40x26  8833.1      0.86  0.23  0.69      0.85  0.15  0.73      0.85  0.10  0.77
forget = 0.963457
delta_x = 5.30827
delta_h = 3.23497
delta mean, abs_mean, abs_mean+, abs_mean-: 1.15281 5.30827 4.31418 8.27182
U_c = [[-0.08298919]] U_f = [[-0.01228061]] b_f = [ 1.00422561]
Epoch 2/300
9s - loss: 1650.3087 - val_loss: 8834.4856
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1617.9193 - val_loss: 8968.2602
Epoch 00002: val_loss did not improve
Epoch 4/300
10s - loss: 1589.7571 - val_loss: 8876.1802
Epoch 00003: val_loss did not improve
Epoch 5/300
10s - loss: 1565.4838 - val_loss: 8880.1567
Epoch 00004: val_loss did not improve
Epoch 6/300
10s - loss: 1545.7483 - val_loss: 8789.8958
Epoch 00005: val_loss improved from 8833.13842 to 8789.89585, saving model to huabei40x26_weights.hdf5
         huabei40x26  1552.5      0.74  0.35  0.53      0.73  0.22  0.60      0.74  0.16  0.65
forget = 0.961255
delta_x = 5.08912
delta_h = 1.84821
delta mean, abs_mean, abs_mean+, abs_mean-: 0.953722 5.08912 3.94492 8.83262
U_c = [[-0.06489055]] U_f = [[-0.01708481]] b_f = [ 0.67417598]
         huabei40x26  8789.9      0.84  0.22  0.68      0.83  0.14  0.72      0.83  0.10  0.76
forget = 0.967099
delta_x = 5.29259
delta_h = 2.73304
delta mean, abs_mean, abs_mean+, abs_mean-: 0.828706 5.29259 3.89436 10.4256
U_c = [[-0.06489055]] U_f = [[-0.01708481]] b_f = [ 0.67417598]
Epoch 7/300
10s - loss: 1528.2689 - val_loss: 9205.6935
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1500.4881 - val_loss: 9066.6765
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1473.1722 - val_loss: 9296.8085
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1451.6409 - val_loss: 9311.6434
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1430.8534 - val_loss: 9589.1910
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1405.6457 - val_loss: 9811.5092
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1365.4782 - val_loss: 10241.8817
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1325.0289 - val_loss: 10337.2586
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1295.7250 - val_loss: 9857.3528
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1270.0698 - val_loss: 10242.6963
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1246.2320 - val_loss: 11060.0998
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1226.9905 - val_loss: 11482.5352
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1211.0618 - val_loss: 11103.0925
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1198.7921 - val_loss: 11284.4952
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1187.0936 - val_loss: 10459.7091
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1178.8065 - val_loss: 11731.9258
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1169.8814 - val_loss: 11301.1428
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1161.5301 - val_loss: 11133.2489
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1152.7107 - val_loss: 11445.2007
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1144.8793 - val_loss: 11174.6164
Epoch 00025: val_loss did not improve
Epoch 27/300
10s - loss: 1138.7859 - val_loss: 11361.5413
Epoch 00026: val_loss did not improve

training huabei40x27
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x27  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x27 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
9s - loss: 1816.9881 - val_loss: 9102.1583
Epoch 00000: val_loss improved from inf to 9102.15833, saving model to huabei40x27_weights.hdf5
         huabei40x27  1666.3      0.65  0.34  0.49      0.63  0.20  0.54      0.65  0.14  0.58
forget = 0.955889
delta_x = 4.42891
delta_h = 2.09623
delta mean, abs_mean, abs_mean+, abs_mean-: 0.795879 4.42891 3.70978 6.14089
U_c = [[-0.08568208]] U_f = [[-0.01150293]] b_f = [ 0.99573153]
         huabei40x27  9102.2      0.86  0.22  0.68      0.84  0.14  0.73      0.84  0.10  0.77
forget = 0.960915
delta_x = 5.09652
delta_h = 3.1033
delta mean, abs_mean, abs_mean+, abs_mean-: 0.889166 5.09652 4.12293 7.67487
U_c = [[-0.08568208]] U_f = [[-0.01150293]] b_f = [ 0.99573153]
Epoch 2/300
10s - loss: 1650.1637 - val_loss: 9044.1634
Epoch 00001: val_loss improved from 9102.15833 to 9044.16343, saving model to huabei40x27_weights.hdf5
         huabei40x27  1634.4      0.65  0.32  0.49      0.63  0.20  0.54      0.64  0.13  0.58
forget = 0.954847
delta_x = 4.68014
delta_h = 2.10597
delta mean, abs_mean, abs_mean+, abs_mean-: 0.645239 4.68014 3.74765 6.96865
U_c = [[-0.08494788]] U_f = [[-0.01337183]] b_f = [ 0.92300874]
         huabei40x27  9044.2      0.86  0.22  0.68      0.84  0.14  0.73      0.84  0.10  0.77
forget = 0.962941
delta_x = 5.22525
delta_h = 3.18822
delta mean, abs_mean, abs_mean+, abs_mean-: 0.95975 5.22525 4.10001 8.67912
U_c = [[-0.08494788]] U_f = [[-0.01337183]] b_f = [ 0.92300874]
Epoch 3/300
10s - loss: 1618.8787 - val_loss: 8798.6278
Epoch 00002: val_loss improved from 9044.16343 to 8798.62785, saving model to huabei40x27_weights.hdf5
         huabei40x27  1608.9      0.70  0.35  0.51      0.69  0.22  0.58      0.70  0.15  0.62
forget = 0.961243
delta_x = 4.84663
delta_h = 2.00525
delta mean, abs_mean, abs_mean+, abs_mean-: 0.972354 4.84663 3.94787 7.36488
U_c = [[-0.07570185]] U_f = [[-0.01449534]] b_f = [ 0.84768981]
         huabei40x27  8798.6      0.86  0.23  0.68      0.85  0.15  0.73      0.85  0.10  0.77
forget = 0.967761
delta_x = 5.31144
delta_h = 3.06068
delta mean, abs_mean, abs_mean+, abs_mean-: 1.1594 5.31144 4.16643 9.29053
U_c = [[-0.07570185]] U_f = [[-0.01449534]] b_f = [ 0.84768981]
Epoch 4/300
10s - loss: 1592.2936 - val_loss: 9002.0377
Epoch 00003: val_loss did not improve
Epoch 5/300
10s - loss: 1554.0265 - val_loss: 9122.4547
Epoch 00004: val_loss did not improve
Epoch 6/300
10s - loss: 1517.8088 - val_loss: 8836.2119
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1490.1105 - val_loss: 8681.9179
Epoch 00006: val_loss improved from 8798.62785 to 8681.91794, saving model to huabei40x27_weights.hdf5
         huabei40x27  1474.7      0.73  0.32  0.54      0.71  0.19  0.60      0.72  0.13  0.64
forget = 0.960982
delta_x = 5.23717
delta_h = 1.59562
delta mean, abs_mean, abs_mean+, abs_mean-: 0.723526 5.23717 4.58856 6.43917
U_c = [[-0.05975819]] U_f = [[-0.01905744]] b_f = [ 0.54973269]
         huabei40x27  8681.9      0.80  0.21  0.66      0.79  0.14  0.70      0.80  0.09  0.73
forget = 0.966407
delta_x = 5.35264
delta_h = 2.44032
delta mean, abs_mean, abs_mean+, abs_mean-: 0.45181 5.35264 4.14991 8.15027
U_c = [[-0.05975819]] U_f = [[-0.01905744]] b_f = [ 0.54973269]
Epoch 8/300
10s - loss: 1463.6166 - val_loss: 8889.5042
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1426.6936 - val_loss: 9581.8975
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1397.8574 - val_loss: 9901.3595
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1374.7268 - val_loss: 9799.9438
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1349.7912 - val_loss: 9675.8072
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1319.9598 - val_loss: 9878.3065
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1289.3871 - val_loss: 9767.2120
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1262.6556 - val_loss: 10020.7032
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1243.5538 - val_loss: 10213.0293
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1227.3302 - val_loss: 10358.8409
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1214.9585 - val_loss: 10679.6976
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1204.5689 - val_loss: 10606.7320
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1192.1021 - val_loss: 10858.3130
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1179.3999 - val_loss: 10779.0197
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1168.0099 - val_loss: 11243.5457
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1153.8497 - val_loss: 11173.4088
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1142.9061 - val_loss: 10983.6964
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1132.8453 - val_loss: 11745.0864
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1121.7283 - val_loss: 10721.7492
Epoch 00025: val_loss did not improve
Epoch 27/300
10s - loss: 1112.6642 - val_loss: 11148.6553
Epoch 00026: val_loss did not improve
Epoch 28/300
10s - loss: 1104.3110 - val_loss: 11937.8526
Epoch 00027: val_loss did not improve

training huabei40x28
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x28  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x28 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
9s - loss: 1821.0547 - val_loss: 8830.5152
Epoch 00000: val_loss improved from inf to 8830.51517, saving model to huabei40x28_weights.hdf5
         huabei40x28  1669.7      0.66  0.35  0.49      0.64  0.21  0.55      0.65  0.14  0.59
forget = 0.958236
delta_x = 4.30755
delta_h = 2.051
delta mean, abs_mean, abs_mean+, abs_mean-: 0.87596 4.30755 3.65799 5.88645
U_c = [[-0.08198866]] U_f = [[-0.00970276]] b_f = [ 1.00268173]
         huabei40x28  8830.5      0.87  0.23  0.69      0.85  0.15  0.74      0.86  0.10  0.77
forget = 0.964216
delta_x = 5.21254
delta_h = 3.27771
delta mean, abs_mean, abs_mean+, abs_mean-: 1.31306 5.21254 4.30608 8.04745
U_c = [[-0.08198866]] U_f = [[-0.00970276]] b_f = [ 1.00268173]
Epoch 2/300
9s - loss: 1649.7585 - val_loss: 9082.1867
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1619.4081 - val_loss: 8874.4222
Epoch 00002: val_loss did not improve
Epoch 4/300
10s - loss: 1587.9515 - val_loss: 8810.7893
Epoch 00003: val_loss improved from 8830.51517 to 8810.78930, saving model to huabei40x28_weights.hdf5
         huabei40x28  1569.0      0.72  0.34  0.52      0.71  0.22  0.59      0.73  0.16  0.64
forget = 0.958239
delta_x = 5.3175
delta_h = 1.8647
delta mean, abs_mean, abs_mean+, abs_mean-: 0.902734 5.3175 4.25217 8.21867
U_c = [[-0.06754558]] U_f = [[-0.01608223]] b_f = [ 0.76338345]
         huabei40x28  8810.8      0.84  0.21  0.68      0.82  0.14  0.72      0.83  0.09  0.76
forget = 0.963746
delta_x = 5.53305
delta_h = 2.67321
delta mean, abs_mean, abs_mean+, abs_mean-: 0.637343 5.53305 4.13278 9.65694
U_c = [[-0.06754558]] U_f = [[-0.01608223]] b_f = [ 0.76338345]
Epoch 5/300
10s - loss: 1544.5161 - val_loss: 9117.5618
Epoch 00004: val_loss did not improve
Epoch 6/300
10s - loss: 1504.5416 - val_loss: 9443.2746
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1458.7395 - val_loss: 9322.3791
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1417.0281 - val_loss: 9411.2983
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1383.9219 - val_loss: 9601.7595
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1354.7075 - val_loss: 9771.4056
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1331.4204 - val_loss: 9666.9947
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1312.8055 - val_loss: 9842.6447
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1294.9252 - val_loss: 9517.4819
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1277.1513 - val_loss: 9703.3178
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1258.3564 - val_loss: 10098.0255
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1238.0987 - val_loss: 10305.2998
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1218.6690 - val_loss: 9846.6730
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1200.2170 - val_loss: 10633.6630
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1184.1315 - val_loss: 9821.3054
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1171.4148 - val_loss: 9774.3168
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1156.3103 - val_loss: 9721.7216
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1146.4860 - val_loss: 9962.0860
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1134.5224 - val_loss: 10389.5460
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1124.0605 - val_loss: 10169.2936
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1113.1794 - val_loss: 10432.3402
Epoch 00024: val_loss did not improve

training huabei40x29
Train on 98420 samples, validate on 11655 samples
Before training:
         huabei40x29  6184.9      0.02  -nan  0.02      0.04  -nan  0.04      0.06  -nan  0.06
forget = 0.731057
delta_x = 3.94437
delta_h = 2.50113
delta mean, abs_mean, abs_mean+, abs_mean-: -3.94437 3.94437 nan 3.94437
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
         huabei40x29 24783.3      0.04  -nan  0.04      0.06  -nan  0.06      0.11  -nan  0.11
forget = 0.731059
delta_x = 6.03627
delta_h = 3.45949
delta mean, abs_mean, abs_mean+, abs_mean-: -6.03627 6.03627 nan 6.03627
U_c = [[-0.05]] U_f = [[ 0.]] b_f = [ 1.]
Epoch 1/300
9s - loss: 1821.3492 - val_loss: 8928.0874
Epoch 00000: val_loss improved from inf to 8928.08737, saving model to huabei40x29_weights.hdf5
         huabei40x29  1667.3      0.64  0.33  0.49      0.62  0.20  0.54      0.63  0.13  0.57
forget = 0.956547
delta_x = 4.2705
delta_h = 1.99838
delta mean, abs_mean, abs_mean+, abs_mean-: 0.707699 4.2705 3.54785 5.96943
U_c = [[-0.08374485]] U_f = [[-0.01162646]] b_f = [ 0.99754769]
         huabei40x29  8928.1      0.85  0.22  0.68      0.84  0.14  0.73      0.84  0.10  0.77
forget = 0.961343
delta_x = 5.10804
delta_h = 3.15613
delta mean, abs_mean, abs_mean+, abs_mean-: 0.902357 5.10804 4.0954 7.89945
U_c = [[-0.08374485]] U_f = [[-0.01162646]] b_f = [ 0.99754769]
Epoch 2/300
9s - loss: 1651.0517 - val_loss: 8937.1893
Epoch 00001: val_loss did not improve
Epoch 3/300
10s - loss: 1622.1361 - val_loss: 8950.5825
Epoch 00002: val_loss did not improve
Epoch 4/300
10s - loss: 1596.4392 - val_loss: 8827.5699
Epoch 00003: val_loss improved from 8928.08737 to 8827.56989, saving model to huabei40x29_weights.hdf5
         huabei40x29  1593.0      0.71  0.35  0.52      0.69  0.22  0.58      0.70  0.15  0.62
forget = 0.962626
delta_x = 4.7911
delta_h = 1.97059
delta mean, abs_mean, abs_mean+, abs_mean-: 0.958823 4.7911 3.86164 7.49931
U_c = [[-0.07480054]] U_f = [[-0.01267504]] b_f = [ 0.80497134]
         huabei40x29  8827.6      0.84  0.21  0.69      0.83  0.13  0.73      0.83  0.09  0.76
forget = 0.965226
delta_x = 5.37961
delta_h = 2.97638
delta mean, abs_mean, abs_mean+, abs_mean-: 0.758203 5.37961 4.00692 9.87072
U_c = [[-0.07480054]] U_f = [[-0.01267504]] b_f = [ 0.80497134]
Epoch 5/300
10s - loss: 1570.8357 - val_loss: 8668.6482
Epoch 00004: val_loss improved from 8827.56989 to 8668.64817, saving model to huabei40x29_weights.hdf5
         huabei40x29  1552.3      0.70  0.33  0.52      0.68  0.21  0.58      0.70  0.14  0.63
forget = 0.959271
delta_x = 5.10351
delta_h = 1.83331
delta mean, abs_mean, abs_mean+, abs_mean-: 0.781846 5.10351 4.00306 8.15734
U_c = [[-0.0687902]] U_f = [[-0.01685835]] b_f = [ 0.74253458]
         huabei40x29  8668.6      0.83  0.19  0.69      0.82  0.12  0.73      0.82  0.08  0.76
forget = 0.963325
delta_x = 5.6293
delta_h = 2.81996
delta mean, abs_mean, abs_mean+, abs_mean-: 0.482144 5.6293 4.00199 10.8842
U_c = [[-0.0687902]] U_f = [[-0.01685835]] b_f = [ 0.74253458]
Epoch 6/300
10s - loss: 1542.3114 - val_loss: 8984.4764
Epoch 00005: val_loss did not improve
Epoch 7/300
10s - loss: 1517.8802 - val_loss: 8696.5792
Epoch 00006: val_loss did not improve
Epoch 8/300
10s - loss: 1491.7600 - val_loss: 8826.2994
Epoch 00007: val_loss did not improve
Epoch 9/300
10s - loss: 1449.4267 - val_loss: 9873.0210
Epoch 00008: val_loss did not improve
Epoch 10/300
10s - loss: 1402.8831 - val_loss: 10449.5265
Epoch 00009: val_loss did not improve
Epoch 11/300
10s - loss: 1361.4189 - val_loss: 10683.5022
Epoch 00010: val_loss did not improve
Epoch 12/300
10s - loss: 1333.3067 - val_loss: 10644.3960
Epoch 00011: val_loss did not improve
Epoch 13/300
10s - loss: 1306.5748 - val_loss: 10884.9551
Epoch 00012: val_loss did not improve
Epoch 14/300
10s - loss: 1283.0801 - val_loss: 10911.2824
Epoch 00013: val_loss did not improve
Epoch 15/300
10s - loss: 1263.8575 - val_loss: 11415.1771
Epoch 00014: val_loss did not improve
Epoch 16/300
10s - loss: 1248.6631 - val_loss: 11193.1525
Epoch 00015: val_loss did not improve
Epoch 17/300
10s - loss: 1235.5274 - val_loss: 10975.7194
Epoch 00016: val_loss did not improve
Epoch 18/300
10s - loss: 1222.4554 - val_loss: 10962.8739
Epoch 00017: val_loss did not improve
Epoch 19/300
10s - loss: 1211.4022 - val_loss: 11376.8369
Epoch 00018: val_loss did not improve
Epoch 20/300
10s - loss: 1199.4377 - val_loss: 10632.2844
Epoch 00019: val_loss did not improve
Epoch 21/300
10s - loss: 1190.8557 - val_loss: 10459.1218
Epoch 00020: val_loss did not improve
Epoch 22/300
10s - loss: 1180.1720 - val_loss: 11172.8574
Epoch 00021: val_loss did not improve
Epoch 23/300
10s - loss: 1172.0088 - val_loss: 11482.1459
Epoch 00022: val_loss did not improve
Epoch 24/300
10s - loss: 1163.0573 - val_loss: 10843.1956
Epoch 00023: val_loss did not improve
Epoch 25/300
10s - loss: 1154.6945 - val_loss: 11405.2120
Epoch 00024: val_loss did not improve
Epoch 26/300
10s - loss: 1148.0750 - val_loss: 11738.8316
Epoch 00025: val_loss did not improve
         huabei40x20  1638.1      0.68  0.34  0.50      0.66  0.21  0.56      0.67  0.14  0.60
         huabei40x20  8878.5      0.87  0.23  0.69      0.85  0.15  0.74      0.85  0.10  0.77
         huabei40x21  1674.8      0.68  0.35  0.50      0.66  0.22  0.56      0.67  0.15  0.60
         huabei40x21  8792.5      0.88  0.24  0.68      0.86  0.16  0.73      0.86  0.11  0.78
         huabei40x22  1501.2      0.73  0.33  0.54      0.71  0.21  0.60      0.73  0.15  0.64
         huabei40x22  8701.9      0.81  0.19  0.67      0.79  0.12  0.71      0.80  0.09  0.74
         huabei40x23  1488.7      0.73  0.32  0.54      0.71  0.20  0.60      0.73  0.14  0.65
         huabei40x23  8570.0      0.83  0.21  0.68      0.82  0.14  0.72      0.83  0.09  0.76
         huabei40x24  1470.3      0.72  0.31  0.54      0.70  0.19  0.60      0.72  0.14  0.64
         huabei40x24  8882.4      0.80  0.20  0.66      0.78  0.12  0.70      0.79  0.08  0.73
         huabei40x25  1552.4      0.69  0.33  0.52      0.68  0.21  0.58      0.70  0.15  0.63
         huabei40x25  8606.0      0.84  0.20  0.69      0.82  0.13  0.73      0.83  0.09  0.76
         huabei40x26  1552.5      0.74  0.35  0.53      0.73  0.22  0.60      0.74  0.16  0.65
         huabei40x26  8789.9      0.84  0.22  0.68      0.83  0.14  0.72      0.83  0.10  0.76
         huabei40x27  1474.7      0.73  0.32  0.54      0.71  0.19  0.60      0.72  0.13  0.64
         huabei40x27  8681.9      0.80  0.21  0.66      0.79  0.14  0.70      0.80  0.09  0.73
         huabei40x28  1569.0      0.72  0.34  0.52      0.71  0.22  0.59      0.73  0.16  0.64
         huabei40x28  8810.8      0.84  0.21  0.68      0.82  0.14  0.72      0.83  0.09  0.76
         huabei40x29  1552.3      0.70  0.33  0.52      0.68  0.21  0.58      0.70  0.14  0.63
         huabei40x29  8668.6      0.83  0.19  0.69      0.82  0.12  0.73      0.82  0.08  0.76
