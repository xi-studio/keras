X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid0
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid0 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid029822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5930.9112 - val_loss: 9819.9469
Epoch 00000: val_loss improved from inf to 9819.94687, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 3281.8136      0.55  0.21  0.48      0.55  0.19  0.49      0.53  0.17  0.48
beijing_tanh+hardsigmoid0 9819.9469      0.78  0.09  0.73      0.78  0.05  0.75      0.79  0.03  0.77
forget mean min: 0.847061 0.321567
incx.max(), incx.min(), incx.mean() 2.9118 -2.74386 1.34693
fgtx.max(), fgtx.min(), fgtx.mean() 1.93236 -1.98585 0.848239
abs_mean, abs_mean+, abs_mean-: 6.50135 2.59967 12.5001
U_c = [[-0.05977378]] U_f = [[ 0.]] b_c = [ 0.12257833] b_f = [ 1.09368801]
W_c max, min, mean, abs_mean: 0.149267 -0.148845 0.00126757 0.147293
W_f max, min, mean, abs_mean: 0.103696 -0.103764 0.00069203 0.102043
Epoch 2/300
1s - loss: 2439.3958 - val_loss: 11124.9411
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1865.8328 - val_loss: 9645.0261
Epoch 00002: val_loss improved from 9819.94687 to 9645.02614, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1736.9116      0.85  0.23  0.67      0.85  0.21  0.69      0.86  0.18  0.72
beijing_tanh+hardsigmoid0 9645.0262      0.79  0.15  0.69      0.79  0.12  0.71      0.82  0.09  0.76
forget mean min: 0.910629 0.415561
incx.max(), incx.min(), incx.mean() 6.27787 -5.72545 4.14407
fgtx.max(), fgtx.min(), fgtx.mean() 1.53384 -1.53387 0.988503
abs_mean, abs_mean+, abs_mean-: 8.79334 5.31654 17.9134
U_c = [[-0.01175671]] U_f = [[ 0.]] b_c = [ 0.27627212] b_f = [ 1.1116749]
W_c max, min, mean, abs_mean: 0.303222 -0.302802 0.00125311 0.301264
W_f max, min, mean, abs_mean: 0.078632 -0.0787714 0.000680853 0.0769945
Epoch 4/300
1s - loss: 1736.5360 - val_loss: 8467.9100
Epoch 00003: val_loss improved from 9645.02614 to 8467.91003, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1657.5330      0.86  0.23  0.69      0.87  0.21  0.71      0.87  0.18  0.73
beijing_tanh+hardsigmoid0 8467.9100      0.83  0.16  0.72      0.84  0.12  0.75      0.86  0.09  0.79
forget mean min: 0.914259 0.386028
incx.max(), incx.min(), incx.mean() 6.8093 -6.2101 4.49731
fgtx.max(), fgtx.min(), fgtx.mean() 1.67635 -1.67733 1.0808
abs_mean, abs_mean+, abs_mean-: 9.56886 5.88837 20.6528
U_c = [[-0.0192317]] U_f = [[ 0.]] b_c = [ 0.30149779] b_f = [ 1.10746717]
W_c max, min, mean, abs_mean: 0.328608 -0.328184 0.00125123 0.326649
W_f max, min, mean, abs_mean: 0.0857854 -0.0859348 0.000677555 0.0841416
Epoch 5/300
1s - loss: 1681.8955 - val_loss: 8058.2123
Epoch 00004: val_loss improved from 8467.91003 to 8058.21231, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1609.7365      0.88  0.23  0.69      0.88  0.22  0.71      0.88  0.18  0.74
beijing_tanh+hardsigmoid0 8058.2122      0.85  0.15  0.74      0.85  0.12  0.76      0.88  0.09  0.81
forget mean min: 0.913321 0.342687
incx.max(), incx.min(), incx.mean() 7.10044 -6.47991 4.63432
fgtx.max(), fgtx.min(), fgtx.mean() 1.88718 -1.88972 1.20132
abs_mean, abs_mean+, abs_mean-: 10.2182 6.17961 24.5603
U_c = [[-0.01788208]] U_f = [[ 0.]] b_c = [ 0.31482264] b_f = [ 1.10315001]
W_c max, min, mean, abs_mean: 0.342625 -0.342198 0.00124961 0.340664
W_f max, min, mean, abs_mean: 0.0963946 -0.0965577 0.000673513 0.0947438
Epoch 6/300
1s - loss: 1656.6680 - val_loss: 7685.4796
Epoch 00005: val_loss improved from 8058.21231 to 7685.47961, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1596.9450      0.89  0.25  0.68      0.89  0.24  0.70      0.90  0.20  0.73
beijing_tanh+hardsigmoid0 7685.4796      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.08  0.81
forget mean min: 0.908691 0.311551
incx.max(), incx.min(), incx.mean() 7.2569 -6.63299 4.60036
fgtx.max(), fgtx.min(), fgtx.mean() 2.03418 -2.04005 1.25496
abs_mean, abs_mean+, abs_mean-: 10.3595 6.24765 24.7305
U_c = [[-0.01779914]] U_f = [[ 0.]] b_c = [ 0.32194835] b_f = [ 1.09780145]
W_c max, min, mean, abs_mean: 0.350735 -0.350303 0.00124728 0.348772
W_f max, min, mean, abs_mean: 0.10396 -0.104138 0.000669677 0.102303
Epoch 7/300
1s - loss: 1645.6883 - val_loss: 7327.1982
Epoch 00006: val_loss improved from 7685.47961 to 7327.19823, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1579.4144      0.89  0.25  0.69      0.89  0.23  0.71      0.90  0.20  0.73
beijing_tanh+hardsigmoid0 7327.1982      0.85  0.13  0.75      0.85  0.11  0.78      0.88  0.08  0.82
forget mean min: 0.906895 0.291438
incx.max(), incx.min(), incx.mean() 7.36671 -6.74531 4.58023
fgtx.max(), fgtx.min(), fgtx.mean() 2.12574 -2.13586 1.28427
abs_mean, abs_mean+, abs_mean-: 10.37 6.26394 24.5553
U_c = [[-0.01897046]] U_f = [[ 0.]] b_c = [ 0.32745034] b_f = [ 1.09305215]
W_c max, min, mean, abs_mean: 0.356836 -0.356401 0.00124529 0.354873
W_f max, min, mean, abs_mean: 0.108828 -0.109019 0.00066565 0.107166
Epoch 8/300
1s - loss: 1635.9169 - val_loss: 7272.7219
Epoch 00007: val_loss improved from 7327.19823 to 7272.72192, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1567.1940      0.88  0.24  0.69      0.88  0.22  0.71      0.89  0.18  0.74
beijing_tanh+hardsigmoid0 7272.7218      0.85  0.13  0.75      0.85  0.10  0.77      0.88  0.08  0.82
forget mean min: 0.907888 0.283608
incx.max(), incx.min(), incx.mean() 7.46236 -6.83744 4.56937
fgtx.max(), fgtx.min(), fgtx.mean() 2.15975 -2.17158 1.28349
abs_mean, abs_mean+, abs_mean-: 10.2964 6.18729 24.1631
U_c = [[-0.01734089]] U_f = [[ 0.]] b_c = [ 0.33198509] b_f = [ 1.08962369]
W_c max, min, mean, abs_mean: 0.362002 -0.361563 0.00124331 0.360037
W_f max, min, mean, abs_mean: 0.110718 -0.110919 0.000663168 0.109053
Epoch 9/300
1s - loss: 1627.0254 - val_loss: 7158.3146
Epoch 00008: val_loss improved from 7272.72192 to 7158.31464, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1558.8417      0.89  0.24  0.69      0.89  0.22  0.71      0.90  0.19  0.74
beijing_tanh+hardsigmoid0 7158.3146      0.85  0.13  0.75      0.85  0.10  0.78      0.88  0.07  0.82
forget mean min: 0.908034 0.266067
incx.max(), incx.min(), incx.mean() 7.54092 -6.90592 4.56898
fgtx.max(), fgtx.min(), fgtx.mean() 2.24827 -2.25969 1.32092
abs_mean, abs_mean+, abs_mean-: 10.2848 6.19101 24.1364
U_c = [[-0.0181491]] U_f = [[ 0.]] b_c = [ 0.33579987] b_f = [ 1.09002197]
W_c max, min, mean, abs_mean: 0.365937 -0.365496 0.00124171 0.363972
W_f max, min, mean, abs_mean: 0.115239 -0.115451 0.000660121 0.113573
Epoch 10/300
1s - loss: 1616.4995 - val_loss: 7191.4150
Epoch 00009: val_loss did not improve
Epoch 11/300
1s - loss: 1612.3699 - val_loss: 7159.3624
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1598.1731 - val_loss: 7076.9953
Epoch 00011: val_loss improved from 7158.31464 to 7076.99532, saving model to beijing_tanh+hardsigmoid0_weights.hdf5
beijing_tanh+hardsigmoid0 1507.8198      0.88  0.23  0.70      0.89  0.20  0.73      0.90  0.17  0.75
beijing_tanh+hardsigmoid0 7076.9953      0.84  0.11  0.76      0.84  0.08  0.78      0.86  0.06  0.82
forget mean min: 0.908519 0.273913
incx.max(), incx.min(), incx.mean() 8.00087 -7.31164 4.60337
fgtx.max(), fgtx.min(), fgtx.mean() 2.20951 -2.21664 1.22746
abs_mean, abs_mean+, abs_mean-: 9.93869 6.05012 20.8213
U_c = [[-0.0091265]] U_f = [[ 0.]] b_c = [ 0.35694259] b_f = [ 1.08620012]
W_c max, min, mean, abs_mean: 0.388487 -0.388036 0.00123714 0.386521
W_f max, min, mean, abs_mean: 0.113407 -0.113614 0.000659654 0.111726
Epoch 13/300
1s - loss: 1578.8422 - val_loss: 7207.2068
Epoch 00012: val_loss did not improve
Epoch 14/300
1s - loss: 1573.8159 - val_loss: 7207.6214
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1571.7113 - val_loss: 7360.6511
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1569.2504 - val_loss: 7244.0578
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1572.4178 - val_loss: 7299.2798
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1561.1082 - val_loss: 7856.7636
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1559.0951 - val_loss: 7290.1660
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1557.2977 - val_loss: 7816.6382
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1549.0173 - val_loss: 7404.5276
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1552.1412 - val_loss: 7782.1810
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1545.0726 - val_loss: 7449.9791
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1525.7629 - val_loss: 7733.5039
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1507.6524 - val_loss: 7526.6833
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1480.9617 - val_loss: 7578.4983
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1445.0967 - val_loss: 8297.0667
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1412.2357 - val_loss: 7603.0558
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1379.1889 - val_loss: 8140.3189
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1344.2539 - val_loss: 8489.9751
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1319.6351 - val_loss: 8404.2990
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1302.2182 - val_loss: 8577.1298
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1275.4356 - val_loss: 8512.9594
Epoch 00032: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid1
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid1 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid129822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5886.6289 - val_loss: 9590.4779
Epoch 00000: val_loss improved from inf to 9590.47785, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 3314.0687      0.55  0.22  0.48      0.55  0.20  0.49      0.53  0.18  0.48
beijing_tanh+hardsigmoid1 9590.4780      0.79  0.09  0.74      0.79  0.05  0.76      0.80  0.03  0.78
forget mean min: 0.844059 0.317555
incx.max(), incx.min(), incx.mean() 2.92629 -2.75903 1.3515
fgtx.max(), fgtx.min(), fgtx.mean() 1.95311 -2.00706 0.856173
abs_mean, abs_mean+, abs_mean-: 6.53053 2.60283 13.2062
U_c = [[-0.06074174]] U_f = [[ 0.]] b_c = [ 0.12235387] b_f = [ 1.09483457]
W_c max, min, mean, abs_mean: 0.149059 -0.149217 -0.00038426 0.14809
W_f max, min, mean, abs_mean: 0.103989 -0.104146 -0.000241898 0.103154
Epoch 2/300
1s - loss: 2473.5852 - val_loss: 10628.1331
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1884.7230 - val_loss: 9835.4645
Epoch 00002: val_loss did not improve
Epoch 4/300
1s - loss: 1739.9825 - val_loss: 8629.2818
Epoch 00003: val_loss improved from 9590.47785 to 8629.28181, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1659.2989      0.88  0.26  0.68      0.89  0.24  0.69      0.89  0.21  0.72
beijing_tanh+hardsigmoid1 8629.2817      0.84  0.15  0.73      0.84  0.12  0.76      0.87  0.09  0.80
forget mean min: 0.912943 0.376771
incx.max(), incx.min(), incx.mean() 6.91463 -6.30667 4.55132
fgtx.max(), fgtx.min(), fgtx.mean() 1.72409 -1.72518 1.10753
abs_mean, abs_mean+, abs_mean-: 9.90879 6.06108 22.4604
U_c = [[-0.01609817]] U_f = [[ 0.]] b_c = [ 0.30607501] b_f = [ 1.10904121]
W_c max, min, mean, abs_mean: 0.332534 -0.332712 -0.00037929 0.331565
W_f max, min, mean, abs_mean: 0.0873806 -0.0875181 -0.000238173 0.0865014
Epoch 5/300
1s - loss: 1682.6225 - val_loss: 8099.1065
Epoch 00004: val_loss improved from 8629.28181 to 8099.10652, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1627.0875      0.87  0.23  0.69      0.87  0.21  0.71      0.87  0.18  0.74
beijing_tanh+hardsigmoid1 8099.1066      0.84  0.15  0.73      0.84  0.12  0.76      0.87  0.09  0.80
forget mean min: 0.910342 0.345547
incx.max(), incx.min(), incx.mean() 7.08164 -6.46448 4.53368
fgtx.max(), fgtx.min(), fgtx.mean() 1.87008 -1.87272 1.16607
abs_mean, abs_mean+, abs_mean-: 9.9745 6.11758 21.9747
U_c = [[-0.0203271]] U_f = [[ 0.]] b_c = [ 0.31334805] b_f = [ 1.10044825]
W_c max, min, mean, abs_mean: 0.340829 -0.341011 -0.000379142 0.339859
W_f max, min, mean, abs_mean: 0.0947915 -0.0949267 -0.000239567 0.0939031
Epoch 6/300
1s - loss: 1663.5949 - val_loss: 7934.8595
Epoch 00005: val_loss improved from 8099.10652 to 7934.85952, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1596.2863      0.88  0.23  0.70      0.88  0.21  0.71      0.88  0.18  0.74
beijing_tanh+hardsigmoid1 7934.8596      0.85  0.15  0.74      0.85  0.12  0.76      0.88  0.09  0.81
forget mean min: 0.91128 0.311603
incx.max(), incx.min(), incx.mean() 7.28987 -6.65809 4.67503
fgtx.max(), fgtx.min(), fgtx.mean() 2.03569 -2.03986 1.27163
abs_mean, abs_mean+, abs_mean-: 10.4266 6.26993 25.4357
U_c = [[-0.01693794]] U_f = [[ 0.]] b_c = [ 0.32303554] b_f = [ 1.09787679]
W_c max, min, mean, abs_mean: 0.351027 -0.351213 -0.000378756 0.350055
W_f max, min, mean, abs_mean: 0.10318 -0.103312 -0.000240164 0.102285
Epoch 7/300
1s - loss: 1648.8109 - val_loss: 7352.5891
Epoch 00006: val_loss improved from 7934.85952 to 7352.58906, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1598.8031      0.90  0.26  0.68      0.90  0.24  0.70      0.91  0.21  0.73
beijing_tanh+hardsigmoid1 7352.5891      0.85  0.13  0.75      0.86  0.10  0.78      0.88  0.08  0.82
forget mean min: 0.904918 0.28766
incx.max(), incx.min(), incx.mean() 7.3543 -6.72948 4.57588
fgtx.max(), fgtx.min(), fgtx.mean() 2.14882 -2.15764 1.29925
abs_mean, abs_mean+, abs_mean-: 10.4559 6.30084 25.1408
U_c = [[-0.01935389]] U_f = [[ 0.]] b_c = [ 0.32682395] b_f = [ 1.09593701]
W_c max, min, mean, abs_mean: 0.354853 -0.355042 -0.000378372 0.35388
W_f max, min, mean, abs_mean: 0.109112 -0.109241 -0.000240959 0.108208
Epoch 8/300
1s - loss: 1637.9885 - val_loss: 7321.6965
Epoch 00007: val_loss improved from 7352.58906 to 7321.69645, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1574.3869      0.88  0.24  0.69      0.89  0.22  0.71      0.89  0.19  0.74
beijing_tanh+hardsigmoid1 7321.6965      0.84  0.13  0.75      0.85  0.10  0.77      0.88  0.08  0.82
forget mean min: 0.904634 0.277977
incx.max(), incx.min(), incx.mean() 7.43168 -6.81326 4.53629
fgtx.max(), fgtx.min(), fgtx.mean() 2.19027 -2.20386 1.29713
abs_mean, abs_mean+, abs_mean-: 10.369 6.23718 24.3952
U_c = [[-0.01950304]] U_f = [[ 0.]] b_c = [ 0.33123559] b_f = [ 1.0937494]
W_c max, min, mean, abs_mean: 0.359643 -0.359835 -0.000377883 0.358668
W_f max, min, mean, abs_mean: 0.11155 -0.111676 -0.000241433 0.110638
Epoch 9/300
1s - loss: 1634.6070 - val_loss: 7344.7121
Epoch 00008: val_loss did not improve
Epoch 10/300
1s - loss: 1627.5036 - val_loss: 7090.8122
Epoch 00009: val_loss improved from 7321.69645 to 7090.81217, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1558.9100      0.88  0.24  0.69      0.89  0.22  0.71      0.89  0.19  0.74
beijing_tanh+hardsigmoid1 7090.8123      0.84  0.12  0.75      0.84  0.09  0.78      0.87  0.06  0.82
forget mean min: 0.901323 0.254646
incx.max(), incx.min(), incx.mean() 7.55336 -6.93402 4.39217
fgtx.max(), fgtx.min(), fgtx.mean() 2.29946 -2.31743 1.29203
abs_mean, abs_mean+, abs_mean-: 10.1532 6.11958 22.7414
U_c = [[-0.01995021]] U_f = [[ 0.]] b_c = [ 0.33787352] b_f = [ 1.09066558]
W_c max, min, mean, abs_mean: 0.36665 -0.366848 -0.000376931 0.365673
W_f max, min, mean, abs_mean: 0.117455 -0.117577 -0.000242996 0.116534
Epoch 11/300
1s - loss: 1626.7372 - val_loss: 7099.7126
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1617.8993 - val_loss: 7067.0615
Epoch 00011: val_loss improved from 7090.81217 to 7067.06149, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1556.0042      0.89  0.24  0.69      0.90  0.23  0.71      0.90  0.20  0.74
beijing_tanh+hardsigmoid1 7067.0614      0.85  0.12  0.76      0.85  0.09  0.78      0.88  0.06  0.83
forget mean min: 0.908802 0.248334
incx.max(), incx.min(), incx.mean() 7.88533 -7.22543 4.5692
fgtx.max(), fgtx.min(), fgtx.mean() 2.34051 -2.35532 1.30999
abs_mean, abs_mean+, abs_mean-: 10.0649 6.14831 21.9131
U_c = [[-0.01378037]] U_f = [[ 0.]] b_c = [ 0.35376319] b_f = [ 1.09698355]
W_c max, min, mean, abs_mean: 0.383001 -0.383205 -0.000375554 0.382021
W_f max, min, mean, abs_mean: 0.119646 -0.119763 -0.000242569 0.118717
Epoch 13/300
1s - loss: 1615.4247 - val_loss: 6971.9644
Epoch 00012: val_loss improved from 7067.06149 to 6971.96443, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1534.6560      0.88  0.22  0.70      0.89  0.20  0.72      0.89  0.17  0.75
beijing_tanh+hardsigmoid1 6971.9644      0.85  0.12  0.76      0.85  0.09  0.78      0.87  0.06  0.82
forget mean min: 0.906657 0.252584
incx.max(), incx.min(), incx.mean() 7.95226 -7.28029 4.50904
fgtx.max(), fgtx.min(), fgtx.mean() 2.32 -2.33301 1.26821
abs_mean, abs_mean+, abs_mean-: 9.99222 6.11087 21.4424
U_c = [[-0.01563342]] U_f = [[ 0.]] b_c = [ 0.35728097] b_f = [ 1.0959233]
W_c max, min, mean, abs_mean: 0.386428 -0.386633 -0.000375043 0.385447
W_f max, min, mean, abs_mean: 0.118671 -0.118786 -0.000243293 0.11774
Epoch 14/300
1s - loss: 1602.5001 - val_loss: 7007.8294
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1584.5162 - val_loss: 6956.9052
Epoch 00014: val_loss improved from 6971.96443 to 6956.90521, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1529.8139      0.89  0.23  0.70      0.90  0.21  0.72      0.90  0.18  0.75
beijing_tanh+hardsigmoid1 6956.9052      0.84  0.11  0.76      0.85  0.08  0.79      0.87  0.06  0.83
forget mean min: 0.907271 0.242747
incx.max(), incx.min(), incx.mean() 8.34181 -7.61768 4.6689
fgtx.max(), fgtx.min(), fgtx.mean() 2.37804 -2.38622 1.2816
abs_mean, abs_mean+, abs_mean-: 10.3734 6.26453 22.9043
U_c = [[-0.00874611]] U_f = [[ 0.]] b_c = [ 0.37575468] b_f = [ 1.09995306]
W_c max, min, mean, abs_mean: 0.404583 -0.404796 -0.00037418 0.403605
W_f max, min, mean, abs_mean: 0.121419 -0.121531 -0.000242951 0.120485
Epoch 16/300
1s - loss: 1582.3123 - val_loss: 7449.9706
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1573.8924 - val_loss: 7334.5979
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1571.8888 - val_loss: 7121.6726
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1572.3542 - val_loss: 7165.4390
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1553.1864 - val_loss: 7343.8096
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1560.9774 - val_loss: 7208.7503
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1554.9942 - val_loss: 7359.2480
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1545.5268 - val_loss: 7474.8478
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1534.4119 - val_loss: 7310.3838
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1514.6892 - val_loss: 7254.4219
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1493.1117 - val_loss: 7336.5709
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1471.1550 - val_loss: 6956.5628
Epoch 00026: val_loss improved from 6956.90521 to 6956.56283, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1376.1294      0.87  0.21  0.71      0.88  0.18  0.74      0.89  0.15  0.77
beijing_tanh+hardsigmoid1 6956.5628      0.84  0.11  0.76      0.84  0.08  0.78      0.86  0.06  0.82
forget mean min: 0.913613 0.201617
incx.max(), incx.min(), incx.mean() 9.31815 -8.27673 4.94002
fgtx.max(), fgtx.min(), fgtx.mean() 2.64824 -2.59951 1.34243
abs_mean, abs_mean+, abs_mean-: 10.1836 6.50149 18.7952
U_c = [[-0.00689904]] U_f = [[ 0.]] b_c = [ 0.43902299] b_f = [ 1.10758924]
W_c max, min, mean, abs_mean: 0.457983 -0.458298 -0.000376436 0.457074
W_f max, min, mean, abs_mean: 0.137871 -0.138878 -0.000257664 0.136325
Epoch 28/300
1s - loss: 1447.3840 - val_loss: 7168.3835
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1419.8829 - val_loss: 7025.4596
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1395.3430 - val_loss: 7172.0558
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1369.9642 - val_loss: 7338.9222
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1350.0198 - val_loss: 7821.3638
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1322.9894 - val_loss: 7378.2496
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1304.5857 - val_loss: 7656.0654
Epoch 00033: val_loss did not improve
Epoch 35/300
1s - loss: 1277.1813 - val_loss: 7614.2567
Epoch 00034: val_loss did not improve
Epoch 36/300
1s - loss: 1241.7277 - val_loss: 7714.3243
Epoch 00035: val_loss did not improve
Epoch 37/300
1s - loss: 1216.4616 - val_loss: 8330.8381
Epoch 00036: val_loss did not improve
Epoch 38/300
1s - loss: 1197.1874 - val_loss: 7523.2118
Epoch 00037: val_loss did not improve
Epoch 39/300
1s - loss: 1179.1212 - val_loss: 7513.3485
Epoch 00038: val_loss did not improve
Epoch 40/300
1s - loss: 1156.9506 - val_loss: 7353.3104
Epoch 00039: val_loss did not improve
Epoch 41/300
1s - loss: 1146.7316 - val_loss: 7100.6333
Epoch 00040: val_loss did not improve
Epoch 42/300
1s - loss: 1138.9790 - val_loss: 7204.0912
Epoch 00041: val_loss did not improve
Epoch 43/300
1s - loss: 1127.0958 - val_loss: 6957.9789
Epoch 00042: val_loss did not improve
Epoch 44/300
1s - loss: 1119.2999 - val_loss: 6617.3673
Epoch 00043: val_loss improved from 6956.56283 to 6617.36731, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1080.4952      0.90  0.19  0.74      0.92  0.16  0.79      0.92  0.13  0.82
beijing_tanh+hardsigmoid1 6617.3673      0.89  0.08  0.82      0.88  0.05  0.84      0.90  0.04  0.86
forget mean min: 0.901192 0.0772575
incx.max(), incx.min(), incx.mean() 13.888 -10.4333 6.04409
fgtx.max(), fgtx.min(), fgtx.mean() 3.70312 -3.11856 1.50294
abs_mean, abs_mean+, abs_mean-: 11.316 8.32984 16.2511
U_c = [[-0.01042485]] U_f = [[ 0.]] b_c = [ 0.68559563] b_f = [ 1.00484812]
W_c max, min, mean, abs_mean: 0.704633 -0.706342 -0.000579017 0.700669
W_f max, min, mean, abs_mean: 0.202671 -0.206706 -0.000352743 0.196534
Epoch 45/300
1s - loss: 1117.2704 - val_loss: 6992.1366
Epoch 00044: val_loss did not improve
Epoch 46/300
1s - loss: 1109.2639 - val_loss: 6128.8064
Epoch 00045: val_loss improved from 6617.36731 to 6128.80644, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1061.6970      0.90  0.19  0.74      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid1 6128.8064      0.91  0.08  0.84      0.90  0.06  0.85      0.91  0.04  0.88
forget mean min: 0.901777 0.0582315
incx.max(), incx.min(), incx.mean() 14.1095 -10.6332 6.09964
fgtx.max(), fgtx.min(), fgtx.mean() 3.79849 -3.20997 1.52948
abs_mean, abs_mean+, abs_mean-: 11.3189 8.40932 16.1637
U_c = [[-0.01496792]] U_f = [[ 0.]] b_c = [ 0.69966274] b_f = [ 1.00112808]
W_c max, min, mean, abs_mean: 0.718982 -0.720864 -0.00054245 0.714137
W_f max, min, mean, abs_mean: 0.209072 -0.213701 -0.000439306 0.202293
Epoch 47/300
1s - loss: 1103.4235 - val_loss: 6848.1641
Epoch 00046: val_loss did not improve
Epoch 48/300
1s - loss: 1098.1917 - val_loss: 6297.4482
Epoch 00047: val_loss did not improve
Epoch 49/300
1s - loss: 1098.3874 - val_loss: 6059.0811
Epoch 00048: val_loss improved from 6128.80644 to 6059.08114, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1062.1648      0.90  0.19  0.75      0.93  0.15  0.79      0.93  0.12  0.82
beijing_tanh+hardsigmoid1 6059.0812      0.90  0.08  0.84      0.90  0.05  0.85      0.91  0.04  0.88
forget mean min: 0.90308 0.0303632
incx.max(), incx.min(), incx.mean() 14.441 -10.9894 6.20693
fgtx.max(), fgtx.min(), fgtx.mean() 3.91952 -3.3437 1.56758
abs_mean, abs_mean+, abs_mean-: 11.4831 8.46348 16.7569
U_c = [[-0.01443681]] U_f = [[ 0.]] b_c = [ 0.71825868] b_f = [ 0.99551517]
W_c max, min, mean, abs_mean: 0.738751 -0.740968 -0.000486147 0.732418
W_f max, min, mean, abs_mean: 0.216847 -0.221999 -0.000484235 0.209202
Epoch 50/300
1s - loss: 1089.8925 - val_loss: 6417.6951
Epoch 00049: val_loss did not improve
Epoch 51/300
1s - loss: 1085.3955 - val_loss: 6326.8890
Epoch 00050: val_loss did not improve
Epoch 52/300
1s - loss: 1085.0764 - val_loss: 6022.9469
Epoch 00051: val_loss improved from 6059.08114 to 6022.94691, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1  999.4266      0.89  0.16  0.76      0.91  0.12  0.81      0.91  0.09  0.83
beijing_tanh+hardsigmoid1 6022.9469      0.90  0.08  0.83      0.89  0.05  0.85      0.91  0.04  0.87
forget mean min: 0.905236 0.0341011
incx.max(), incx.min(), incx.mean() 14.6444 -10.89 6.183
fgtx.max(), fgtx.min(), fgtx.mean() 3.97543 -3.32046 1.55761
abs_mean, abs_mean+, abs_mean-: 11.3751 8.46556 16.1332
U_c = [[-0.01669479]] U_f = [[ 0.]] b_c = [ 0.73173136] b_f = [ 0.99096638]
W_c max, min, mean, abs_mean: 0.753687 -0.756285 -0.000422364 0.745355
W_f max, min, mean, abs_mean: 0.22177 -0.22752 -0.000587107 0.212986
Epoch 53/300
1s - loss: 1082.5668 - val_loss: 6186.6505
Epoch 00052: val_loss did not improve
Epoch 54/300
1s - loss: 1075.7281 - val_loss: 6072.7934
Epoch 00053: val_loss did not improve
Epoch 55/300
1s - loss: 1070.7950 - val_loss: 5866.4359
Epoch 00054: val_loss improved from 6022.94691 to 5866.43587, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1029.0754      0.91  0.19  0.75      0.93  0.15  0.80      0.93  0.11  0.83
beijing_tanh+hardsigmoid1 5866.4359      0.90  0.08  0.84      0.90  0.05  0.86      0.92  0.04  0.88
forget mean min: 0.90613 0.00447602
incx.max(), incx.min(), incx.mean() 14.9014 -11.2249 6.29363
fgtx.max(), fgtx.min(), fgtx.mean() 4.10116 -3.4681 1.60723
abs_mean, abs_mean+, abs_mean-: 11.5408 8.50081 16.8845
U_c = [[-0.01448444]] U_f = [[ 0.]] b_c = [ 0.74670255] b_f = [ 0.9904781]
W_c max, min, mean, abs_mean: 0.770893 -0.774013 -0.000358871 0.760247
W_f max, min, mean, abs_mean: 0.230344 -0.236702 -0.00070819 0.220277
Epoch 56/300
1s - loss: 1072.4889 - val_loss: 6341.5358
Epoch 00055: val_loss did not improve
Epoch 57/300
1s - loss: 1066.3063 - val_loss: 6191.0812
Epoch 00056: val_loss did not improve
Epoch 58/300
1s - loss: 1063.1486 - val_loss: 6194.0432
Epoch 00057: val_loss did not improve
Epoch 59/300
1s - loss: 1059.8155 - val_loss: 5969.2019
Epoch 00058: val_loss did not improve
Epoch 60/300
1s - loss: 1058.9033 - val_loss: 6796.7951
Epoch 00059: val_loss did not improve
Epoch 61/300
1s - loss: 1046.6680 - val_loss: 6399.8321
Epoch 00060: val_loss did not improve
Epoch 62/300
1s - loss: 1044.1084 - val_loss: 5818.1991
Epoch 00061: val_loss improved from 5866.43587 to 5818.19910, saving model to beijing_tanh+hardsigmoid1_weights.hdf5
beijing_tanh+hardsigmoid1 1141.8007      0.91  0.19  0.75      0.93  0.16  0.79      0.92  0.13  0.81
beijing_tanh+hardsigmoid1 5818.1990      0.91  0.08  0.84      0.91  0.05  0.86      0.92  0.04  0.89
forget mean min: 0.898504 0.0
incx.max(), incx.min(), incx.mean() 15.3705 -11.5905 6.34339
fgtx.max(), fgtx.min(), fgtx.mean() 4.16894 -3.5341 1.59027
abs_mean, abs_mean+, abs_mean-: 11.9131 8.54835 18.5714
U_c = [[-0.01504009]] U_f = [[ 0.]] b_c = [ 0.78129858] b_f = [ 0.97997832]
W_c max, min, mean, abs_mean: 0.808746 -0.813315 -0.000211734 0.791217
W_f max, min, mean, abs_mean: 0.240006 -0.247891 -0.00103771 0.226078
Epoch 63/300
1s - loss: 1044.1868 - val_loss: 6483.8292
Epoch 00062: val_loss did not improve
Epoch 64/300
1s - loss: 1035.2736 - val_loss: 6122.1762
Epoch 00063: val_loss did not improve
Epoch 65/300
1s - loss: 1032.1427 - val_loss: 6226.3817
Epoch 00064: val_loss did not improve
Epoch 66/300
1s - loss: 1027.9187 - val_loss: 6100.6177
Epoch 00065: val_loss did not improve
Epoch 67/300
1s - loss: 1025.2056 - val_loss: 6368.0761
Epoch 00066: val_loss did not improve
Epoch 68/300
1s - loss: 1024.5979 - val_loss: 6063.8595
Epoch 00067: val_loss did not improve
Epoch 69/300
1s - loss: 1016.3196 - val_loss: 6412.4456
Epoch 00068: val_loss did not improve
Epoch 70/300
1s - loss: 1011.1247 - val_loss: 6740.0947
Epoch 00069: val_loss did not improve
Epoch 71/300
1s - loss: 1004.8938 - val_loss: 6207.7921
Epoch 00070: val_loss did not improve
Epoch 72/300
1s - loss: 1003.8772 - val_loss: 6197.5470
Epoch 00071: val_loss did not improve
Epoch 73/300
1s - loss: 999.3684 - val_loss: 6181.2606
Epoch 00072: val_loss did not improve
Epoch 74/300
1s - loss: 995.0563 - val_loss: 7148.5537
Epoch 00073: val_loss did not improve
Epoch 75/300
1s - loss: 990.3551 - val_loss: 6688.3522
Epoch 00074: val_loss did not improve
Epoch 76/300
1s - loss: 991.2605 - val_loss: 6621.6712
Epoch 00075: val_loss did not improve
Epoch 77/300
1s - loss: 986.0177 - val_loss: 6661.2625
Epoch 00076: val_loss did not improve
Epoch 78/300
1s - loss: 982.8545 - val_loss: 6792.7234
Epoch 00077: val_loss did not improve
Epoch 79/300
1s - loss: 980.4827 - val_loss: 6669.0777
Epoch 00078: val_loss did not improve
Epoch 80/300
1s - loss: 971.7611 - val_loss: 7500.0777
Epoch 00079: val_loss did not improve
Epoch 81/300
1s - loss: 973.0972 - val_loss: 6679.5943
Epoch 00080: val_loss did not improve
Epoch 82/300
1s - loss: 960.1487 - val_loss: 6712.2841
Epoch 00081: val_loss did not improve
Epoch 83/300
1s - loss: 964.2113 - val_loss: 6320.6765
Epoch 00082: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid2
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid2 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid229822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5910.2145 - val_loss: 9721.0381
Epoch 00000: val_loss improved from inf to 9721.03813, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 3291.9648      0.55  0.22  0.48      0.55  0.20  0.49      0.53  0.18  0.48
beijing_tanh+hardsigmoid2 9721.0381      0.79  0.09  0.73      0.79  0.05  0.75      0.80  0.03  0.77
forget mean min: 0.846779 0.32162
incx.max(), incx.min(), incx.mean() 2.90359 -2.73685 1.35162
fgtx.max(), fgtx.min(), fgtx.mean() 1.93201 -1.9863 0.853892
abs_mean, abs_mean+, abs_mean-: 6.4769 2.60558 12.5887
U_c = [[-0.06058642]] U_f = [[ 0.]] b_c = [ 0.12245841] b_f = [ 1.09440398]
W_c max, min, mean, abs_mean: 0.148806 -0.148979 -0.0304075 0.146824
W_f max, min, mean, abs_mean: 0.103432 -0.10355 -0.0213064 0.101995
Epoch 2/300
1s - loss: 2459.9597 - val_loss: 10552.8355
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1870.2092 - val_loss: 9334.4920
Epoch 00002: val_loss improved from 9721.03813 to 9334.49202, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1751.5662      0.85  0.25  0.67      0.86  0.23  0.69      0.87  0.19  0.72
beijing_tanh+hardsigmoid2 9334.4920      0.79  0.15  0.70      0.79  0.12  0.72      0.82  0.08  0.76
forget mean min: 0.909168 0.41996
incx.max(), incx.min(), incx.mean() 6.26254 -5.71301 4.13167
fgtx.max(), fgtx.min(), fgtx.mean() 1.50772 -1.50824 0.971076
abs_mean, abs_mean+, abs_mean-: 8.56134 5.27272 16.3596
U_c = [[-0.01361158]] U_f = [[ 0.]] b_c = [ 0.27577779] b_f = [ 1.10803711]
W_c max, min, mean, abs_mean: 0.302495 -0.302663 -0.0611316 0.300524
W_f max, min, mean, abs_mean: 0.0770951 -0.0772487 -0.0160338 0.075685
Epoch 4/300
1s - loss: 1738.0402 - val_loss: 8531.6543
Epoch 00003: val_loss improved from 9334.49202 to 8531.65434, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1649.2461      0.87  0.24  0.68      0.88  0.22  0.70      0.88  0.19  0.73
beijing_tanh+hardsigmoid2 8531.6543      0.84  0.15  0.73      0.84  0.12  0.75      0.87  0.09  0.80
forget mean min: 0.91222 0.383037
incx.max(), incx.min(), incx.mean() 6.84058 -6.23905 4.5016
fgtx.max(), fgtx.min(), fgtx.mean() 1.68945 -1.69051 1.08502
abs_mean, abs_mean+, abs_mean-: 9.78846 5.97022 22.0446
U_c = [[-0.01787356]] U_f = [[ 0.]] b_c = [ 0.30281496] b_f = [ 1.10569668]
W_c max, min, mean, abs_mean: 0.329917 -0.330083 -0.066615 0.327946
W_f max, min, mean, abs_mean: 0.0861626 -0.0863158 -0.017845 0.0847459
Epoch 5/300
1s - loss: 1683.5240 - val_loss: 8197.6605
Epoch 00004: val_loss improved from 8531.65434 to 8197.66046, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1614.9343      0.88  0.24  0.68      0.89  0.22  0.70      0.89  0.19  0.73
beijing_tanh+hardsigmoid2 8197.6606      0.84  0.15  0.73      0.84  0.12  0.76      0.87  0.09  0.80
forget mean min: 0.909667 0.353802
incx.max(), incx.min(), incx.mean() 7.09182 -6.47251 4.58453
fgtx.max(), fgtx.min(), fgtx.mean() 1.82803 -1.83055 1.15176
abs_mean, abs_mean+, abs_mean-: 10.18 6.20642 23.4095
U_c = [[-0.01802201]] U_f = [[ 0.]] b_c = [ 0.31432128] b_f = [ 1.09955776]
W_c max, min, mean, abs_mean: 0.342119 -0.342283 -0.0690545 0.340147
W_f max, min, mean, abs_mean: 0.0931685 -0.0933215 -0.0192439 0.0917445
Epoch 6/300
1s - loss: 1655.1984 - val_loss: 7819.2469
Epoch 00005: val_loss improved from 8197.66046 to 7819.24692, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1594.9626      0.87  0.23  0.69      0.88  0.21  0.71      0.88  0.18  0.74
beijing_tanh+hardsigmoid2 7819.2469      0.84  0.14  0.74      0.84  0.11  0.76      0.87  0.08  0.81
forget mean min: 0.907084 0.327513
incx.max(), incx.min(), incx.mean() 7.2011 -6.57947 4.55206
fgtx.max(), fgtx.min(), fgtx.mean() 1.94847 -1.95334 1.19842
abs_mean, abs_mean+, abs_mean-: 10.2798 6.21334 23.7982
U_c = [[-0.01824933]] U_f = [[ 0.]] b_c = [ 0.31942609] b_f = [ 1.09091115]
W_c max, min, mean, abs_mean: 0.347803 -0.347966 -0.0701904 0.34583
W_f max, min, mean, abs_mean: 0.0993539 -0.0995047 -0.0204777 0.0979179
Epoch 7/300
1s - loss: 1649.2182 - val_loss: 7355.8894
Epoch 00006: val_loss improved from 7819.24692 to 7355.88943, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1583.0659      0.89  0.25  0.69      0.89  0.23  0.71      0.90  0.20  0.74
beijing_tanh+hardsigmoid2 7355.8893      0.85  0.14  0.75      0.86  0.11  0.78      0.88  0.08  0.82
forget mean min: 0.910116 0.306849
incx.max(), incx.min(), incx.mean() 7.37157 -6.74235 4.6719
fgtx.max(), fgtx.min(), fgtx.mean() 2.04622 -2.05419 1.2619
abs_mean, abs_mean+, abs_mean-: 10.3839 6.30177 24.4607
U_c = [[-0.01878711]] U_f = [[ 0.]] b_c = [ 0.32832482] b_f = [ 1.08843029]
W_c max, min, mean, abs_mean: 0.356607 -0.356768 -0.0719502 0.354633
W_f max, min, mean, abs_mean: 0.104468 -0.104623 -0.0214979 0.103029
Epoch 8/300
1s - loss: 1635.7176 - val_loss: 7349.9295
Epoch 00007: val_loss improved from 7355.88943 to 7349.92951, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1575.9917      0.89  0.24  0.69      0.89  0.22  0.71      0.90  0.19  0.74
beijing_tanh+hardsigmoid2 7349.9295      0.85  0.13  0.75      0.85  0.10  0.78      0.88  0.08  0.82
forget mean min: 0.907549 0.284921
incx.max(), incx.min(), incx.mean() 7.47027 -6.83889 4.64415
fgtx.max(), fgtx.min(), fgtx.mean() 2.1507 -2.16114 1.29909
abs_mean, abs_mean+, abs_mean-: 10.498 6.30904 25.1631
U_c = [[-0.01645089]] U_f = [[ 0.]] b_c = [ 0.3330186] b_f = [ 1.08574891]
W_c max, min, mean, abs_mean: 0.361815 -0.361973 -0.0729905 0.35984
W_f max, min, mean, abs_mean: 0.109879 -0.110034 -0.0225775 0.108432
Epoch 9/300
1s - loss: 1635.2735 - val_loss: 6968.4219
Epoch 00008: val_loss improved from 7349.92951 to 6968.42193, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1571.0403      0.88  0.24  0.69      0.89  0.22  0.71      0.89  0.19  0.73
beijing_tanh+hardsigmoid2 6968.4220      0.85  0.12  0.76      0.86  0.09  0.79      0.88  0.07  0.83
forget mean min: 0.901709 0.279755
incx.max(), incx.min(), incx.mean() 7.44187 -6.82376 4.44959
fgtx.max(), fgtx.min(), fgtx.mean() 2.17189 -2.18621 1.25776
abs_mean, abs_mean+, abs_mean-: 10.2226 6.17711 23.499
U_c = [[-0.02163205]] U_f = [[ 0.]] b_c = [ 0.33249679] b_f = [ 1.08498681]
W_c max, min, mean, abs_mean: 0.361391 -0.361547 -0.0729044 0.359415
W_f max, min, mean, abs_mean: 0.111243 -0.111407 -0.0228501 0.1098
Epoch 10/300
1s - loss: 1627.8047 - val_loss: 7125.6394
Epoch 00009: val_loss did not improve
Epoch 11/300
1s - loss: 1623.1977 - val_loss: 7137.4772
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1612.8497 - val_loss: 6954.7914
Epoch 00011: val_loss improved from 6968.42193 to 6954.79141, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1532.5520      0.88  0.22  0.70      0.89  0.20  0.72      0.89  0.17  0.75
beijing_tanh+hardsigmoid2 6954.7914      0.85  0.12  0.76      0.85  0.09  0.79      0.88  0.07  0.83
forget mean min: 0.910765 0.270693
incx.max(), incx.min(), incx.mean() 7.87343 -7.19993 4.61883
fgtx.max(), fgtx.min(), fgtx.mean() 2.22279 -2.23263 1.26079
abs_mean, abs_mean+, abs_mean-: 10.0095 6.12205 21.7323
U_c = [[-0.01423743]] U_f = [[ 0.]] b_c = [ 0.35339734] b_f = [ 1.08609629]
W_c max, min, mean, abs_mean: 0.382776 -0.382924 -0.0771784 0.380801
W_f max, min, mean, abs_mean: 0.114011 -0.11418 -0.0233991 0.112558
Epoch 13/300
1s - loss: 1603.0248 - val_loss: 7047.8598
Epoch 00012: val_loss did not improve
Epoch 14/300
1s - loss: 1586.0156 - val_loss: 7099.2272
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1579.6413 - val_loss: 6919.7154
Epoch 00014: val_loss improved from 6954.79141 to 6919.71541, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1509.4330      0.89  0.23  0.70      0.90  0.21  0.72      0.90  0.18  0.75
beijing_tanh+hardsigmoid2 6919.7154      0.85  0.11  0.77      0.85  0.08  0.79      0.88  0.06  0.83
forget mean min: 0.909323 0.254899
incx.max(), incx.min(), incx.mean() 8.35334 -7.62583 4.75434
fgtx.max(), fgtx.min(), fgtx.mean() 2.31028 -2.31763 1.26793
abs_mean, abs_mean+, abs_mean-: 10.3546 6.2978 22.4993
U_c = [[-0.0089505]] U_f = [[ 0.]] b_c = [ 0.37643892] b_f = [ 1.0921222]
W_c max, min, mean, abs_mean: 0.405791 -0.405933 -0.0817817 0.403821
W_f max, min, mean, abs_mean: 0.118414 -0.118593 -0.0242798 0.116955
Epoch 16/300
1s - loss: 1581.0847 - val_loss: 7229.5575
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1573.7336 - val_loss: 7196.7398
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1572.8059 - val_loss: 7213.6669
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1567.2985 - val_loss: 7210.0468
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1559.2748 - val_loss: 7169.0630
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1560.1802 - val_loss: 7421.5700
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1552.4659 - val_loss: 7543.7621
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1547.3310 - val_loss: 7756.0373
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1548.9630 - val_loss: 7435.8174
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1528.3006 - val_loss: 7506.3414
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1508.9819 - val_loss: 7473.6806
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1490.5047 - val_loss: 7368.1838
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1457.6432 - val_loss: 7381.6773
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1419.3425 - val_loss: 7241.4368
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1387.8129 - val_loss: 7519.0534
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1350.5802 - val_loss: 7968.3649
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1314.6014 - val_loss: 7753.3703
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1277.0332 - val_loss: 7949.6604
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1245.0479 - val_loss: 7504.3133
Epoch 00033: val_loss did not improve
Epoch 35/300
1s - loss: 1217.0604 - val_loss: 6951.9606
Epoch 00034: val_loss did not improve
Epoch 36/300
1s - loss: 1195.8598 - val_loss: 6830.1913
Epoch 00035: val_loss improved from 6919.71541 to 6830.19132, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1123.9830      0.89  0.20  0.73      0.90  0.18  0.76      0.91  0.15  0.79
beijing_tanh+hardsigmoid2 6830.1913      0.88  0.10  0.80      0.88  0.07  0.82      0.90  0.05  0.86
forget mean min: 0.902764 0.127105
incx.max(), incx.min(), incx.mean() 12.1246 -10.1169 5.54383
fgtx.max(), fgtx.min(), fgtx.mean() 3.17239 -2.93297 1.36655
abs_mean, abs_mean+, abs_mean-: 11.1987 7.54302 18.8381
U_c = [[-0.00731317]] U_f = [[ 0.]] b_c = [ 0.56778461] b_f = [ 1.06849313]
W_c max, min, mean, abs_mean: 0.603015 -0.607695 -0.122364 0.601111
W_f max, min, mean, abs_mean: 0.167859 -0.168357 -0.0336739 0.165004
Epoch 37/300
1s - loss: 1176.2387 - val_loss: 6553.5248
Epoch 00036: val_loss improved from 6830.19132 to 6553.52476, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1118.9961      0.89  0.20  0.73      0.91  0.17  0.76      0.91  0.14  0.79
beijing_tanh+hardsigmoid2 6553.5247      0.89  0.11  0.80      0.89  0.07  0.83      0.91  0.05  0.87
forget mean min: 0.905728 0.121071
incx.max(), incx.min(), incx.mean() 12.4126 -10.2864 5.69452
fgtx.max(), fgtx.min(), fgtx.mean() 3.22172 -2.95975 1.39283
abs_mean, abs_mean+, abs_mean-: 11.3135 7.6237 19.3932
U_c = [[-0.00793453]] U_f = [[ 0.]] b_c = [ 0.58221769] b_f = [ 1.06510806]
W_c max, min, mean, abs_mean: 0.617968 -0.622873 -0.125421 0.616134
W_f max, min, mean, abs_mean: 0.170719 -0.17125 -0.0342406 0.167786
Epoch 38/300
1s - loss: 1158.6374 - val_loss: 6459.2126
Epoch 00037: val_loss improved from 6553.52476 to 6459.21264, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1077.1465      0.89  0.19  0.74      0.91  0.16  0.77      0.91  0.14  0.80
beijing_tanh+hardsigmoid2 6459.2126      0.89  0.10  0.81      0.89  0.07  0.84      0.90  0.05  0.86
forget mean min: 0.902475 0.123434
incx.max(), incx.min(), incx.mean() 12.6825 -10.4378 5.70957
fgtx.max(), fgtx.min(), fgtx.mean() 3.22502 -2.94418 1.36499
abs_mean, abs_mean+, abs_mean-: 11.2644 7.73782 18.4416
U_c = [[-0.01093364]] U_f = [[ 0.]] b_c = [ 0.59616685] b_f = [ 1.06134927]
W_c max, min, mean, abs_mean: 0.631372 -0.636609 -0.128167 0.629606
W_f max, min, mean, abs_mean: 0.171128 -0.171315 -0.0343467 0.167997
Epoch 39/300
1s - loss: 1151.1833 - val_loss: 6678.4839
Epoch 00038: val_loss did not improve
Epoch 40/300
1s - loss: 1145.8789 - val_loss: 6089.9717
Epoch 00039: val_loss improved from 6459.21264 to 6089.97174, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1060.0894      0.89  0.18  0.74      0.91  0.16  0.78      0.91  0.13  0.80
beijing_tanh+hardsigmoid2 6089.9717      0.90  0.10  0.81      0.90  0.07  0.85      0.92  0.05  0.87
forget mean min: 0.902614 0.119853
incx.max(), incx.min(), incx.mean() 13.0611 -10.6367 5.83135
fgtx.max(), fgtx.min(), fgtx.mean() 3.27227 -2.95839 1.37201
abs_mean, abs_mean+, abs_mean-: 11.3373 7.8685 18.4406
U_c = [[-0.01185522]] U_f = [[ 0.]] b_c = [ 0.61533278] b_f = [ 1.0576582]
W_c max, min, mean, abs_mean: 0.650599 -0.656799 -0.132138 0.649031
W_f max, min, mean, abs_mean: 0.174456 -0.174429 -0.0348558 0.170643
Epoch 41/300
1s - loss: 1130.5670 - val_loss: 5872.4475
Epoch 00040: val_loss improved from 6089.97174 to 5872.44749, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1077.1534      0.90  0.19  0.74      0.91  0.16  0.77      0.91  0.14  0.80
beijing_tanh+hardsigmoid2 5872.4475      0.90  0.10  0.82      0.91  0.07  0.85      0.92  0.05  0.88
forget mean min: 0.902731 0.116886
incx.max(), incx.min(), incx.mean() 13.2427 -10.7601 5.94181
fgtx.max(), fgtx.min(), fgtx.mean() 3.29274 -2.97103 1.38811
abs_mean, abs_mean+, abs_mean-: 11.5228 8.0363 18.8561
U_c = [[-0.01250272]] U_f = [[ 0.]] b_c = [ 0.62494028] b_f = [ 1.05546236]
W_c max, min, mean, abs_mean: 0.659151 -0.665765 -0.133899 0.657699
W_f max, min, mean, abs_mean: 0.175865 -0.175658 -0.0350606 0.171633
Epoch 42/300
1s - loss: 1127.1715 - val_loss: 6004.2698
Epoch 00041: val_loss did not improve
Epoch 43/300
1s - loss: 1118.1832 - val_loss: 6006.0527
Epoch 00042: val_loss did not improve
Epoch 44/300
1s - loss: 1106.7176 - val_loss: 6099.1415
Epoch 00043: val_loss did not improve
Epoch 45/300
1s - loss: 1099.6774 - val_loss: 5666.8970
Epoch 00044: val_loss improved from 5872.44749 to 5666.89703, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1029.8811      0.89  0.18  0.75      0.91  0.15  0.79      0.91  0.12  0.81
beijing_tanh+hardsigmoid2 5666.8970      0.90  0.09  0.83      0.91  0.06  0.86      0.92  0.05  0.88
forget mean min: 0.898646 0.107777
incx.max(), incx.min(), incx.mean() 13.7173 -10.8782 5.96434
fgtx.max(), fgtx.min(), fgtx.mean() 3.41152 -3.01002 1.38806
abs_mean, abs_mean+, abs_mean-: 11.5982 8.22124 18.2458
U_c = [[-0.01253551]] U_f = [[ 0.]] b_c = [ 0.6505186] b_f = [ 1.04890883]
W_c max, min, mean, abs_mean: 0.683762 -0.691257 -0.138788 0.681538
W_f max, min, mean, abs_mean: 0.18382 -0.183654 -0.0364086 0.17794
Epoch 46/300
1s - loss: 1096.4057 - val_loss: 5653.7850
Epoch 00045: val_loss improved from 5666.89703 to 5653.78501, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1016.4798      0.90  0.17  0.75      0.92  0.15  0.79      0.92  0.11  0.82
beijing_tanh+hardsigmoid2 5653.7850      0.90  0.09  0.83      0.91  0.06  0.86      0.92  0.04  0.88
forget mean min: 0.901075 0.100401
incx.max(), incx.min(), incx.mean() 13.7867 -10.8817 5.99728
fgtx.max(), fgtx.min(), fgtx.mean() 3.46855 -3.04681 1.41208
abs_mean, abs_mean+, abs_mean-: 11.606 8.21643 18.329
U_c = [[-0.0136425]] U_f = [[ 0.]] b_c = [ 0.65391457] b_f = [ 1.04880965]
W_c max, min, mean, abs_mean: 0.686875 -0.694672 -0.139389 0.684283
W_f max, min, mean, abs_mean: 0.186955 -0.186724 -0.0369312 0.180731
Epoch 47/300
1s - loss: 1089.1784 - val_loss: 5618.6609
Epoch 00046: val_loss improved from 5653.78501 to 5618.66086, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1003.4575      0.89  0.16  0.76      0.91  0.13  0.80      0.91  0.10  0.83
beijing_tanh+hardsigmoid2 5618.6608      0.90  0.09  0.83      0.90  0.05  0.86      0.92  0.04  0.88
forget mean min: 0.899946 0.104623
incx.max(), incx.min(), incx.mean() 13.8595 -10.8256 5.96963
fgtx.max(), fgtx.min(), fgtx.mean() 3.47585 -3.02356 1.39944
abs_mean, abs_mean+, abs_mean-: 11.5514 8.21367 17.9751
U_c = [[-0.01297543]] U_f = [[ 0.]] b_c = [ 0.65779305] b_f = [ 1.04667473]
W_c max, min, mean, abs_mean: 0.691665 -0.69977 -0.140314 0.688653
W_f max, min, mean, abs_mean: 0.188128 -0.187844 -0.0370464 0.181317
Epoch 48/300
1s - loss: 1082.5107 - val_loss: 5472.8400
Epoch 00047: val_loss improved from 5618.66086 to 5472.84003, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2 1016.5929      0.90  0.18  0.75      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid2 5472.8400      0.91  0.09  0.83      0.91  0.06  0.86      0.93  0.04  0.89
forget mean min: 0.89942 0.0932518
incx.max(), incx.min(), incx.mean() 13.9724 -10.86 6.03424
fgtx.max(), fgtx.min(), fgtx.mean() 3.55742 -3.0804 1.43649
abs_mean, abs_mean+, abs_mean-: 11.7064 8.29994 18.6409
U_c = [[-0.0135846]] U_f = [[ 0.]] b_c = [ 0.66354281] b_f = [ 1.04665887]
W_c max, min, mean, abs_mean: 0.698014 -0.706421 -0.141542 0.694558
W_f max, min, mean, abs_mean: 0.192929 -0.192758 -0.0379878 0.185658
Epoch 49/300
1s - loss: 1079.0758 - val_loss: 5492.2718
Epoch 00048: val_loss did not improve
Epoch 50/300
1s - loss: 1069.7083 - val_loss: 5400.7473
Epoch 00049: val_loss improved from 5472.84003 to 5400.74726, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2  991.0761      0.89  0.16  0.76      0.91  0.13  0.80      0.91  0.10  0.83
beijing_tanh+hardsigmoid2 5400.7471      0.91  0.08  0.84      0.91  0.06  0.86      0.93  0.04  0.89
forget mean min: 0.900207 0.0911148
incx.max(), incx.min(), incx.mean() 14.2016 -10.7795 6.07074
fgtx.max(), fgtx.min(), fgtx.mean() 3.64425 -3.08587 1.45498
abs_mean, abs_mean+, abs_mean-: 11.7328 8.29693 18.6345
U_c = [[-0.01321757]] U_f = [[ 0.]] b_c = [ 0.67421037] b_f = [ 1.04144216]
W_c max, min, mean, abs_mean: 0.709541 -0.718749 -0.143768 0.705085
W_f max, min, mean, abs_mean: 0.197973 -0.19777 -0.0388706 0.189953
Epoch 51/300
1s - loss: 1066.3887 - val_loss: 5404.5356
Epoch 00050: val_loss did not improve
Epoch 52/300
1s - loss: 1059.0589 - val_loss: 5522.1867
Epoch 00051: val_loss did not improve
Epoch 53/300
1s - loss: 1052.0226 - val_loss: 5357.7570
Epoch 00052: val_loss improved from 5400.74726 to 5357.75697, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2  984.0937      0.89  0.16  0.77      0.91  0.12  0.81      0.91  0.10  0.82
beijing_tanh+hardsigmoid2 5357.7570      0.91  0.08  0.84      0.91  0.06  0.87      0.93  0.04  0.89
forget mean min: 0.895847 0.0881092
incx.max(), incx.min(), incx.mean() 14.5248 -10.781 6.12924
fgtx.max(), fgtx.min(), fgtx.mean() 3.72664 -3.09151 1.46648
abs_mean, abs_mean+, abs_mean-: 11.954 8.46889 19.1304
U_c = [[-0.0119414]] U_f = [[ 0.]] b_c = [ 0.6923666] b_f = [ 1.03205788]
W_c max, min, mean, abs_mean: 0.72908 -0.738865 -0.147335 0.721885
W_f max, min, mean, abs_mean: 0.204112 -0.203655 -0.0397407 0.194492
Epoch 54/300
1s - loss: 1044.2882 - val_loss: 5259.6671
Epoch 00053: val_loss improved from 5357.75697 to 5259.66707, saving model to beijing_tanh+hardsigmoid2_weights.hdf5
beijing_tanh+hardsigmoid2  987.8508      0.90  0.17  0.76      0.92  0.14  0.80      0.92  0.11  0.83
beijing_tanh+hardsigmoid2 5259.6670      0.91  0.09  0.84      0.92  0.06  0.87      0.94  0.04  0.90
forget mean min: 0.902411 0.0810632
incx.max(), incx.min(), incx.mean() 14.629 -10.7114 6.28825
fgtx.max(), fgtx.min(), fgtx.mean() 3.81495 -3.12563 1.53257
abs_mean, abs_mean+, abs_mean-: 12.1074 8.48589 20.1734
U_c = [[-0.0133762]] U_f = [[ 0.]] b_c = [ 0.69940996] b_f = [ 1.03094757]
W_c max, min, mean, abs_mean: 0.735316 -0.745107 -0.148416 0.726839
W_f max, min, mean, abs_mean: 0.208831 -0.208374 -0.0406762 0.19907
Epoch 55/300
1s - loss: 1039.1692 - val_loss: 5458.2147
Epoch 00054: val_loss did not improve
Epoch 56/300
1s - loss: 1029.9232 - val_loss: 5328.7530
Epoch 00055: val_loss did not improve
Epoch 57/300
1s - loss: 1023.5296 - val_loss: 5331.9796
Epoch 00056: val_loss did not improve
Epoch 58/300
1s - loss: 1017.5897 - val_loss: 5325.5043
Epoch 00057: val_loss did not improve
Epoch 59/300
1s - loss: 1010.7652 - val_loss: 5535.8582
Epoch 00058: val_loss did not improve
Epoch 60/300
1s - loss: 1009.9604 - val_loss: 5463.4968
Epoch 00059: val_loss did not improve
Epoch 61/300
1s - loss: 1005.8567 - val_loss: 5670.3314
Epoch 00060: val_loss did not improve
Epoch 62/300
1s - loss: 996.0668 - val_loss: 5573.5304
Epoch 00061: val_loss did not improve
Epoch 63/300
1s - loss: 992.7330 - val_loss: 5743.0015
Epoch 00062: val_loss did not improve
Epoch 64/300
1s - loss: 989.5347 - val_loss: 5957.4528
Epoch 00063: val_loss did not improve
Epoch 65/300
1s - loss: 984.5107 - val_loss: 5536.1601
Epoch 00064: val_loss did not improve
Epoch 66/300
1s - loss: 981.6118 - val_loss: 6445.0933
Epoch 00065: val_loss did not improve
Epoch 67/300
1s - loss: 966.3439 - val_loss: 6394.7817
Epoch 00066: val_loss did not improve
Epoch 68/300
1s - loss: 965.4124 - val_loss: 5893.6382
Epoch 00067: val_loss did not improve
Epoch 69/300
1s - loss: 961.2564 - val_loss: 6332.7395
Epoch 00068: val_loss did not improve
Epoch 70/300
1s - loss: 956.4048 - val_loss: 6325.2196
Epoch 00069: val_loss did not improve
Epoch 71/300
1s - loss: 957.6057 - val_loss: 6427.7676
Epoch 00070: val_loss did not improve
Epoch 72/300
1s - loss: 948.6321 - val_loss: 6054.1743
Epoch 00071: val_loss did not improve
Epoch 73/300
1s - loss: 940.3262 - val_loss: 7046.3590
Epoch 00072: val_loss did not improve
Epoch 74/300
1s - loss: 937.1189 - val_loss: 6256.8789
Epoch 00073: val_loss did not improve
Epoch 75/300
1s - loss: 933.4508 - val_loss: 6353.3486
Epoch 00074: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid3
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid3 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid329822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5900.7594 - val_loss: 9573.2399
Epoch 00000: val_loss improved from inf to 9573.23993, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 3320.3055      0.55  0.22  0.48      0.55  0.20  0.49      0.52  0.18  0.48
beijing_tanh+hardsigmoid3 9573.2399      0.79  0.09  0.74      0.79  0.05  0.76      0.80  0.03  0.78
forget mean min: 0.844388 0.31603
incx.max(), incx.min(), incx.mean() 2.94024 -2.7659 1.35797
fgtx.max(), fgtx.min(), fgtx.mean() 1.9643 -2.01409 0.861121
abs_mean, abs_mean+, abs_mean-: 6.56818 2.61287 13.2536
U_c = [[-0.06245899]] U_f = [[ 0.]] b_c = [ 0.1228781] b_f = [ 1.0942378]
W_c max, min, mean, abs_mean: 0.149351 -0.149595 -0.000406031 0.148469
W_f max, min, mean, abs_mean: 0.104047 -0.104132 -0.000200982 0.103514
Epoch 2/300
1s - loss: 2454.0823 - val_loss: 10495.8378
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1872.7533 - val_loss: 9632.5875
Epoch 00002: val_loss did not improve
Epoch 4/300
1s - loss: 1735.8000 - val_loss: 8555.9026
Epoch 00003: val_loss improved from 9573.23993 to 8555.90257, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1648.5564      0.87  0.24  0.68      0.87  0.21  0.70      0.87  0.18  0.73
beijing_tanh+hardsigmoid3 8555.9026      0.83  0.15  0.73      0.84  0.12  0.75      0.86  0.09  0.80
forget mean min: 0.91355 0.384401
incx.max(), incx.min(), incx.mean() 6.80864 -6.2129 4.4764
fgtx.max(), fgtx.min(), fgtx.mean() 1.68569 -1.68696 1.08162
abs_mean, abs_mean+, abs_mean-: 9.63332 5.88524 21.271
U_c = [[-0.01645068]] U_f = [[ 0.]] b_c = [ 0.30032301] b_f = [ 1.10896409]
W_c max, min, mean, abs_mean: 0.327643 -0.327899 -0.000407769 0.326759
W_f max, min, mean, abs_mean: 0.0852467 -0.085256 -0.000195979 0.0846323
Epoch 5/300
1s - loss: 1677.7583 - val_loss: 8198.2361
Epoch 00004: val_loss improved from 8555.90257 to 8198.23605, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1610.2380      0.88  0.24  0.69      0.89  0.22  0.70      0.89  0.19  0.73
beijing_tanh+hardsigmoid3 8198.2361      0.84  0.15  0.73      0.84  0.12  0.76      0.87  0.09  0.80
forget mean min: 0.913235 0.343061
incx.max(), incx.min(), incx.mean() 7.13628 -6.51602 4.63303
fgtx.max(), fgtx.min(), fgtx.mean() 1.88423 -1.88705 1.19274
abs_mean, abs_mean+, abs_mean-: 10.1763 6.1791 23.638
U_c = [[-0.01619489]] U_f = [[ 0.]] b_c = [ 0.31521496] b_f = [ 1.10235119]
W_c max, min, mean, abs_mean: 0.343421 -0.34368 -0.000408427 0.342537
W_f max, min, mean, abs_mean: 0.0952457 -0.0952503 -0.000198449 0.0946217
Epoch 6/300
1s - loss: 1661.9600 - val_loss: 7794.6597
Epoch 00005: val_loss improved from 8198.23605 to 7794.65968, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1598.5879      0.87  0.23  0.69      0.88  0.21  0.71      0.88  0.18  0.74
beijing_tanh+hardsigmoid3 7794.6597      0.84  0.14  0.73      0.84  0.11  0.76      0.87  0.08  0.80
forget mean min: 0.909432 0.327752
incx.max(), incx.min(), incx.mean() 7.28252 -6.65848 4.5964
fgtx.max(), fgtx.min(), fgtx.mean() 1.9476 -1.95303 1.19604
abs_mean, abs_mean+, abs_mean-: 10.1453 6.19262 22.5727
U_c = [[-0.01775876]] U_f = [[ 0.]] b_c = [ 0.32171428] b_f = [ 1.09178793]
W_c max, min, mean, abs_mean: 0.350979 -0.351239 -0.000409211 0.350094
W_f max, min, mean, abs_mean: 0.0985922 -0.0985894 -0.000200949 0.0979548
Epoch 7/300
1s - loss: 1644.7170 - val_loss: 7326.0811
Epoch 00006: val_loss improved from 7794.65968 to 7326.08114, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1576.5578      0.89  0.24  0.69      0.89  0.23  0.71      0.89  0.19  0.73
beijing_tanh+hardsigmoid3 7326.0811      0.85  0.13  0.75      0.85  0.10  0.78      0.88  0.08  0.82
forget mean min: 0.90682 0.297328
incx.max(), incx.min(), incx.mean() 7.34417 -6.72615 4.57271
fgtx.max(), fgtx.min(), fgtx.mean() 2.09418 -2.10384 1.26729
abs_mean, abs_mean+, abs_mean-: 10.3477 6.25619 24.2768
U_c = [[-0.01874956]] U_f = [[ 0.]] b_c = [ 0.32518259] b_f = [ 1.09047425]
W_c max, min, mean, abs_mean: 0.354558 -0.354821 -0.000409767 0.353673
W_f max, min, mean, abs_mean: 0.10617 -0.106161 -0.000202833 0.105522
Epoch 8/300
1s - loss: 1638.1794 - val_loss: 7277.1242
Epoch 00007: val_loss improved from 7326.08114 to 7277.12416, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1570.5763      0.88  0.24  0.69      0.88  0.21  0.71      0.89  0.18  0.74
beijing_tanh+hardsigmoid3 7277.1242      0.85  0.13  0.75      0.85  0.10  0.77      0.88  0.08  0.82
forget mean min: 0.90883 0.289777
incx.max(), incx.min(), incx.mean() 7.50194 -6.87312 4.61221
fgtx.max(), fgtx.min(), fgtx.mean() 2.12701 -2.13795 1.26965
abs_mean, abs_mean+, abs_mean-: 10.2751 6.21492 23.5501
U_c = [[-0.01755367]] U_f = [[ 0.]] b_c = [ 0.33284935] b_f = [ 1.0868336]
W_c max, min, mean, abs_mean: 0.362611 -0.362877 -0.000410485 0.361726
W_f max, min, mean, abs_mean: 0.107979 -0.107964 -0.000204096 0.107321
Epoch 9/300
1s - loss: 1625.5044 - val_loss: 7101.6320
Epoch 00008: val_loss improved from 7277.12416 to 7101.63205, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1564.8584      0.88  0.23  0.69      0.89  0.21  0.71      0.89  0.18  0.74
beijing_tanh+hardsigmoid3 7101.6320      0.85  0.13  0.76      0.86  0.10  0.78      0.88  0.07  0.83
forget mean min: 0.910074 0.271431
incx.max(), incx.min(), incx.mean() 7.52867 -6.90078 4.57305
fgtx.max(), fgtx.min(), fgtx.mean() 2.21761 -2.23022 1.30655
abs_mean, abs_mean+, abs_mean-: 10.1657 6.1348 23.3743
U_c = [[-0.01669061]] U_f = [[ 0.]] b_c = [ 0.33439016] b_f = [ 1.08737111]
W_c max, min, mean, abs_mean: 0.364364 -0.364631 -0.00041099 0.363479
W_f max, min, mean, abs_mean: 0.112706 -0.112687 -0.000205331 0.112041
Epoch 10/300
1s - loss: 1620.5915 - val_loss: 7067.9382
Epoch 00009: val_loss improved from 7101.63205 to 7067.93818, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1560.6016      0.89  0.24  0.69      0.89  0.22  0.71      0.90  0.19  0.74
beijing_tanh+hardsigmoid3 7067.9382      0.85  0.13  0.76      0.85  0.09  0.78      0.87  0.07  0.82
forget mean min: 0.905318 0.267155
incx.max(), incx.min(), incx.mean() 7.64203 -7.00249 4.50845
fgtx.max(), fgtx.min(), fgtx.mean() 2.23896 -2.25138 1.27813
abs_mean, abs_mean+, abs_mean-: 10.1858 6.17394 23.0279
U_c = [[-0.0188152]] U_f = [[ 0.]] b_c = [ 0.34001473] b_f = [ 1.08715427]
W_c max, min, mean, abs_mean: 0.370116 -0.370385 -0.000411481 0.36923
W_f max, min, mean, abs_mean: 0.113891 -0.113865 -0.000206181 0.113214
Epoch 11/300
1s - loss: 1614.7332 - val_loss: 7088.2645
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1602.2054 - val_loss: 7040.0932
Epoch 00011: val_loss improved from 7067.93818 to 7040.09319, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1528.7839      0.89  0.23  0.70      0.90  0.21  0.72      0.90  0.18  0.75
beijing_tanh+hardsigmoid3 7040.0932      0.84  0.12  0.76      0.85  0.09  0.78      0.87  0.06  0.83
forget mean min: 0.909256 0.261479
incx.max(), incx.min(), incx.mean() 7.99303 -7.3079 4.62783
fgtx.max(), fgtx.min(), fgtx.mean() 2.27345 -2.28124 1.27171
abs_mean, abs_mean+, abs_mean-: 10.1019 6.12381 22.0442
U_c = [[-0.00997063]] U_f = [[ 0.]] b_c = [ 0.35566777] b_f = [ 1.08863771]
W_c max, min, mean, abs_mean: 0.386911 -0.387179 -0.00041206 0.386022
W_f max, min, mean, abs_mean: 0.115592 -0.115559 -0.000206529 0.114909
Epoch 13/300
1s - loss: 1590.1362 - val_loss: 6931.4396
Epoch 00012: val_loss improved from 7040.09319 to 6931.43956, saving model to beijing_tanh+hardsigmoid3_weights.hdf5
beijing_tanh+hardsigmoid3 1508.7489      0.88  0.23  0.70      0.89  0.20  0.73      0.89  0.17  0.75
beijing_tanh+hardsigmoid3 6931.4395      0.84  0.11  0.76      0.84  0.08  0.78      0.87  0.06  0.83
forget mean min: 0.907469 0.267745
incx.max(), incx.min(), incx.mean() 8.14567 -7.44364 4.63373
fgtx.max(), fgtx.min(), fgtx.mean() 2.2434 -2.25061 1.23099
abs_mean, abs_mean+, abs_mean-: 10.1031 6.13521 21.6122
U_c = [[-0.01082286]] U_f = [[ 0.]] b_c = [ 0.36352196] b_f = [ 1.08933437]
W_c max, min, mean, abs_mean: 0.394568 -0.394835 -0.000412104 0.393677
W_f max, min, mean, abs_mean: 0.11418 -0.11414 -0.000206615 0.113487
Epoch 14/300
1s - loss: 1579.8083 - val_loss: 7355.5945
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1581.5131 - val_loss: 7071.2251
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1578.7676 - val_loss: 7163.4229
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1568.2174 - val_loss: 7349.4577
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1565.5411 - val_loss: 7461.1725
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1556.7404 - val_loss: 7466.5612
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1563.6643 - val_loss: 7487.5175
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1552.7537 - val_loss: 7451.3471
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1546.5759 - val_loss: 7418.7048
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1538.1933 - val_loss: 7471.2903
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1510.6907 - val_loss: 7474.3836
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1497.1302 - val_loss: 7534.5146
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1470.8967 - val_loss: 7183.3653
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1436.8389 - val_loss: 7623.5186
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1404.4079 - val_loss: 8263.0940
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1377.1307 - val_loss: 8398.6406
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1352.4907 - val_loss: 8140.8832
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1327.0861 - val_loss: 8762.1538
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1300.0373 - val_loss: 9005.8529
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1282.3295 - val_loss: 8367.1044
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1260.1470 - val_loss: 8787.6336
Epoch 00033: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid4
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid4 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid429822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5901.5803 - val_loss: 9681.3953
Epoch 00000: val_loss improved from inf to 9681.39527, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 3295.0632      0.55  0.21  0.48      0.55  0.20  0.49      0.53  0.18  0.48
beijing_tanh+hardsigmoid4 9681.3953      0.79  0.09  0.73      0.79  0.05  0.75      0.80  0.03  0.77
forget mean min: 0.847023 0.317324
incx.max(), incx.min(), incx.mean() 2.91456 -2.74594 1.3553
fgtx.max(), fgtx.min(), fgtx.mean() 1.95473 -2.00871 0.862949
abs_mean, abs_mean+, abs_mean-: 6.46556 2.60656 12.5815
U_c = [[-0.06025401]] U_f = [[ 0.]] b_c = [ 0.12285119] b_f = [ 1.09532416]
W_c max, min, mean, abs_mean: 0.149736 -0.148846 0.0738845 0.147514
W_f max, min, mean, abs_mean: 0.104351 -0.104235 0.0514906 0.103288
Epoch 2/300
1s - loss: 2446.0957 - val_loss: 10659.3398
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1876.3274 - val_loss: 9468.8317
Epoch 00002: val_loss improved from 9681.39527 to 9468.83175, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1743.6526      0.86  0.25  0.67      0.86  0.23  0.69      0.87  0.20  0.72
beijing_tanh+hardsigmoid4 9468.8316      0.80  0.15  0.70      0.80  0.12  0.73      0.83  0.09  0.77
forget mean min: 0.909419 0.414051
incx.max(), incx.min(), incx.mean() 6.29425 -5.74097 4.1311
fgtx.max(), fgtx.min(), fgtx.mean() 1.54136 -1.54178 0.987207
abs_mean, abs_mean+, abs_mean-: 8.88839 5.38595 18.3602
U_c = [[-0.01267933]] U_f = [[ 0.]] b_c = [ 0.27746627] b_f = [ 1.11203885]
W_c max, min, mean, abs_mean: 0.304306 -0.303391 0.151163 0.302072
W_f max, min, mean, abs_mean: 0.0784502 -0.078393 0.038533 0.077384
Epoch 4/300
1s - loss: 1739.6542 - val_loss: 8814.7968
Epoch 00003: val_loss improved from 9468.83175 to 8814.79682, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1658.1860      0.87  0.25  0.68      0.88  0.22  0.70      0.88  0.19  0.73
beijing_tanh+hardsigmoid4 8814.7967      0.83  0.16  0.72      0.84  0.13  0.75      0.86  0.10  0.79
forget mean min: 0.913546 0.37971
incx.max(), incx.min(), incx.mean() 6.86648 -6.26195 4.53079
fgtx.max(), fgtx.min(), fgtx.mean() 1.70968 -1.71071 1.10115
abs_mean, abs_mean+, abs_mean-: 9.84144 5.99618 22.4809
U_c = [[-0.01671174]] U_f = [[ 0.]] b_c = [ 0.30423954] b_f = [ 1.10926151]
W_c max, min, mean, abs_mean: 0.331556 -0.330634 0.164786 0.329319
W_f max, min, mean, abs_mean: 0.0868812 -0.0868182 0.04274 0.0857985
Epoch 5/300
1s - loss: 1680.3810 - val_loss: 8129.3790
Epoch 00004: val_loss improved from 8814.79682 to 8129.37897, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1640.1159      0.89  0.26  0.68      0.90  0.24  0.70      0.90  0.21  0.72
beijing_tanh+hardsigmoid4 8129.3790      0.85  0.14  0.74      0.85  0.11  0.77      0.88  0.09  0.81
forget mean min: 0.908839 0.342892
incx.max(), incx.min(), incx.mean() 7.18752 -6.55967 4.62832
fgtx.max(), fgtx.min(), fgtx.mean() 1.88655 -1.88919 1.18365
abs_mean, abs_mean+, abs_mean-: 10.3547 6.30415 24.2736
U_c = [[-0.01699812]] U_f = [[ 0.]] b_c = [ 0.31873399] b_f = [ 1.10365236]
W_c max, min, mean, abs_mean: 0.346943 -0.346015 0.172477 0.344703
W_f max, min, mean, abs_mean: 0.0957763 -0.0957066 0.0471779 0.0946746
Epoch 6/300
1s - loss: 1657.5616 - val_loss: 7538.8291
Epoch 00005: val_loss improved from 8129.37897 to 7538.82911, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1595.2179      0.89  0.25  0.68      0.89  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid4 7538.8291      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.08  0.82
forget mean min: 0.905641 0.311956
incx.max(), incx.min(), incx.mean() 7.2816 -6.65373 4.57811
fgtx.max(), fgtx.min(), fgtx.mean() 2.02873 -2.03401 1.24054
abs_mean, abs_mean+, abs_mean-: 10.4059 6.30137 24.3636
U_c = [[-0.02033924]] U_f = [[ 0.]] b_c = [ 0.32298774] b_f = [ 1.09378803]
W_c max, min, mean, abs_mean: 0.351928 -0.350993 0.174966 0.349684
W_f max, min, mean, abs_mean: 0.103073 -0.102994 0.050814 0.101948
Epoch 7/300
1s - loss: 1648.0739 - val_loss: 7183.1045
Epoch 00006: val_loss improved from 7538.82911 to 7183.10450, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1592.5820      0.89  0.25  0.68      0.89  0.23  0.70      0.90  0.20  0.73
beijing_tanh+hardsigmoid4 7183.1046      0.85  0.13  0.76      0.86  0.10  0.78      0.88  0.07  0.83
forget mean min: 0.902173 0.291042
incx.max(), incx.min(), incx.mean() 7.3735 -6.74416 4.55719
fgtx.max(), fgtx.min(), fgtx.mean() 2.12972 -2.13757 1.27844
abs_mean, abs_mean+, abs_mean-: 10.4971 6.32815 25.1074
U_c = [[-0.021682]] U_f = [[ 0.]] b_c = [ 0.32766876] b_f = [ 1.09278512]
W_c max, min, mean, abs_mean: 0.356777 -0.355836 0.177388 0.354529
W_f max, min, mean, abs_mean: 0.1083 -0.10822 0.0534206 0.107162
Epoch 8/300
1s - loss: 1636.4059 - val_loss: 7150.6861
Epoch 00007: val_loss improved from 7183.10450 to 7150.68605, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1603.3732      0.90  0.26  0.68      0.90  0.24  0.70      0.90  0.21  0.73
beijing_tanh+hardsigmoid4 7150.6860      0.85  0.13  0.76      0.86  0.10  0.78      0.88  0.07  0.83
forget mean min: 0.898781 0.272638
incx.max(), incx.min(), incx.mean() 7.48008 -6.8504 4.54266
fgtx.max(), fgtx.min(), fgtx.mean() 2.21691 -2.22828 1.30574
abs_mean, abs_mean+, abs_mean-: 10.5949 6.39876 25.117
U_c = [[-0.01995748]] U_f = [[ 0.]] b_c = [ 0.33316943] b_f = [ 1.09146726]
W_c max, min, mean, abs_mean: 0.36248 -0.361533 0.180236 0.360226
W_f max, min, mean, abs_mean: 0.11289 -0.112808 0.0557087 0.111739
Epoch 9/300
1s - loss: 1634.6296 - val_loss: 7042.7078
Epoch 00008: val_loss improved from 7150.68605 to 7042.70778, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1573.4324      0.89  0.25  0.69      0.90  0.23  0.70      0.90  0.20  0.73
beijing_tanh+hardsigmoid4 7042.7078      0.85  0.13  0.76      0.86  0.10  0.79      0.88  0.07  0.83
forget mean min: 0.903949 0.26688
incx.max(), incx.min(), incx.mean() 7.54686 -6.91552 4.56117
fgtx.max(), fgtx.min(), fgtx.mean() 2.24109 -2.25457 1.31298
abs_mean, abs_mean+, abs_mean-: 10.4252 6.27848 24.6352
U_c = [[-0.01986031]] U_f = [[ 0.]] b_c = [ 0.33735055] b_f = [ 1.08896589]
W_c max, min, mean, abs_mean: 0.366383 -0.365431 0.182186 0.364126
W_f max, min, mean, abs_mean: 0.11435 -0.114267 0.0564337 0.113189
Epoch 10/300
1s - loss: 1632.7861 - val_loss: 7278.3085
Epoch 00009: val_loss did not improve
Epoch 11/300
1s - loss: 1623.3578 - val_loss: 7127.6193
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1620.9574 - val_loss: 6977.5130
Epoch 00011: val_loss improved from 7042.70778 to 6977.51301, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1551.6580      0.88  0.23  0.70      0.89  0.21  0.72      0.89  0.18  0.74
beijing_tanh+hardsigmoid4 6977.5131      0.85  0.12  0.76      0.85  0.09  0.79      0.88  0.06  0.83
forget mean min: 0.906935 0.247036
incx.max(), incx.min(), incx.mean() 7.7481 -7.09437 4.50625
fgtx.max(), fgtx.min(), fgtx.mean() 2.34489 -2.35807 1.31768
abs_mean, abs_mean+, abs_mean-: 10.0816 6.1334 22.1777
U_c = [[-0.01761369]] U_f = [[ 0.]] b_c = [ 0.3476539] b_f = [ 1.09324777]
W_c max, min, mean, abs_mean: 0.376914 -0.375948 0.187444 0.374645
W_f max, min, mean, abs_mean: 0.11988 -0.119799 0.0591937 0.11871
Epoch 13/300
1s - loss: 1616.9203 - val_loss: 7031.6976
Epoch 00012: val_loss did not improve
Epoch 14/300
1s - loss: 1603.2215 - val_loss: 6951.2182
Epoch 00013: val_loss improved from 6977.51301 to 6951.21819, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1524.2153      0.88  0.22  0.70      0.89  0.19  0.73      0.89  0.16  0.76
beijing_tanh+hardsigmoid4 6951.2182      0.84  0.11  0.76      0.84  0.09  0.78      0.87  0.06  0.83
forget mean min: 0.906767 0.258763
incx.max(), incx.min(), incx.mean() 8.03979 -7.35216 4.52778
fgtx.max(), fgtx.min(), fgtx.mean() 2.29097 -2.30171 1.24305
abs_mean, abs_mean+, abs_mean-: 9.98979 6.07972 21.2492
U_c = [[-0.0127951]] U_f = [[ 0.]] b_c = [ 0.36181015] b_f = [ 1.09553051]
W_c max, min, mean, abs_mean: 0.391662 -0.390685 0.194814 0.389387
W_f max, min, mean, abs_mean: 0.117341 -0.117266 0.0579323 0.116186
Epoch 15/300
1s - loss: 1601.7393 - val_loss: 6856.5547
Epoch 00014: val_loss improved from 6951.21819 to 6856.55470, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1511.2457      0.89  0.22  0.71      0.89  0.20  0.73      0.90  0.17  0.75
beijing_tanh+hardsigmoid4 6856.5546      0.85  0.11  0.77      0.85  0.08  0.79      0.87  0.06  0.83
forget mean min: 0.908078 0.258815
incx.max(), incx.min(), incx.mean() 8.20071 -7.48925 4.61015
fgtx.max(), fgtx.min(), fgtx.mean() 2.29631 -2.30445 1.24345
abs_mean, abs_mean+, abs_mean-: 10.0278 6.13043 21.2709
U_c = [[-0.01195961]] U_f = [[ 0.]] b_c = [ 0.36961123] b_f = [ 1.09852755]
W_c max, min, mean, abs_mean: 0.399334 -0.39835 0.198649 0.397057
W_f max, min, mean, abs_mean: 0.117578 -0.117506 0.0580535 0.116429
Epoch 16/300
1s - loss: 1586.4952 - val_loss: 7253.9641
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1578.1361 - val_loss: 6936.3692
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1574.1923 - val_loss: 7077.4480
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1572.8250 - val_loss: 7005.4563
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1556.8996 - val_loss: 7011.7236
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1547.8071 - val_loss: 7049.6234
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1536.5307 - val_loss: 7090.1690
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1527.2713 - val_loss: 7387.7463
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1507.1888 - val_loss: 6975.7579
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1492.4923 - val_loss: 7050.4792
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1465.4339 - val_loss: 6942.0873
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1437.8814 - val_loss: 6978.9107
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1417.2886 - val_loss: 6793.3611
Epoch 00027: val_loss improved from 6856.55470 to 6793.36112, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1317.9523      0.88  0.20  0.72      0.89  0.17  0.75      0.90  0.14  0.78
beijing_tanh+hardsigmoid4 6793.3611      0.85  0.10  0.77      0.85  0.07  0.79      0.87  0.05  0.83
forget mean min: 0.91175 0.168788
incx.max(), incx.min(), incx.mean() 9.80276 -8.58737 4.99494
fgtx.max(), fgtx.min(), fgtx.mean() 2.83996 -2.75192 1.37805
abs_mean, abs_mean+, abs_mean-: 10.2291 6.63121 18.0825
U_c = [[-0.00651943]] U_f = [[ 0.]] b_c = [ 0.46298638] b_f = [ 1.0958612]
W_c max, min, mean, abs_mean: 0.48551 -0.48422 0.241578 0.483003
W_f max, min, mean, abs_mean: 0.150328 -0.150676 0.0733389 0.146868
Epoch 29/300
1s - loss: 1392.0250 - val_loss: 6885.4674
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1360.0428 - val_loss: 6954.1728
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1334.5537 - val_loss: 7405.6792
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1294.9833 - val_loss: 7476.0141
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1256.7629 - val_loss: 7291.5210
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1217.0858 - val_loss: 7057.0141
Epoch 00033: val_loss did not improve
Epoch 35/300
1s - loss: 1192.4714 - val_loss: 6630.2455
Epoch 00034: val_loss improved from 6793.36112 to 6630.24547, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1098.4252      0.88  0.19  0.73      0.90  0.16  0.77      0.90  0.12  0.80
beijing_tanh+hardsigmoid4 6630.2454      0.87  0.07  0.81      0.87  0.05  0.83      0.88  0.04  0.85
forget mean min: 0.892355 0.154692
incx.max(), incx.min(), incx.mean() 12.3713 -9.79308 5.43182
fgtx.max(), fgtx.min(), fgtx.mean() 3.14278 -2.76934 1.29232
abs_mean, abs_mean+, abs_mean-: 10.7364 7.54272 16.1262
U_c = [[-0.00808008]] U_f = [[ 0.]] b_c = [ 0.58947706] b_f = [ 1.0428009]
W_c max, min, mean, abs_mean: 0.620015 -0.618504 0.308055 0.616198
W_f max, min, mean, abs_mean: 0.17425 -0.17274 0.0822129 0.164373
Epoch 36/300
1s - loss: 1169.5237 - val_loss: 6682.7613
Epoch 00035: val_loss did not improve
Epoch 37/300
1s - loss: 1148.2287 - val_loss: 6349.2002
Epoch 00036: val_loss improved from 6630.24547 to 6349.20022, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1060.0483      0.88  0.18  0.74      0.90  0.14  0.79      0.90  0.11  0.82
beijing_tanh+hardsigmoid4 6349.2002      0.88  0.07  0.82      0.88  0.05  0.84      0.89  0.04  0.86
forget mean min: 0.893557 0.129724
incx.max(), incx.min(), incx.mean() 12.9491 -10.1354 5.59859
fgtx.max(), fgtx.min(), fgtx.mean() 3.30649 -2.88288 1.3364
abs_mean, abs_mean+, abs_mean-: 10.9305 7.75391 16.2306
U_c = [[-0.01163366]] U_f = [[ 0.]] b_c = [ 0.61741728] b_f = [ 1.031497]
W_c max, min, mean, abs_mean: 0.65034 -0.649298 0.323051 0.646244
W_f max, min, mean, abs_mean: 0.184055 -0.182489 0.0866633 0.173283
Epoch 38/300
1s - loss: 1135.4733 - val_loss: 6111.5014
Epoch 00037: val_loss improved from 6349.20022 to 6111.50143, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1063.1101      0.89  0.18  0.74      0.91  0.15  0.78      0.92  0.12  0.82
beijing_tanh+hardsigmoid4 6111.5015      0.89  0.07  0.83      0.88  0.05  0.85      0.90  0.04  0.87
forget mean min: 0.895163 0.119021
incx.max(), incx.min(), incx.mean() 13.2453 -10.3448 5.74386
fgtx.max(), fgtx.min(), fgtx.mean() 3.36995 -2.93203 1.36675
abs_mean, abs_mean+, abs_mean-: 11.1676 7.89368 16.9028
U_c = [[-0.01019756]] U_f = [[ 0.]] b_c = [ 0.63120192] b_f = [ 1.02713764]
W_c max, min, mean, abs_mean: 0.665038 -0.66433 0.33034 0.660844
W_f max, min, mean, abs_mean: 0.187773 -0.186146 0.0883011 0.176556
Epoch 39/300
1s - loss: 1124.9243 - val_loss: 5690.1649
Epoch 00038: val_loss improved from 6111.50143 to 5690.16487, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1074.4075      0.90  0.19  0.74      0.91  0.15  0.79      0.91  0.12  0.81
beijing_tanh+hardsigmoid4 5690.1649      0.90  0.08  0.84      0.90  0.05  0.86      0.92  0.04  0.88
forget mean min: 0.897503 0.107469
incx.max(), incx.min(), incx.mean() 13.4168 -10.4751 5.87078
fgtx.max(), fgtx.min(), fgtx.mean() 3.43301 -2.98673 1.40625
abs_mean, abs_mean+, abs_mean-: 11.364 8.04558 17.4142
U_c = [[-0.0123805]] U_f = [[ 0.]] b_c = [ 0.64104068] b_f = [ 1.02407777]
W_c max, min, mean, abs_mean: 0.67422 -0.673842 0.334848 0.66989
W_f max, min, mean, abs_mean: 0.192066 -0.19028 0.090035 0.180016
Epoch 40/300
1s - loss: 1122.2175 - val_loss: 6551.7780
Epoch 00039: val_loss did not improve
Epoch 41/300
1s - loss: 1112.3580 - val_loss: 6229.2091
Epoch 00040: val_loss did not improve
Epoch 42/300
1s - loss: 1106.1567 - val_loss: 6579.6288
Epoch 00041: val_loss did not improve
Epoch 43/300
1s - loss: 1099.7128 - val_loss: 6474.3984
Epoch 00042: val_loss did not improve
Epoch 44/300
1s - loss: 1085.6154 - val_loss: 6043.0473
Epoch 00043: val_loss did not improve
Epoch 45/300
1s - loss: 1085.0115 - val_loss: 6196.7985
Epoch 00044: val_loss did not improve
Epoch 46/300
1s - loss: 1081.0354 - val_loss: 5552.7745
Epoch 00045: val_loss improved from 5690.16487 to 5552.77446, saving model to beijing_tanh+hardsigmoid4_weights.hdf5
beijing_tanh+hardsigmoid4 1058.8500      0.90  0.18  0.75      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid4 5552.7744      0.89  0.08  0.83      0.90  0.05  0.86      0.91  0.04  0.88
forget mean min: 0.895942 0.0726809
incx.max(), incx.min(), incx.mean() 14.3262 -10.9341 6.06567
fgtx.max(), fgtx.min(), fgtx.mean() 3.66951 -3.13315 1.44652
abs_mean, abs_mean+, abs_mean-: 11.5014 8.22944 17.3418
U_c = [[-0.0112675]] U_f = [[ 0.]] b_c = [ 0.70174557] b_f = [ 0.99655479]
W_c max, min, mean, abs_mean: 0.725722 -0.725964 0.359015 0.718418
W_f max, min, mean, abs_mean: 0.210772 -0.208692 0.0968048 0.193511
Epoch 47/300
1s - loss: 1073.0640 - val_loss: 5752.3744
Epoch 00046: val_loss did not improve
Epoch 48/300
1s - loss: 1062.5980 - val_loss: 6430.0034
Epoch 00047: val_loss did not improve
Epoch 49/300
1s - loss: 1065.5418 - val_loss: 5975.9866
Epoch 00048: val_loss did not improve
Epoch 50/300
1s - loss: 1056.5619 - val_loss: 6256.4998
Epoch 00049: val_loss did not improve
Epoch 51/300
1s - loss: 1054.9508 - val_loss: 6286.6514
Epoch 00050: val_loss did not improve
Epoch 52/300
1s - loss: 1048.4679 - val_loss: 6342.4270
Epoch 00051: val_loss did not improve
Epoch 53/300
1s - loss: 1037.1906 - val_loss: 5766.3207
Epoch 00052: val_loss did not improve
Epoch 54/300
1s - loss: 1042.6751 - val_loss: 7123.6286
Epoch 00053: val_loss did not improve
Epoch 55/300
1s - loss: 1029.0542 - val_loss: 6303.5914
Epoch 00054: val_loss did not improve
Epoch 56/300
1s - loss: 1030.1354 - val_loss: 6389.8276
Epoch 00055: val_loss did not improve
Epoch 57/300
1s - loss: 1030.7929 - val_loss: 5886.3828
Epoch 00056: val_loss did not improve
Epoch 58/300
1s - loss: 1025.3454 - val_loss: 6817.5846
Epoch 00057: val_loss did not improve
Epoch 59/300
1s - loss: 1020.1402 - val_loss: 6273.9197
Epoch 00058: val_loss did not improve
Epoch 60/300
1s - loss: 1012.3457 - val_loss: 6316.3697
Epoch 00059: val_loss did not improve
Epoch 61/300
1s - loss: 1008.1129 - val_loss: 7151.7108
Epoch 00060: val_loss did not improve
Epoch 62/300
1s - loss: 999.7639 - val_loss: 6392.0588
Epoch 00061: val_loss did not improve
Epoch 63/300
1s - loss: 997.0136 - val_loss: 7191.8295
Epoch 00062: val_loss did not improve
Epoch 64/300
1s - loss: 994.9371 - val_loss: 6608.3405
Epoch 00063: val_loss did not improve
Epoch 65/300
1s - loss: 992.9418 - val_loss: 6939.6209
Epoch 00064: val_loss did not improve
Epoch 66/300
1s - loss: 984.7078 - val_loss: 7411.1755
Epoch 00065: val_loss did not improve
Epoch 67/300
1s - loss: 980.4457 - val_loss: 7477.0090
Epoch 00066: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid5
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid5 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid529822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5979.6072 - val_loss: 9715.5583
Epoch 00000: val_loss improved from inf to 9715.55827, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 3304.3233      0.55  0.22  0.48      0.55  0.20  0.49      0.53  0.18  0.48
beijing_tanh+hardsigmoid5 9715.5583      0.79  0.09  0.73      0.79  0.05  0.76      0.80  0.03  0.78
forget mean min: 0.845927 0.319346
incx.max(), incx.min(), incx.mean() 2.9074 -2.74117 1.34659
fgtx.max(), fgtx.min(), fgtx.mean() 1.94314 -1.99832 0.854036
abs_mean, abs_mean+, abs_mean-: 6.54418 2.61093 12.7331
U_c = [[-0.06228712]] U_f = [[ 0.]] b_c = [ 0.12264903] b_f = [ 1.09504569]
W_c max, min, mean, abs_mean: 0.148416 -0.148913 -0.0587158 0.14714
W_f max, min, mean, abs_mean: 0.10437 -0.104387 -0.0411276 0.102671
Epoch 2/300
1s - loss: 2455.9023 - val_loss: 10656.5813
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1868.6625 - val_loss: 9583.6203
Epoch 00002: val_loss improved from 9715.55827 to 9583.62030, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1740.5634      0.85  0.24  0.67      0.86  0.22  0.69      0.86  0.19  0.72
beijing_tanh+hardsigmoid5 9583.6203      0.79  0.15  0.69      0.79  0.12  0.71      0.81  0.09  0.76
forget mean min: 0.910382 0.420566
incx.max(), incx.min(), incx.mean() 6.30158 -5.74588 4.17529
fgtx.max(), fgtx.min(), fgtx.mean() 1.50666 -1.50682 0.974807
abs_mean, abs_mean+, abs_mean-: 8.56488 5.2522 16.5212
U_c = [[-0.01101611]] U_f = [[ 0.]] b_c = [ 0.27815393] b_f = [ 1.1096462]
W_c max, min, mean, abs_mean: 0.30369 -0.304213 -0.120839 0.302433
W_f max, min, mean, abs_mean: 0.0773616 -0.0773815 -0.0303171 0.0756489
Epoch 4/300
1s - loss: 1740.7105 - val_loss: 8957.5182
Epoch 00003: val_loss improved from 9583.62030 to 8957.51819, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1668.9238      0.87  0.25  0.68      0.88  0.23  0.70      0.88  0.20  0.73
beijing_tanh+hardsigmoid5 8957.5180      0.83  0.16  0.72      0.83  0.12  0.74      0.86  0.09  0.79
forget mean min: 0.913267 0.38448
incx.max(), incx.min(), incx.mean() 6.86071 -6.25613 4.54254
fgtx.max(), fgtx.min(), fgtx.mean() 1.68457 -1.68565 1.08895
abs_mean, abs_mean+, abs_mean-: 9.91 6.0175 22.755
U_c = [[-0.01384344]] U_f = [[ 0.]] b_c = [ 0.30438757] b_f = [ 1.10805321]
W_c max, min, mean, abs_mean: 0.330162 -0.330692 -0.131431 0.328908
W_f max, min, mean, abs_mean: 0.0862279 -0.0862467 -0.0338648 0.0845093
Epoch 5/300
1s - loss: 1684.6469 - val_loss: 8121.1008
Epoch 00004: val_loss improved from 8957.51819 to 8121.10083, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1635.2941      0.89  0.25  0.68      0.89  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid5 8121.1007      0.85  0.14  0.74      0.85  0.11  0.77      0.88  0.09  0.81
forget mean min: 0.909346 0.343286
incx.max(), incx.min(), incx.mean() 7.08838 -6.46888 4.6003
fgtx.max(), fgtx.min(), fgtx.mean() 1.8834 -1.886 1.19163
abs_mean, abs_mean+, abs_mean-: 10.3326 6.24729 25.0326
U_c = [[-0.01733122]] U_f = [[ 0.]] b_c = [ 0.31441972] b_f = [ 1.1024313]
W_c max, min, mean, abs_mean: 0.341178 -0.341717 -0.135841 0.339928
W_f max, min, mean, abs_mean: 0.09624 -0.0962575 -0.0378724 0.0945121
Epoch 6/300
1s - loss: 1657.6443 - val_loss: 7624.3881
Epoch 00005: val_loss improved from 8121.10083 to 7624.38812, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1597.6629      0.89  0.25  0.69      0.89  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid5 7624.3881      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.08  0.82
forget mean min: 0.907074 0.314095
incx.max(), incx.min(), incx.mean() 7.20896 -6.58661 4.56666
fgtx.max(), fgtx.min(), fgtx.mean() 2.01928 -2.02438 1.24479
abs_mean, abs_mean+, abs_mean-: 10.3445 6.24006 24.6465
U_c = [[-0.01920352]] U_f = [[ 0.]] b_c = [ 0.31988081] b_f = [ 1.09485233]
W_c max, min, mean, abs_mean: 0.347456 -0.348003 -0.138356 0.346209
W_f max, min, mean, abs_mean: 0.103217 -0.103233 -0.0406661 0.101478
Epoch 7/300
1s - loss: 1650.0402 - val_loss: 7619.9293
Epoch 00006: val_loss improved from 7624.38812 to 7619.92932, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1581.4718      0.88  0.23  0.70      0.88  0.21  0.72      0.89  0.18  0.74
beijing_tanh+hardsigmoid5 7619.9294      0.84  0.14  0.74      0.85  0.11  0.77      0.87  0.08  0.81
forget mean min: 0.910278 0.294704
incx.max(), incx.min(), incx.mean() 7.3139 -6.6908 4.60447
fgtx.max(), fgtx.min(), fgtx.mean() 2.10992 -2.11811 1.29194
abs_mean, abs_mean+, abs_mean-: 10.3224 6.2101 24.5322
U_c = [[-0.01750094]] U_f = [[ 0.]] b_c = [ 0.32510692] b_f = [ 1.09162807]
W_c max, min, mean, abs_mean: 0.353306 -0.353862 -0.140699 0.352062
W_f max, min, mean, abs_mean: 0.108037 -0.10805 -0.042595 0.106288
Epoch 8/300
1s - loss: 1638.2237 - val_loss: 7294.9327
Epoch 00007: val_loss improved from 7619.92932 to 7294.93265, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1571.1776      0.88  0.23  0.70      0.89  0.21  0.72      0.89  0.18  0.75
beijing_tanh+hardsigmoid5 7294.9328      0.85  0.14  0.75      0.85  0.11  0.78      0.88  0.08  0.82
forget mean min: 0.910282 0.278501
incx.max(), incx.min(), incx.mean() 7.40601 -6.78748 4.6096
fgtx.max(), fgtx.min(), fgtx.mean() 2.18589 -2.1988 1.32202
abs_mean, abs_mean+, abs_mean-: 10.3414 6.205 24.939
U_c = [[-0.01860025]] U_f = [[ 0.]] b_c = [ 0.33016485] b_f = [ 1.09130526]
W_c max, min, mean, abs_mean: 0.358665 -0.359228 -0.142846 0.357424
W_f max, min, mean, abs_mean: 0.112171 -0.112184 -0.0442509 0.110416
Epoch 9/300
1s - loss: 1631.0554 - val_loss: 7142.9291
Epoch 00008: val_loss improved from 7294.93265 to 7142.92911, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1576.8471      0.89  0.25  0.69      0.89  0.23  0.70      0.90  0.20  0.73
beijing_tanh+hardsigmoid5 7142.9292      0.85  0.13  0.76      0.86  0.10  0.79      0.88  0.07  0.83
forget mean min: 0.903589 0.262422
incx.max(), incx.min(), incx.mean() 7.52067 -6.89424 4.55107
fgtx.max(), fgtx.min(), fgtx.mean() 2.26504 -2.2791 1.32891
abs_mean, abs_mean+, abs_mean-: 10.4673 6.27583 25.3197
U_c = [[-0.01783722]] U_f = [[ 0.]] b_c = [ 0.33551428] b_f = [ 1.09121251]
W_c max, min, mean, abs_mean: 0.364335 -0.364905 -0.145117 0.363097
W_f max, min, mean, abs_mean: 0.116228 -0.116238 -0.0458726 0.114462
Epoch 10/300
1s - loss: 1626.5377 - val_loss: 7148.2337
Epoch 00009: val_loss did not improve
Epoch 11/300
1s - loss: 1625.4502 - val_loss: 7202.2076
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1616.7178 - val_loss: 7004.0932
Epoch 00011: val_loss improved from 7142.92911 to 7004.09324, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1546.0829      0.88  0.23  0.70      0.89  0.21  0.72      0.90  0.18  0.75
beijing_tanh+hardsigmoid5 7004.0932      0.85  0.12  0.76      0.85  0.09  0.78      0.87  0.07  0.82
forget mean min: 0.911479 0.258735
incx.max(), incx.min(), incx.mean() 7.88365 -7.21703 4.57699
fgtx.max(), fgtx.min(), fgtx.mean() 2.2867 -2.299 1.28255
abs_mean, abs_mean+, abs_mean-: 9.83307 6.06029 20.6019
U_c = [[-0.01533278]] U_f = [[ 0.]] b_c = [ 0.35356119] b_f = [ 1.09267223]
W_c max, min, mean, abs_mean: 0.383153 -0.38373 -0.152649 0.381914
W_f max, min, mean, abs_mean: 0.117763 -0.11777 -0.0464818 0.115978
Epoch 13/300
1s - loss: 1603.6448 - val_loss: 6967.0094
Epoch 00012: val_loss improved from 7004.09324 to 6967.00941, saving model to beijing_tanh+hardsigmoid5_weights.hdf5
beijing_tanh+hardsigmoid5 1528.0020      0.88  0.22  0.70      0.89  0.20  0.73      0.89  0.17  0.75
beijing_tanh+hardsigmoid5 6967.0094      0.85  0.11  0.76      0.85  0.09  0.79      0.88  0.06  0.83
forget mean min: 0.908718 0.25324
incx.max(), incx.min(), incx.mean() 8.04735 -7.36512 4.59776
fgtx.max(), fgtx.min(), fgtx.mean() 2.31498 -2.32676 1.27608
abs_mean, abs_mean+, abs_mean-: 10.1392 6.14132 22.273
U_c = [[-0.01017717]] U_f = [[ 0.]] b_c = [ 0.36066827] b_f = [ 1.09295869]
W_c max, min, mean, abs_mean: 0.390881 -0.391457 -0.15574 0.389639
W_f max, min, mean, abs_mean: 0.119136 -0.11914 -0.0470315 0.117347
Epoch 14/300
1s - loss: 1594.3718 - val_loss: 6989.5689
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1582.8037 - val_loss: 7017.0468
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1581.3412 - val_loss: 7219.7245
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1571.7917 - val_loss: 7215.4799
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1572.1887 - val_loss: 7416.4597
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1571.1987 - val_loss: 7372.7821
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1566.8750 - val_loss: 7416.1820
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1558.8423 - val_loss: 7303.9721
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1558.9165 - val_loss: 7349.8571
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1554.9454 - val_loss: 7443.9722
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1543.6389 - val_loss: 7341.7873
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1521.2062 - val_loss: 7406.5703
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1513.3685 - val_loss: 7509.9262
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1477.7035 - val_loss: 7654.5128
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1452.4261 - val_loss: 7417.6880
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1421.4420 - val_loss: 7947.0450
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1390.9631 - val_loss: 8093.4367
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1365.9518 - val_loss: 8298.9576
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1334.9254 - val_loss: 8676.0304
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1310.1392 - val_loss: 9057.2012
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1291.3464 - val_loss: 9168.4107
Epoch 00033: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid6
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid6 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid629822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5876.4451 - val_loss: 9959.1554
Epoch 00000: val_loss improved from inf to 9959.15542, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 3244.0255      0.56  0.22  0.49      0.56  0.20  0.50      0.54  0.18  0.49
beijing_tanh+hardsigmoid6 9959.1554      0.77  0.09  0.72      0.78  0.05  0.74      0.79  0.03  0.76
forget mean min: 0.845124 0.317454
incx.max(), incx.min(), incx.mean() 2.92449 -2.76624 1.34402
fgtx.max(), fgtx.min(), fgtx.mean() 1.94629 -2.00734 0.848257
abs_mean, abs_mean+, abs_mean-: 6.53884 2.60413 12.6724
U_c = [[-0.05874204]] U_f = [[ 0.]] b_c = [ 0.12306515] b_f = [ 1.09461308]
W_c max, min, mean, abs_mean: 0.150001 -0.149904 -0.0155034 0.148117
W_f max, min, mean, abs_mean: 0.104315 -0.104628 -0.011049 0.102904
Epoch 2/300
1s - loss: 2451.0520 - val_loss: 10776.3699
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1878.8447 - val_loss: 9160.0756
Epoch 00002: val_loss improved from 9959.15542 to 9160.07557, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1744.4842      0.86  0.25  0.66      0.86  0.23  0.69      0.87  0.20  0.71
beijing_tanh+hardsigmoid6 9160.0757      0.80  0.14  0.71      0.80  0.11  0.73      0.83  0.08  0.77
forget mean min: 0.908079 0.413394
incx.max(), incx.min(), incx.mean() 6.21908 -5.67522 4.06494
fgtx.max(), fgtx.min(), fgtx.mean() 1.54176 -1.54268 0.983141
abs_mean, abs_mean+, abs_mean-: 8.8547 5.33274 18.4228
U_c = [[-0.01379718]] U_f = [[ 0.]] b_c = [ 0.27371147] b_f = [ 1.10965073]
W_c max, min, mean, abs_mean: 0.300297 -0.300189 -0.0305285 0.298423
W_f max, min, mean, abs_mean: 0.0788173 -0.0791356 -0.00849896 0.0773876
Epoch 4/300
1s - loss: 1740.5100 - val_loss: 8690.4913
Epoch 00003: val_loss improved from 9160.07557 to 8690.49132, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1657.5217      0.86  0.22  0.69      0.86  0.20  0.71      0.86  0.17  0.73
beijing_tanh+hardsigmoid6 8690.4912      0.83  0.16  0.72      0.83  0.13  0.74      0.86  0.10  0.79
forget mean min: 0.915172 0.385215
incx.max(), incx.min(), incx.mean() 6.781 -6.1848 4.50376
fgtx.max(), fgtx.min(), fgtx.mean() 1.67791 -1.67873 1.08837
abs_mean, abs_mean+, abs_mean-: 9.58099 5.86182 20.9576
U_c = [[-0.01724614]] U_f = [[ 0.]] b_c = [ 0.29967165] b_f = [ 1.1048038]
W_c max, min, mean, abs_mean: 0.327116 -0.327006 -0.0332114 0.325242
W_f max, min, mean, abs_mean: 0.0856352 -0.0859557 -0.00918219 0.0842
Epoch 5/300
1s - loss: 1683.6084 - val_loss: 8052.7382
Epoch 00004: val_loss improved from 8690.49132 to 8052.73822, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1628.1893      0.88  0.25  0.68      0.89  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid6 8052.7382      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.09  0.81
forget mean min: 0.908768 0.344615
incx.max(), incx.min(), incx.mean() 7.09991 -6.47914 4.60079
fgtx.max(), fgtx.min(), fgtx.mean() 1.87483 -1.8771 1.18431
abs_mean, abs_mean+, abs_mean-: 10.3416 6.26387 24.8456
U_c = [[-0.01764654]] U_f = [[ 0.]] b_c = [ 0.31448945] b_f = [ 1.10017252]
W_c max, min, mean, abs_mean: 0.342295 -0.342182 -0.0347301 0.340419
W_f max, min, mean, abs_mean: 0.0955015 -0.0958228 -0.0101704 0.0940587
Epoch 6/300
1s - loss: 1661.5305 - val_loss: 7648.1423
Epoch 00005: val_loss improved from 8052.73822 to 7648.14234, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1605.3216      0.88  0.24  0.69      0.88  0.22  0.70      0.89  0.19  0.73
beijing_tanh+hardsigmoid6 7648.1423      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.08  0.82
forget mean min: 0.904972 0.323176
incx.max(), incx.min(), incx.mean() 7.18135 -6.56147 4.53147
fgtx.max(), fgtx.min(), fgtx.mean() 1.97332 -1.97806 1.21141
abs_mean, abs_mean+, abs_mean-: 10.3317 6.23465 24.5621
U_c = [[-0.01988763]] U_f = [[ 0.]] b_c = [ 0.31818509] b_f = [ 1.09393501]
W_c max, min, mean, abs_mean: 0.346725 -0.346609 -0.0351739 0.344849
W_f max, min, mean, abs_mean: 0.100602 -0.100926 -0.0106821 0.0991519
Epoch 7/300
1s - loss: 1652.2081 - val_loss: 7289.2143
Epoch 00006: val_loss improved from 7648.14234 to 7289.21429, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1587.6321      0.88  0.24  0.69      0.89  0.23  0.71      0.89  0.19  0.73
beijing_tanh+hardsigmoid6 7289.2143      0.85  0.13  0.75      0.85  0.10  0.78      0.88  0.07  0.82
forget mean min: 0.905884 0.310819
incx.max(), incx.min(), incx.mean() 7.292 -6.67274 4.52418
fgtx.max(), fgtx.min(), fgtx.mean() 2.0246 -2.03292 1.22039
abs_mean, abs_mean+, abs_mean-: 10.1548 6.18548 22.7332
U_c = [[-0.02184285]] U_f = [[ 0.]] b_c = [ 0.32395574] b_f = [ 1.08701813]
W_c max, min, mean, abs_mean: 0.352827 -0.352709 -0.0357847 0.350951
W_f max, min, mean, abs_mean: 0.103428 -0.103753 -0.0109658 0.10197
Epoch 8/300
1s - loss: 1639.4107 - val_loss: 7340.0201
Epoch 00007: val_loss did not improve
Epoch 9/300
1s - loss: 1628.1070 - val_loss: 7111.4025
Epoch 00008: val_loss improved from 7289.21429 to 7111.40255, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1561.7383      0.88  0.23  0.69      0.89  0.21  0.71      0.89  0.18  0.74
beijing_tanh+hardsigmoid6 7111.4026      0.85  0.13  0.75      0.85  0.10  0.78      0.88  0.07  0.82
forget mean min: 0.905082 0.275539
incx.max(), incx.min(), incx.mean() 7.49727 -6.87112 4.5361
fgtx.max(), fgtx.min(), fgtx.mean() 2.19161 -2.20444 1.28563
abs_mean, abs_mean+, abs_mean-: 10.3112 6.20788 23.9081
U_c = [[-0.01960585]] U_f = [[ 0.]] b_c = [ 0.3340418] b_f = [ 1.08213782]
W_c max, min, mean, abs_mean: 0.363665 -0.363549 -0.0368691 0.361789
W_f max, min, mean, abs_mean: 0.112156 -0.112489 -0.0118405 0.11069
Epoch 10/300
1s - loss: 1630.2162 - val_loss: 7123.9922
Epoch 00009: val_loss did not improve
Epoch 11/300
1s - loss: 1620.0040 - val_loss: 7079.4231
Epoch 00010: val_loss improved from 7111.40255 to 7079.42312, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1552.2865      0.89  0.24  0.69      0.89  0.22  0.71      0.90  0.19  0.74
beijing_tanh+hardsigmoid6 7079.4232      0.85  0.13  0.76      0.85  0.10  0.78      0.88  0.07  0.82
forget mean min: 0.910189 0.266292
incx.max(), incx.min(), incx.mean() 7.6946 -7.04305 4.60216
fgtx.max(), fgtx.min(), fgtx.mean() 2.2433 -2.25451 1.29951
abs_mean, abs_mean+, abs_mean-: 10.1583 6.16549 23.1208
U_c = [[-0.01651128]] U_f = [[ 0.]] b_c = [ 0.344129] b_f = [ 1.08596504]
W_c max, min, mean, abs_mean: 0.373743 -0.373636 -0.0378776 0.37187
W_f max, min, mean, abs_mean: 0.114964 -0.115299 -0.0121217 0.113491
Epoch 12/300
1s - loss: 1613.9117 - val_loss: 6929.4957
Epoch 00011: val_loss improved from 7079.42312 to 6929.49565, saving model to beijing_tanh+hardsigmoid6_weights.hdf5
beijing_tanh+hardsigmoid6 1557.3817      0.89  0.24  0.70      0.90  0.22  0.72      0.90  0.19  0.75
beijing_tanh+hardsigmoid6 6929.4957      0.86  0.12  0.77      0.86  0.09  0.79      0.89  0.07  0.83
forget mean min: 0.91088 0.259964
incx.max(), incx.min(), incx.mean() 7.85241 -7.18515 4.67273
fgtx.max(), fgtx.min(), fgtx.mean() 2.27748 -2.28819 1.31207
abs_mean, abs_mean+, abs_mean-: 10.2604 6.21509 23.8095
U_c = [[-0.01464731]] U_f = [[ 0.]] b_c = [ 0.35126859] b_f = [ 1.08801162]
W_c max, min, mean, abs_mean: 0.381366 -0.381263 -0.0386402 0.379493
W_f max, min, mean, abs_mean: 0.116695 -0.11703 -0.0122949 0.115221
Epoch 13/300
1s - loss: 1599.7368 - val_loss: 7209.4171
Epoch 00012: val_loss did not improve
Epoch 14/300
1s - loss: 1601.4092 - val_loss: 7076.2779
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1580.6802 - val_loss: 7421.1007
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1583.8978 - val_loss: 7058.7301
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1569.9105 - val_loss: 7404.5681
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1560.3253 - val_loss: 7278.5489
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1570.7121 - val_loss: 7242.2526
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1556.9306 - val_loss: 7495.3908
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1550.9123 - val_loss: 7421.3616
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1542.6957 - val_loss: 7805.4675
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1531.1110 - val_loss: 7636.3304
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1525.6985 - val_loss: 7868.2618
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1495.2612 - val_loss: 7714.0505
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1479.9735 - val_loss: 8274.1245
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1457.9449 - val_loss: 8735.3712
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1436.2212 - val_loss: 9122.5249
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1412.5347 - val_loss: 8500.8569
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1386.0296 - val_loss: 9418.0572
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1357.7402 - val_loss: 9059.2414
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1339.7819 - val_loss: 9786.8959
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1315.3686 - val_loss: 9567.8383
Epoch 00032: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid7
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid7 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid729822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5939.0189 - val_loss: 9622.4097
Epoch 00000: val_loss improved from inf to 9622.40969, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 3300.5831      0.55  0.21  0.48      0.55  0.20  0.49      0.53  0.18  0.48
beijing_tanh+hardsigmoid7 9622.4097      0.79  0.09  0.73      0.79  0.05  0.75      0.80  0.03  0.78
forget mean min: 0.846883 0.321435
incx.max(), incx.min(), incx.mean() 2.93437 -2.75829 1.36382
fgtx.max(), fgtx.min(), fgtx.mean() 1.93987 -1.98784 0.856255
abs_mean, abs_mean+, abs_mean-: 6.50844 2.62065 12.7419
U_c = [[-0.06128609]] U_f = [[ 0.]] b_c = [ 0.12279876] b_f = [ 1.09501469]
W_c max, min, mean, abs_mean: 0.148733 -0.149076 -0.0292369 0.147989
W_f max, min, mean, abs_mean: 0.102263 -0.103046 -0.0204225 0.102107
Epoch 2/300
1s - loss: 2462.1314 - val_loss: 10972.4344
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1866.7990 - val_loss: 9260.9711
Epoch 00002: val_loss improved from 9622.40969 to 9260.97114, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1749.3897      0.86  0.25  0.67      0.87  0.24  0.69      0.88  0.20  0.71
beijing_tanh+hardsigmoid7 9260.9712      0.81  0.15  0.71      0.81  0.12  0.73      0.84  0.09  0.78
forget mean min: 0.911456 0.403388
incx.max(), incx.min(), incx.mean() 6.25688 -5.70842 4.12997
fgtx.max(), fgtx.min(), fgtx.mean() 1.59638 -1.59683 1.02876
abs_mean, abs_mean+, abs_mean-: 9.11758 5.48623 20.0252
U_c = [[-0.01553843]] U_f = [[ 0.]] b_c = [ 0.2750895] b_f = [ 1.11377656]
W_c max, min, mean, abs_mean: 0.300949 -0.301299 -0.0596846 0.300206
W_f max, min, mean, abs_mean: 0.0802919 -0.0811292 -0.0160344 0.0801168
Epoch 4/300
1s - loss: 1732.9249 - val_loss: 8700.0510
Epoch 00003: val_loss improved from 9260.97114 to 8700.05096, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1651.8330      0.88  0.25  0.68      0.89  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid7 8700.0509      0.84  0.16  0.72      0.84  0.12  0.75      0.87  0.09  0.80
forget mean min: 0.914267 0.370503
incx.max(), incx.min(), incx.mean() 6.87346 -6.27003 4.53977
fgtx.max(), fgtx.min(), fgtx.mean() 1.75634 -1.75737 1.13246
abs_mean, abs_mean+, abs_mean-: 9.90598 6.0307 22.7084
U_c = [[-0.01620067]] U_f = [[ 0.]] b_c = [ 0.30363265] b_f = [ 1.109882]
W_c max, min, mean, abs_mean: 0.330346 -0.330698 -0.0655653 0.329604
W_f max, min, mean, abs_mean: 0.0882921 -0.0891456 -0.0176347 0.0881145
Epoch 5/300
1s - loss: 1685.2757 - val_loss: 7993.3153
Epoch 00004: val_loss improved from 8700.05096 to 7993.31528, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1612.5411      0.88  0.24  0.69      0.88  0.22  0.71      0.89  0.19  0.73
beijing_tanh+hardsigmoid7 7993.3153      0.85  0.15  0.74      0.85  0.12  0.76      0.88  0.09  0.81
forget mean min: 0.91005 0.335733
incx.max(), incx.min(), incx.mean() 7.0351 -6.42222 4.5445
fgtx.max(), fgtx.min(), fgtx.mean() 1.92153 -1.92405 1.20981
abs_mean, abs_mean+, abs_mean-: 10.1726 6.16268 24.089
U_c = [[-0.02063089]] U_f = [[ 0.]] b_c = [ 0.31085324] b_f = [ 1.10272014]
W_c max, min, mean, abs_mean: 0.338194 -0.338548 -0.0671358 0.337453
W_f max, min, mean, abs_mean: 0.0966096 -0.0974808 -0.019298 0.096431
Epoch 6/300
1s - loss: 1663.8649 - val_loss: 7752.6687
Epoch 00005: val_loss improved from 7993.31528 to 7752.66874, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1605.7915      0.89  0.25  0.68      0.90  0.24  0.70      0.90  0.20  0.73
beijing_tanh+hardsigmoid7 7752.6688      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.08  0.82
forget mean min: 0.90803 0.3212
incx.max(), incx.min(), incx.mean() 7.30722 -6.67631 4.65622
fgtx.max(), fgtx.min(), fgtx.mean() 1.98451 -1.98908 1.2312
abs_mean, abs_mean+, abs_mean-: 10.4777 6.32872 25.0937
U_c = [[-0.0172748]] U_f = [[ 0.]] b_c = [ 0.32348874] b_f = [ 1.0950768]
W_c max, min, mean, abs_mean: 0.351575 -0.35193 -0.069813 0.350833
W_f max, min, mean, abs_mean: 0.099875 -0.100766 -0.0199518 0.0996935
Epoch 7/300
1s - loss: 1645.5511 - val_loss: 7382.6221
Epoch 00006: val_loss improved from 7752.66874 to 7382.62215, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1583.9967      0.88  0.24  0.69      0.88  0.22  0.71      0.89  0.19  0.74
beijing_tanh+hardsigmoid7 7382.6222      0.85  0.14  0.75      0.85  0.11  0.77      0.87  0.08  0.82
forget mean min: 0.904279 0.292626
incx.max(), incx.min(), incx.mean() 7.31199 -6.69268 4.53853
fgtx.max(), fgtx.min(), fgtx.mean() 2.11809 -2.12684 1.27743
abs_mean, abs_mean+, abs_mean-: 10.4076 6.23434 25.1304
U_c = [[-0.02090071]] U_f = [[ 0.]] b_c = [ 0.32408386] b_f = [ 1.08996761]
W_c max, min, mean, abs_mean: 0.352625 -0.352982 -0.0700241 0.351884
W_f max, min, mean, abs_mean: 0.106841 -0.10775 -0.0213451 0.106659
Epoch 8/300
1s - loss: 1637.1373 - val_loss: 7384.8629
Epoch 00007: val_loss did not improve
Epoch 9/300
1s - loss: 1632.4044 - val_loss: 6941.6463
Epoch 00008: val_loss improved from 7382.62215 to 6941.64626, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1570.0931      0.88  0.23  0.70      0.88  0.21  0.72      0.88  0.17  0.74
beijing_tanh+hardsigmoid7 6941.6462      0.86  0.13  0.76      0.86  0.10  0.78      0.88  0.07  0.83
forget mean min: 0.905449 0.275224
incx.max(), incx.min(), incx.mean() 7.48318 -6.86049 4.53571
fgtx.max(), fgtx.min(), fgtx.mean() 2.19551 -2.20919 1.29039
abs_mean, abs_mean+, abs_mean-: 10.2934 6.22907 23.8868
U_c = [[-0.0227463]] U_f = [[ 0.]] b_c = [ 0.33362257] b_f = [ 1.08530712]
W_c max, min, mean, abs_mean: 0.362094 -0.362456 -0.0719202 0.361354
W_f max, min, mean, abs_mean: 0.111155 -0.112086 -0.0222083 0.110966
Epoch 10/300
1s - loss: 1630.2575 - val_loss: 7005.2809
Epoch 00009: val_loss did not improve
Epoch 11/300
1s - loss: 1620.1679 - val_loss: 7240.8829
Epoch 00010: val_loss did not improve
Epoch 12/300
1s - loss: 1620.6100 - val_loss: 7141.6654
Epoch 00011: val_loss did not improve
Epoch 13/300
1s - loss: 1616.1704 - val_loss: 6911.8368
Epoch 00012: val_loss improved from 6941.64626 to 6911.83679, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1537.4490      0.87  0.21  0.71      0.88  0.19  0.73      0.88  0.16  0.76
beijing_tanh+hardsigmoid7 6911.8368      0.84  0.12  0.76      0.85  0.09  0.78      0.87  0.06  0.83
forget mean min: 0.907937 0.265353
incx.max(), incx.min(), incx.mean() 7.87373 -7.20775 4.47789
fgtx.max(), fgtx.min(), fgtx.mean() 2.25001 -2.2622 1.23401
abs_mean, abs_mean+, abs_mean-: 9.76527 5.99162 20.1782
U_c = [[-0.01617947]] U_f = [[ 0.]] b_c = [ 0.35336643] b_f = [ 1.08896852]
W_c max, min, mean, abs_mean: 0.382352 -0.382725 -0.0759762 0.381613
W_f max, min, mean, abs_mean: 0.114372 -0.115313 -0.0228528 0.114175
Epoch 14/300
1s - loss: 1600.8408 - val_loss: 6837.5710
Epoch 00013: val_loss improved from 6911.83679 to 6837.57101, saving model to beijing_tanh+hardsigmoid7_weights.hdf5
beijing_tanh+hardsigmoid7 1521.8793      0.88  0.22  0.71      0.89  0.19  0.73      0.89  0.16  0.76
beijing_tanh+hardsigmoid7 6837.5710      0.85  0.12  0.76      0.85  0.09  0.79      0.87  0.06  0.83
forget mean min: 0.909148 0.25622
incx.max(), incx.min(), incx.mean() 8.08918 -7.39249 4.59009
fgtx.max(), fgtx.min(), fgtx.mean() 2.30257 -2.31134 1.25976
abs_mean, abs_mean+, abs_mean-: 10.0422 6.15436 21.2639
U_c = [[-0.01405468]] U_f = [[ 0.]] b_c = [ 0.36306381] b_f = [ 1.09243965]
W_c max, min, mean, abs_mean: 0.392387 -0.392767 -0.0779847 0.391649
W_f max, min, mean, abs_mean: 0.11692 -0.117858 -0.0233623 0.116721
Epoch 15/300
1s - loss: 1589.3758 - val_loss: 6976.9536
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1582.0742 - val_loss: 6952.1934
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1581.2310 - val_loss: 7198.6592
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1571.9997 - val_loss: 7156.9414
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1567.5065 - val_loss: 7165.8409
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1568.1812 - val_loss: 7371.3113
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1566.9531 - val_loss: 7492.7931
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1561.4115 - val_loss: 7441.2233
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1559.6294 - val_loss: 7704.4811
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1557.2259 - val_loss: 7395.0291
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1551.7984 - val_loss: 7516.0551
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1539.1198 - val_loss: 7477.0856
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1531.1718 - val_loss: 7411.1069
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1508.0228 - val_loss: 7409.7656
Epoch 00027: val_loss did not improve
Epoch 29/300
1s - loss: 1485.8583 - val_loss: 7333.2152
Epoch 00028: val_loss did not improve
Epoch 30/300
1s - loss: 1450.7213 - val_loss: 7698.8389
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1433.8128 - val_loss: 7259.4683
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1394.2363 - val_loss: 7600.8747
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1365.3300 - val_loss: 8201.2151
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1339.4411 - val_loss: 8139.4058
Epoch 00033: val_loss did not improve
Epoch 35/300
1s - loss: 1305.6927 - val_loss: 8398.0493
Epoch 00034: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid8
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid8 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid829822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5935.1747 - val_loss: 9618.3150
Epoch 00000: val_loss improved from inf to 9618.31497, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 3325.6915      0.55  0.22  0.48      0.55  0.20  0.49      0.53  0.18  0.48
beijing_tanh+hardsigmoid8 9618.3150      0.79  0.09  0.74      0.79  0.05  0.76      0.80  0.03  0.78
forget mean min: 0.845533 0.31487
incx.max(), incx.min(), incx.mean() 2.90257 -2.73657 1.34532
fgtx.max(), fgtx.min(), fgtx.mean() 1.96525 -2.02106 0.864426
abs_mean, abs_mean+, abs_mean-: 6.51704 2.57927 13.023
U_c = [[-0.06306386]] U_f = [[ 0.]] b_c = [ 0.12247423] b_f = [ 1.09540808]
W_c max, min, mean, abs_mean: 0.149386 -0.149506 -0.044882 0.147024
W_f max, min, mean, abs_mean: 0.105468 -0.105615 -0.0317465 0.103931
Epoch 2/300
1s - loss: 2477.7951 - val_loss: 10486.1378
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1876.1372 - val_loss: 9528.7202
Epoch 00002: val_loss improved from 9618.31497 to 9528.72021, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1747.2957      0.84  0.23  0.67      0.85  0.21  0.69      0.85  0.18  0.72
beijing_tanh+hardsigmoid8 9528.7201      0.78  0.15  0.69      0.79  0.11  0.72      0.81  0.09  0.76
forget mean min: 0.907766 0.42037
incx.max(), incx.min(), incx.mean() 6.18402 -5.64017 4.05774
fgtx.max(), fgtx.min(), fgtx.mean() 1.50554 -1.5059 0.964008
abs_mean, abs_mean+, abs_mean-: 8.56661 5.20884 16.7525
U_c = [[-0.01303584]] U_f = [[ 0.]] b_c = [ 0.27263078] b_f = [ 1.10775197]
W_c max, min, mean, abs_mean: 0.299049 -0.29917 -0.0897819 0.29669
W_f max, min, mean, abs_mean: 0.0770967 -0.0772382 -0.0232328 0.0755627
Epoch 4/300
1s - loss: 1732.9930 - val_loss: 8669.4966
Epoch 00003: val_loss improved from 9528.72021 to 8669.49662, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1663.0205      0.88  0.25  0.68      0.88  0.23  0.70      0.88  0.20  0.73
beijing_tanh+hardsigmoid8 8669.4966      0.84  0.15  0.73      0.84  0.12  0.75      0.87  0.09  0.80
forget mean min: 0.912946 0.378159
incx.max(), incx.min(), incx.mean() 6.79637 -6.1967 4.50022
fgtx.max(), fgtx.min(), fgtx.mean() 1.71731 -1.71819 1.11018
abs_mean, abs_mean+, abs_mean-: 9.92249 6.00679 23.2639
U_c = [[-0.0166443]] U_f = [[ 0.]] b_c = [ 0.30150226] b_f = [ 1.10898542]
W_c max, min, mean, abs_mean: 0.328102 -0.328225 -0.0984981 0.325742
W_f max, min, mean, abs_mean: 0.0876705 -0.0878155 -0.0264035 0.0861295
Epoch 5/300
1s - loss: 1684.4657 - val_loss: 7785.5499
Epoch 00004: val_loss improved from 8669.49662 to 7785.54993, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1614.4407      0.88  0.25  0.68      0.88  0.23  0.70      0.89  0.19  0.73
beijing_tanh+hardsigmoid8 7785.5498      0.85  0.14  0.74      0.85  0.11  0.77      0.88  0.08  0.81
forget mean min: 0.908027 0.350601
incx.max(), incx.min(), incx.mean() 7.02844 -6.41318 4.51635
fgtx.max(), fgtx.min(), fgtx.mean() 1.84549 -1.84803 1.15521
abs_mean, abs_mean+, abs_mean-: 10.1244 6.1626 23.3065
U_c = [[-0.02131059]] U_f = [[ 0.]] b_c = [ 0.3122566] b_f = [ 1.10103726]
W_c max, min, mean, abs_mean: 0.339382 -0.339506 -0.101882 0.337019
W_f max, min, mean, abs_mean: 0.0941565 -0.0943059 -0.0283472 0.092607
Epoch 6/300
1s - loss: 1657.8731 - val_loss: 7593.4442
Epoch 00005: val_loss improved from 7785.54993 to 7593.44417, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1600.4186      0.88  0.24  0.69      0.89  0.23  0.70      0.89  0.19  0.73
beijing_tanh+hardsigmoid8 7593.4443      0.85  0.14  0.75      0.85  0.11  0.77      0.88  0.08  0.82
forget mean min: 0.905913 0.314006
incx.max(), incx.min(), incx.mean() 7.23608 -6.6079 4.60012
fgtx.max(), fgtx.min(), fgtx.mean() 2.02137 -2.02572 1.25078
abs_mean, abs_mean+, abs_mean-: 10.4998 6.32286 25.4789
U_c = [[-0.0190401]] U_f = [[ 0.]] b_c = [ 0.32153147] b_f = [ 1.09575021]
W_c max, min, mean, abs_mean: 0.349632 -0.349758 -0.104958 0.347267
W_f max, min, mean, abs_mean: 0.103079 -0.103234 -0.0310218 0.101518
Epoch 7/300
1s - loss: 1643.4469 - val_loss: 7327.0910
Epoch 00006: val_loss improved from 7593.44417 to 7327.09099, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1584.6341      0.88  0.24  0.69      0.89  0.22  0.71      0.89  0.19  0.74
beijing_tanh+hardsigmoid8 7327.0910      0.85  0.14  0.75      0.86  0.11  0.78      0.88  0.08  0.82
forget mean min: 0.905382 0.301561
incx.max(), incx.min(), incx.mean() 7.2866 -6.66317 4.5613
fgtx.max(), fgtx.min(), fgtx.mean() 2.07726 -2.08499 1.2641
abs_mean, abs_mean+, abs_mean-: 10.4186 6.27353 25.0713
U_c = [[-0.02012791]] U_f = [[ 0.]] b_c = [ 0.32467529] b_f = [ 1.09279883]
W_c max, min, mean, abs_mean: 0.352817 -0.352945 -0.105914 0.35045
W_f max, min, mean, abs_mean: 0.106132 -0.106289 -0.0319363 0.104565
Epoch 8/300
1s - loss: 1640.9947 - val_loss: 7352.3905
Epoch 00007: val_loss did not improve
Epoch 9/300
1s - loss: 1634.5978 - val_loss: 7315.5437
Epoch 00008: val_loss improved from 7327.09099 to 7315.54373, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1567.7875      0.89  0.24  0.69      0.89  0.22  0.71      0.89  0.19  0.74
beijing_tanh+hardsigmoid8 7315.5437      0.85  0.13  0.75      0.85  0.10  0.78      0.88  0.08  0.82
forget mean min: 0.909099 0.270575
incx.max(), incx.min(), incx.mean() 7.49245 -6.85955 4.61974
fgtx.max(), fgtx.min(), fgtx.mean() 2.22761 -2.2393 1.3335
abs_mean, abs_mean+, abs_mean-: 10.446 6.22632 25.7081
U_c = [[-0.01612951]] U_f = [[ 0.]] b_c = [ 0.33522284] b_f = [ 1.09216833]
W_c max, min, mean, abs_mean: 0.363843 -0.363975 -0.109224 0.361474
W_f max, min, mean, abs_mean: 0.114079 -0.11424 -0.0343174 0.112505
Epoch 10/300
1s - loss: 1626.7668 - val_loss: 7202.1973
Epoch 00009: val_loss improved from 7315.54373 to 7202.19729, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1568.6467      0.89  0.24  0.69      0.89  0.23  0.71      0.90  0.20  0.74
beijing_tanh+hardsigmoid8 7202.1974      0.85  0.13  0.76      0.85  0.10  0.78      0.88  0.07  0.82
forget mean min: 0.90874 0.258661
incx.max(), incx.min(), incx.mean() 7.63206 -6.98407 4.6229
fgtx.max(), fgtx.min(), fgtx.mean() 2.28827 -2.29961 1.34372
abs_mean, abs_mean+, abs_mean-: 10.4003 6.2288 25.1788
U_c = [[-0.01582398]] U_f = [[ 0.]] b_c = [ 0.34205025] b_f = [ 1.09291077]
W_c max, min, mean, abs_mean: 0.370795 -0.370929 -0.11131 0.368425
W_f max, min, mean, abs_mean: 0.117222 -0.117383 -0.0352596 0.115646
Epoch 11/300
1s - loss: 1621.2619 - val_loss: 7149.7957
Epoch 00010: val_loss improved from 7202.19729 to 7149.79568, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1578.7149      0.89  0.24  0.69      0.89  0.22  0.71      0.90  0.19  0.74
beijing_tanh+hardsigmoid8 7149.7956      0.85  0.12  0.76      0.85  0.09  0.78      0.88  0.07  0.83
forget mean min: 0.905964 0.256547
incx.max(), incx.min(), incx.mean() 7.74822 -7.0927 4.60673
fgtx.max(), fgtx.min(), fgtx.mean() 2.29855 -2.31084 1.32285
abs_mean, abs_mean+, abs_mean-: 10.4371 6.29315 24.623
U_c = [[-0.01537077]] U_f = [[ 0.]] b_c = [ 0.34754291] b_f = [ 1.09357643]
W_c max, min, mean, abs_mean: 0.376787 -0.376923 -0.113108 0.374417
W_f max, min, mean, abs_mean: 0.117865 -0.11803 -0.0354551 0.116289
Epoch 12/300
1s - loss: 1611.6625 - val_loss: 7058.5029
Epoch 00011: val_loss improved from 7149.79568 to 7058.50285, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1556.3424      0.89  0.24  0.69      0.89  0.22  0.72      0.90  0.19  0.74
beijing_tanh+hardsigmoid8 7058.5028      0.85  0.12  0.76      0.85  0.09  0.79      0.88  0.07  0.83
forget mean min: 0.90846 0.256038
incx.max(), incx.min(), incx.mean() 7.8528 -7.18199 4.59138
fgtx.max(), fgtx.min(), fgtx.mean() 2.30371 -2.31434 1.30194
abs_mean, abs_mean+, abs_mean-: 10.2167 6.19607 23.1956
U_c = [[-0.01420224]] U_f = [[ 0.]] b_c = [ 0.35269699] b_f = [ 1.09452522]
W_c max, min, mean, abs_mean: 0.382201 -0.382338 -0.114732 0.379831
W_f max, min, mean, abs_mean: 0.118241 -0.118409 -0.0355693 0.116668
Epoch 13/300
1s - loss: 1599.8868 - val_loss: 7027.5962
Epoch 00012: val_loss improved from 7058.50285 to 7027.59624, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1518.3101      0.88  0.22  0.70      0.89  0.20  0.73      0.89  0.17  0.76
beijing_tanh+hardsigmoid8 7027.5963      0.84  0.12  0.76      0.84  0.08  0.78      0.86  0.06  0.82
forget mean min: 0.910325 0.272178
incx.max(), incx.min(), incx.mean() 8.05969 -7.35694 4.59426
fgtx.max(), fgtx.min(), fgtx.mean() 2.22418 -2.23076 1.22277
abs_mean, abs_mean+, abs_mean-: 9.74776 6.03961 19.1947
U_c = [[-0.01153961]] U_f = [[ 0.]] b_c = [ 0.36276844] b_f = [ 1.09165335]
W_c max, min, mean, abs_mean: 0.392469 -0.392607 -0.117812 0.390099
W_f max, min, mean, abs_mean: 0.114297 -0.114471 -0.0343917 0.112727
Epoch 14/300
1s - loss: 1595.5666 - val_loss: 6898.8388
Epoch 00013: val_loss improved from 7027.59624 to 6898.83876, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1510.4093      0.89  0.23  0.70      0.90  0.21  0.72      0.90  0.18  0.75
beijing_tanh+hardsigmoid8 6898.8386      0.85  0.11  0.77      0.85  0.08  0.79      0.87  0.06  0.83
forget mean min: 0.908833 0.25863
incx.max(), incx.min(), incx.mean() 8.26592 -7.54031 4.68811
fgtx.max(), fgtx.min(), fgtx.mean() 2.29562 -2.30112 1.25512
abs_mean, abs_mean+, abs_mean-: 10.2181 6.23466 21.8179
U_c = [[-0.00957048]] U_f = [[ 0.]] b_c = [ 0.37227345] b_f = [ 1.09427679]
W_c max, min, mean, abs_mean: 0.401993 -0.40213 -0.120667 0.399622
W_f max, min, mean, abs_mean: 0.117785 -0.117965 -0.0354397 0.116217
Epoch 15/300
1s - loss: 1583.9338 - val_loss: 6913.1988
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1574.6287 - val_loss: 7028.9146
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1551.8781 - val_loss: 7415.1689
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1559.5379 - val_loss: 7169.5310
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1558.0903 - val_loss: 7196.0203
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1537.1739 - val_loss: 7035.6884
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1530.5471 - val_loss: 7061.3285
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1510.4863 - val_loss: 6900.4006
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1497.2846 - val_loss: 6752.1082
Epoch 00022: val_loss improved from 6898.83876 to 6752.10819, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1396.1708      0.87  0.21  0.71      0.88  0.18  0.74      0.89  0.14  0.77
beijing_tanh+hardsigmoid8 6752.1082      0.84  0.11  0.76      0.84  0.09  0.78      0.87  0.06  0.82
forget mean min: 0.91927 0.204313
incx.max(), incx.min(), incx.mean() 9.07894 -8.14197 5.07726
fgtx.max(), fgtx.min(), fgtx.mean() 2.60317 -2.57452 1.4
abs_mean, abs_mean+, abs_mean-: 10.4675 6.57774 20.6626
U_c = [[-0.00680286]] U_f = [[ 0.]] b_c = [ 0.42086732] b_f = [ 1.09608364]
W_c max, min, mean, abs_mean: 0.443507 -0.443614 -0.133029 0.441139
W_f max, min, mean, abs_mean: 0.134468 -0.135456 -0.0405949 0.132635
Epoch 24/300
1s - loss: 1475.1792 - val_loss: 6758.8439
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1447.2486 - val_loss: 6644.6977
Epoch 00024: val_loss improved from 6752.10819 to 6644.69769, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1349.1343      0.88  0.21  0.71      0.89  0.18  0.74      0.90  0.15  0.78
beijing_tanh+hardsigmoid8 6644.6978      0.84  0.11  0.76      0.84  0.08  0.78      0.86  0.06  0.82
forget mean min: 0.916961 0.192522
incx.max(), incx.min(), incx.mean() 9.41091 -8.35042 5.11121
fgtx.max(), fgtx.min(), fgtx.mean() 2.67708 -2.62405 1.39372
abs_mean, abs_mean+, abs_mean-: 10.4218 6.62707 19.7473
U_c = [[-0.00753138]] U_f = [[ 0.]] b_c = [ 0.44146675] b_f = [ 1.08665371]
W_c max, min, mean, abs_mean: 0.460437 -0.460527 -0.138034 0.458086
W_f max, min, mean, abs_mean: 0.139098 -0.140994 -0.0422131 0.136725
Epoch 26/300
1s - loss: 1422.3301 - val_loss: 6544.5785
Epoch 00025: val_loss improved from 6644.69769 to 6544.57851, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1318.8373      0.88  0.20  0.72      0.89  0.17  0.75      0.90  0.14  0.78
beijing_tanh+hardsigmoid8 6544.5785      0.85  0.10  0.77      0.85  0.08  0.79      0.87  0.06  0.83
forget mean min: 0.918424 0.18302
incx.max(), incx.min(), incx.mean() 9.57123 -8.42095 5.19109
fgtx.max(), fgtx.min(), fgtx.mean() 2.73958 -2.66565 1.42362
abs_mean, abs_mean+, abs_mean-: 10.5153 6.65172 20.2567
U_c = [[-0.006421]] U_f = [[ 0.]] b_c = [ 0.45217249] b_f = [ 1.08074653]
W_c max, min, mean, abs_mean: 0.468541 -0.468639 -0.140396 0.466173
W_f max, min, mean, abs_mean: 0.142649 -0.145028 -0.0433686 0.140051
Epoch 27/300
1s - loss: 1395.8276 - val_loss: 6538.4732
Epoch 00026: val_loss improved from 6544.57851 to 6538.47317, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1316.8558      0.89  0.21  0.72      0.90  0.18  0.75      0.91  0.15  0.79
beijing_tanh+hardsigmoid8 6538.4731      0.85  0.10  0.78      0.85  0.08  0.80      0.88  0.05  0.84
forget mean min: 0.918523 0.166913
incx.max(), incx.min(), incx.mean() 9.8281 -8.57533 5.31132
fgtx.max(), fgtx.min(), fgtx.mean() 2.83985 -2.74246 1.46968
abs_mean, abs_mean+, abs_mean-: 10.759 6.80412 20.9499
U_c = [[-0.00374947]] U_f = [[ 0.]] b_c = [ 0.46595418] b_f = [ 1.07702398]
W_c max, min, mean, abs_mean: 0.480894 -0.48097 -0.144006 0.478446
W_f max, min, mean, abs_mean: 0.147964 -0.150892 -0.0450715 0.14513
Epoch 28/300
1s - loss: 1373.7332 - val_loss: 6414.8481
Epoch 00027: val_loss improved from 6538.47317 to 6414.84806, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1267.5282      0.88  0.19  0.72      0.89  0.16  0.76      0.90  0.13  0.79
beijing_tanh+hardsigmoid8 6414.8480      0.86  0.10  0.78      0.85  0.07  0.80      0.88  0.05  0.84
forget mean min: 0.917985 0.158809
incx.max(), incx.min(), incx.mean() 9.91334 -8.52817 5.25246
fgtx.max(), fgtx.min(), fgtx.mean() 2.91272 -2.77695 1.47461
abs_mean, abs_mean+, abs_mean-: 10.5626 6.7232 19.9309
U_c = [[-0.00549554]] U_f = [[ 0.]] b_c = [ 0.47269064] b_f = [ 1.07099676]
W_c max, min, mean, abs_mean: 0.486086 -0.486175 -0.145484 0.483563
W_f max, min, mean, abs_mean: 0.152244 -0.155725 -0.0464716 0.149195
Epoch 29/300
1s - loss: 1341.6319 - val_loss: 6208.5219
Epoch 00028: val_loss improved from 6414.84806 to 6208.52188, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1266.8594      0.88  0.20  0.72      0.90  0.17  0.76      0.91  0.14  0.79
beijing_tanh+hardsigmoid8 6208.5218      0.87  0.10  0.79      0.86  0.07  0.81      0.89  0.05  0.86
forget mean min: 0.91681 0.142672
incx.max(), incx.min(), incx.mean() 10.2359 -8.66736 5.3534
fgtx.max(), fgtx.min(), fgtx.mean() 3.03598 -2.85215 1.51502
abs_mean, abs_mean+, abs_mean-: 10.7592 6.8993 20.169
U_c = [[-0.00382323]] U_f = [[ 0.]] b_c = [ 0.48937374] b_f = [ 1.06551194]
W_c max, min, mean, abs_mean: 0.502547 -0.502625 -0.150284 0.499895
W_f max, min, mean, abs_mean: 0.159028 -0.163269 -0.0486606 0.155715
Epoch 30/300
1s - loss: 1303.1369 - val_loss: 6121.8583
Epoch 00029: val_loss improved from 6208.52188 to 6121.85830, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1215.2796      0.88  0.19  0.73      0.90  0.17  0.76      0.91  0.14  0.79
beijing_tanh+hardsigmoid8 6121.8583      0.88  0.09  0.81      0.88  0.06  0.83      0.90  0.04  0.87
forget mean min: 0.915013 0.150474
incx.max(), incx.min(), incx.mean() 10.559 -8.74772 5.4012
fgtx.max(), fgtx.min(), fgtx.mean() 3.04806 -2.80561 1.48417
abs_mean, abs_mean+, abs_mean-: 10.6967 6.99839 18.9553
U_c = [[-0.00593228]] U_f = [[ 0.]] b_c = [ 0.50602186] b_f = [ 1.05797517]
W_c max, min, mean, abs_mean: 0.51977 -0.519846 -0.155304 0.51697
W_f max, min, mean, abs_mean: 0.160456 -0.165336 -0.0492058 0.156747
Epoch 31/300
1s - loss: 1271.8220 - val_loss: 6136.7781
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1228.6444 - val_loss: 6299.2671
Epoch 00031: val_loss did not improve
Epoch 33/300
1s - loss: 1195.6728 - val_loss: 6162.3172
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1169.0007 - val_loss: 5864.2357
Epoch 00033: val_loss improved from 6121.85830 to 5864.23569, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1074.4600      0.89  0.17  0.75      0.90  0.15  0.78      0.91  0.12  0.81
beijing_tanh+hardsigmoid8 5864.2357      0.89  0.08  0.83      0.89  0.05  0.85      0.91  0.04  0.88
forget mean min: 0.907495 0.119656
incx.max(), incx.min(), incx.mean() 12.2615 -9.65599 5.76027
fgtx.max(), fgtx.min(), fgtx.mean() 3.33586 -2.92578 1.47862
abs_mean, abs_mean+, abs_mean-: 11.1289 7.70251 17.7005
U_c = [[-0.00908779]] U_f = [[ 0.]] b_c = [ 0.5853951] b_f = [ 1.02405763]
W_c max, min, mean, abs_mean: 0.609509 -0.609474 -0.181345 0.605851
W_f max, min, mean, abs_mean: 0.178544 -0.185952 -0.0551185 0.173096
Epoch 35/300
1s - loss: 1144.8201 - val_loss: 6031.9770
Epoch 00034: val_loss did not improve
Epoch 36/300
1s - loss: 1121.3962 - val_loss: 5512.1959
Epoch 00035: val_loss improved from 5864.23569 to 5512.19586, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1076.7590      0.89  0.18  0.75      0.91  0.15  0.79      0.91  0.12  0.82
beijing_tanh+hardsigmoid8 5512.1959      0.90  0.08  0.84      0.91  0.05  0.86      0.93  0.04  0.89
forget mean min: 0.905568 0.0859921
incx.max(), incx.min(), incx.mean() 12.893 -10.2454 5.93849
fgtx.max(), fgtx.min(), fgtx.mean() 3.48277 -3.08007 1.51038
abs_mean, abs_mean+, abs_mean-: 11.4357 8.03613 18.1096
U_c = [[-0.0123982]] U_f = [[ 0.]] b_c = [ 0.61430883] b_f = [ 1.01003253]
W_c max, min, mean, abs_mean: 0.643354 -0.643189 -0.191017 0.639231
W_f max, min, mean, abs_mean: 0.187602 -0.196467 -0.0581376 0.18132
Epoch 37/300
1s - loss: 1110.7625 - val_loss: 5770.7070
Epoch 00036: val_loss did not improve
Epoch 38/300
1s - loss: 1095.5676 - val_loss: 5726.5897
Epoch 00037: val_loss did not improve
Epoch 39/300
1s - loss: 1087.2589 - val_loss: 5965.3362
Epoch 00038: val_loss did not improve
Epoch 40/300
1s - loss: 1084.2009 - val_loss: 5591.7747
Epoch 00039: val_loss did not improve
Epoch 41/300
1s - loss: 1074.3431 - val_loss: 5603.7547
Epoch 00040: val_loss did not improve
Epoch 42/300
1s - loss: 1066.6456 - val_loss: 5741.9742
Epoch 00041: val_loss did not improve
Epoch 43/300
1s - loss: 1065.3108 - val_loss: 5372.0457
Epoch 00042: val_loss improved from 5512.19586 to 5372.04569, saving model to beijing_tanh+hardsigmoid8_weights.hdf5
beijing_tanh+hardsigmoid8 1050.1207      0.90  0.18  0.75      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid8 5372.0456      0.91  0.08  0.84      0.91  0.05  0.87      0.92  0.04  0.89
forget mean min: 0.903566 0.00898227
incx.max(), incx.min(), incx.mean() 14.1177 -11.4758 6.2023
fgtx.max(), fgtx.min(), fgtx.mean() 3.80325 -3.43911 1.56366
abs_mean, abs_mean+, abs_mean-: 11.7926 8.35807 18.3567
U_c = [[-0.01231514]] U_f = [[ 0.]] b_c = [ 0.67801321] b_f = [ 0.98401952]
W_c max, min, mean, abs_mean: 0.71647 -0.716321 -0.208548 0.703449
W_f max, min, mean, abs_mean: 0.208721 -0.22252 -0.065506 0.199089
Epoch 44/300
1s - loss: 1060.2019 - val_loss: 6031.3814
Epoch 00043: val_loss did not improve
Epoch 45/300
1s - loss: 1058.2130 - val_loss: 5684.0144
Epoch 00044: val_loss did not improve
Epoch 46/300
1s - loss: 1056.5394 - val_loss: 5424.0185
Epoch 00045: val_loss did not improve
Epoch 47/300
1s - loss: 1052.9496 - val_loss: 5925.2016
Epoch 00046: val_loss did not improve
Epoch 48/300
1s - loss: 1051.3638 - val_loss: 5491.4736
Epoch 00047: val_loss did not improve
Epoch 49/300
1s - loss: 1046.6428 - val_loss: 5698.7580
Epoch 00048: val_loss did not improve
Epoch 50/300
1s - loss: 1043.2447 - val_loss: 5663.3199
Epoch 00049: val_loss did not improve
Epoch 51/300
1s - loss: 1037.4644 - val_loss: 6454.3147
Epoch 00050: val_loss did not improve
Epoch 52/300
1s - loss: 1033.5017 - val_loss: 6029.6940
Epoch 00051: val_loss did not improve
Epoch 53/300
1s - loss: 1032.2587 - val_loss: 5716.1673
Epoch 00052: val_loss did not improve
Epoch 54/300
1s - loss: 1029.8563 - val_loss: 5962.4947
Epoch 00053: val_loss did not improve
Epoch 55/300
1s - loss: 1024.8025 - val_loss: 7182.0429
Epoch 00054: val_loss did not improve
Epoch 56/300
1s - loss: 1017.5475 - val_loss: 6141.3756
Epoch 00055: val_loss did not improve
Epoch 57/300
1s - loss: 1014.1281 - val_loss: 6310.9484
Epoch 00056: val_loss did not improve
Epoch 58/300
1s - loss: 1009.7079 - val_loss: 6537.0722
Epoch 00057: val_loss did not improve
Epoch 59/300
1s - loss: 1010.6488 - val_loss: 6459.0892
Epoch 00058: val_loss did not improve
Epoch 60/300
1s - loss: 1004.5076 - val_loss: 5869.4695
Epoch 00059: val_loss did not improve
Epoch 61/300
1s - loss: 998.4437 - val_loss: 5747.0057
Epoch 00060: val_loss did not improve
Epoch 62/300
1s - loss: 990.9331 - val_loss: 6972.2324
Epoch 00061: val_loss did not improve
Epoch 63/300
1s - loss: 989.8091 - val_loss: 7102.2298
Epoch 00062: val_loss did not improve
Epoch 64/300
1s - loss: 978.9419 - val_loss: 6857.8273
Epoch 00063: val_loss did not improve
X_train[0].shape = (7032, 40, 23)

training beijing_tanh+hardsigmoid9
Train on 7032 samples, validate on 1392 samples
Before training:
beijing_tanh+hardsigmoid9 9689.3936      0.03  -nan  0.02      0.03  -nan  0.02      0.00  -nan  0.00
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 3.9477 nan 3.9477
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
beijing_tanh+hardsigmoid929822.4830      0.05  -nan  0.05      0.06  -nan  0.06      0.04  -nan  0.04
forget mean min: 0.7 0.7
incx.max(), incx.min(), incx.mean() 0.0 0.0 0.0
fgtx.max(), fgtx.min(), fgtx.mean() 0.0 0.0 0.0
abs_mean, abs_mean+, abs_mean-: 8.27027 nan 8.27027
U_c = [[-0.05]] U_f = [[ 0.]] b_c = [ 0.] b_f = [ 1.]
W_c max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
W_f max, min, mean, abs_mean: 0.0 0.0 0.0 0.0
Epoch 1/300
1s - loss: 5857.5691 - val_loss: 9770.0207
Epoch 00000: val_loss improved from inf to 9770.02073, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 3267.7636      0.55  0.21  0.49      0.56  0.19  0.49      0.53  0.17  0.48
beijing_tanh+hardsigmoid9 9770.0206      0.78  0.09  0.73      0.78  0.05  0.75      0.79  0.03  0.77
forget mean min: 0.846757 0.318295
incx.max(), incx.min(), incx.mean() 2.93979 -2.76901 1.36328
fgtx.max(), fgtx.min(), fgtx.mean() 1.95135 -2.00403 0.85905
abs_mean, abs_mean+, abs_mean-: 6.50703 2.63191 12.576
U_c = [[-0.05936032]] U_f = [[ 0.]] b_c = [ 0.12340704] b_f = [ 1.09550607]
W_c max, min, mean, abs_mean: 0.149792 -0.14992 -0.000108588 0.148535
W_f max, min, mean, abs_mean: 0.104262 -0.104209 -8.30293e-06 0.102914
Epoch 2/300
1s - loss: 2438.5431 - val_loss: 10494.8200
Epoch 00001: val_loss did not improve
Epoch 3/300
1s - loss: 1874.1643 - val_loss: 9537.1258
Epoch 00002: val_loss improved from 9770.02073 to 9537.12581, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1739.1929      0.85  0.24  0.67      0.86  0.22  0.69      0.86  0.19  0.72
beijing_tanh+hardsigmoid9 9537.1258      0.80  0.15  0.70      0.80  0.12  0.72      0.83  0.09  0.77
forget mean min: 0.912817 0.414121
incx.max(), incx.min(), incx.mean() 6.29917 -5.74504 4.20047
fgtx.max(), fgtx.min(), fgtx.mean() 1.54 -1.54021 1.00326
abs_mean, abs_mean+, abs_mean-: 8.82754 5.36927 18.1442
U_c = [[-0.01218668]] U_f = [[ 0.]] b_c = [ 0.27748838] b_f = [ 1.11081815]
W_c max, min, mean, abs_mean: 0.30358 -0.303706 -0.000103745 0.302329
W_f max, min, mean, abs_mean: 0.0786727 -0.0785957 -8.85464e-06 0.0773183
Epoch 4/300
1s - loss: 1734.7240 - val_loss: 8646.3117
Epoch 00003: val_loss improved from 9537.12581 to 8646.31168, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1662.5974      0.88  0.25  0.68      0.88  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid9 8646.3117      0.83  0.15  0.73      0.84  0.12  0.75      0.86  0.09  0.79
forget mean min: 0.911148 0.370944
incx.max(), incx.min(), incx.mean() 6.85161 -6.25038 4.47822
fgtx.max(), fgtx.min(), fgtx.mean() 1.75387 -1.75538 1.11817
abs_mean, abs_mean+, abs_mean-: 9.95609 6.05096 22.8949
U_c = [[-0.01702661]] U_f = [[ 0.]] b_c = [ 0.30343914] b_f = [ 1.11010098]
W_c max, min, mean, abs_mean: 0.329808 -0.329935 -0.00010265 0.328556
W_f max, min, mean, abs_mean: 0.0893674 -0.0892871 -6.10501e-06 0.0880009
Epoch 5/300
1s - loss: 1680.8626 - val_loss: 7743.5485
Epoch 00004: val_loss improved from 8646.31168 to 7743.54847, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1616.3103      0.89  0.25  0.68      0.89  0.23  0.70      0.90  0.20  0.73
beijing_tanh+hardsigmoid9 7743.5486      0.85  0.14  0.75      0.86  0.11  0.77      0.88  0.08  0.82
forget mean min: 0.908611 0.34391
incx.max(), incx.min(), incx.mean() 7.08381 -6.46665 4.54719
fgtx.max(), fgtx.min(), fgtx.mean() 1.87968 -1.88274 1.17536
abs_mean, abs_mean+, abs_mean-: 10.1608 6.19582 23.5426
U_c = [[-0.02131327]] U_f = [[ 0.]] b_c = [ 0.31407562] b_f = [ 1.10228384]
W_c max, min, mean, abs_mean: 0.341079 -0.341207 -0.000101678 0.339826
W_f max, min, mean, abs_mean: 0.0957351 -0.0956535 -2.87145e-06 0.0943562
Epoch 6/300
1s - loss: 1660.0703 - val_loss: 7593.6028
Epoch 00005: val_loss improved from 7743.54847 to 7593.60277, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1610.3282      0.89  0.25  0.68      0.89  0.23  0.70      0.89  0.20  0.73
beijing_tanh+hardsigmoid9 7593.6027      0.85  0.13  0.75      0.85  0.10  0.77      0.88  0.08  0.82
forget mean min: 0.902396 0.320832
incx.max(), incx.min(), incx.mean() 7.28969 -6.66132 4.55613
fgtx.max(), fgtx.min(), fgtx.mean() 1.98583 -1.99099 1.20661
abs_mean, abs_mean+, abs_mean-: 10.4756 6.34405 24.67
U_c = [[-0.01857644]] U_f = [[ 0.]] b_c = [ 0.32323483] b_f = [ 1.09515405]
W_c max, min, mean, abs_mean: 0.351254 -0.351382 -0.000100194 0.349999
W_f max, min, mean, abs_mean: 0.101161 -0.10108 6.41868e-07 0.0997696
Epoch 7/300
1s - loss: 1650.4135 - val_loss: 7369.3871
Epoch 00006: val_loss improved from 7593.60277 to 7369.38713, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1587.2426      0.89  0.25  0.69      0.89  0.23  0.70      0.90  0.20  0.74
beijing_tanh+hardsigmoid9 7369.3871      0.85  0.14  0.75      0.85  0.10  0.78      0.88  0.08  0.82
forget mean min: 0.905873 0.304626
incx.max(), incx.min(), incx.mean() 7.37754 -6.75334 4.59879
fgtx.max(), fgtx.min(), fgtx.mean() 2.05857 -2.068 1.24709
abs_mean, abs_mean+, abs_mean-: 10.4144 6.28553 24.6419
U_c = [[-0.01930468]] U_f = [[ 0.]] b_c = [ 0.32825232] b_f = [ 1.09113264]
W_c max, min, mean, abs_mean: 0.356421 -0.356548 -9.88483e-05 0.355164
W_f max, min, mean, abs_mean: 0.105118 -0.105037 3.60087e-06 0.103717
Epoch 8/300
1s - loss: 1636.5729 - val_loss: 7331.8755
Epoch 00007: val_loss improved from 7369.38713 to 7331.87547, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1569.2693      0.88  0.23  0.69      0.88  0.21  0.71      0.89  0.18  0.74
beijing_tanh+hardsigmoid9 7331.8755      0.84  0.13  0.75      0.84  0.10  0.77      0.87  0.07  0.82
forget mean min: 0.905968 0.290022
incx.max(), incx.min(), incx.mean() 7.41684 -6.80194 4.51168
fgtx.max(), fgtx.min(), fgtx.mean() 2.12294 -2.13682 1.25258
abs_mean, abs_mean+, abs_mean-: 10.1955 6.16864 22.8576
U_c = [[-0.01725426]] U_f = [[ 0.]] b_c = [ 0.3306278] b_f = [ 1.08693528]
W_c max, min, mean, abs_mean: 0.359364 -0.35949 -9.74625e-05 0.358105
W_f max, min, mean, abs_mean: 0.108696 -0.108617 7.27959e-06 0.107284
Epoch 9/300
1s - loss: 1629.0438 - val_loss: 7317.7237
Epoch 00008: val_loss improved from 7331.87547 to 7317.72373, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1582.8336      0.88  0.23  0.69      0.89  0.21  0.72      0.89  0.18  0.74
beijing_tanh+hardsigmoid9 7317.7237      0.85  0.13  0.75      0.85  0.10  0.77      0.88  0.08  0.82
forget mean min: 0.90616 0.269065
incx.max(), incx.min(), incx.mean() 7.55778 -6.92445 4.61746
fgtx.max(), fgtx.min(), fgtx.mean() 2.23084 -2.24362 1.32239
abs_mean, abs_mean+, abs_mean-: 10.5439 6.29142 25.831
U_c = [[-0.01586711]] U_f = [[ 0.]] b_c = [ 0.33733344] b_f = [ 1.08894193]
W_c max, min, mean, abs_mean: 0.365989 -0.366113 -9.63524e-05 0.364727
W_f max, min, mean, abs_mean: 0.114105 -0.114028 9.44622e-06 0.112687
Epoch 10/300
1s - loss: 1625.3892 - val_loss: 7179.1453
Epoch 00009: val_loss improved from 7317.72373 to 7179.14531, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1553.5974      0.88  0.23  0.70      0.89  0.21  0.72      0.89  0.18  0.75
beijing_tanh+hardsigmoid9 7179.1454      0.84  0.13  0.75      0.85  0.10  0.78      0.87  0.07  0.82
forget mean min: 0.908417 0.260029
incx.max(), incx.min(), incx.mean() 7.61094 -6.97472 4.54287
fgtx.max(), fgtx.min(), fgtx.mean() 2.27479 -2.28846 1.31491
abs_mean, abs_mean+, abs_mean-: 10.1912 6.1182 23.7834
U_c = [[-0.0166573]] U_f = [[ 0.]] b_c = [ 0.33996108] b_f = [ 1.08860278]
W_c max, min, mean, abs_mean: 0.36912 -0.369239 -9.5053e-05 0.367853
W_f max, min, mean, abs_mean: 0.116508 -0.116431 1.17972e-05 0.115086
Epoch 11/300
1s - loss: 1617.8113 - val_loss: 7057.5333
Epoch 00010: val_loss improved from 7179.14531 to 7057.53327, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1544.5660      0.88  0.23  0.70      0.88  0.20  0.72      0.89  0.17  0.75
beijing_tanh+hardsigmoid9 7057.5333      0.85  0.13  0.76      0.85  0.10  0.78      0.88  0.07  0.83
forget mean min: 0.909936 0.26353
incx.max(), incx.min(), incx.mean() 7.74515 -7.09499 4.5725
fgtx.max(), fgtx.min(), fgtx.mean() 2.25738 -2.27064 1.28933
abs_mean, abs_mean+, abs_mean-: 10.0853 6.13294 22.3192
U_c = [[-0.01565041]] U_f = [[ 0.]] b_c = [ 0.34680742] b_f = [ 1.08828878]
W_c max, min, mean, abs_mean: 0.375979 -0.376095 -9.40278e-05 0.374708
W_f max, min, mean, abs_mean: 0.115754 -0.11568 1.23285e-05 0.114331
Epoch 12/300
1s - loss: 1609.5039 - val_loss: 7068.8865
Epoch 00011: val_loss did not improve
Epoch 13/300
1s - loss: 1602.6261 - val_loss: 6923.5561
Epoch 00012: val_loss improved from 7057.53327 to 6923.55605, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1518.6395      0.89  0.23  0.70      0.89  0.20  0.73      0.90  0.17  0.75
beijing_tanh+hardsigmoid9 6923.5561      0.85  0.12  0.76      0.85  0.09  0.79      0.88  0.06  0.83
forget mean min: 0.910926 0.263969
incx.max(), incx.min(), incx.mean() 8.02758 -7.33926 4.61277
fgtx.max(), fgtx.min(), fgtx.mean() 2.26372 -2.27355 1.25544
abs_mean, abs_mean+, abs_mean-: 9.97429 6.06813 21.5427
U_c = [[-0.01211626]] U_f = [[ 0.]] b_c = [ 0.3608025] b_f = [ 1.09339702]
W_c max, min, mean, abs_mean: 0.389941 -0.390051 -9.21041e-05 0.388661
W_f max, min, mean, abs_mean: 0.116183 -0.116111 1.30646e-05 0.114757
Epoch 14/300
1s - loss: 1589.8991 - val_loss: 7037.1060
Epoch 00013: val_loss did not improve
Epoch 15/300
1s - loss: 1584.2586 - val_loss: 7252.5257
Epoch 00014: val_loss did not improve
Epoch 16/300
1s - loss: 1573.4929 - val_loss: 7307.9220
Epoch 00015: val_loss did not improve
Epoch 17/300
1s - loss: 1566.1722 - val_loss: 7147.3127
Epoch 00016: val_loss did not improve
Epoch 18/300
1s - loss: 1575.1229 - val_loss: 7132.9263
Epoch 00017: val_loss did not improve
Epoch 19/300
1s - loss: 1557.2161 - val_loss: 7514.7996
Epoch 00018: val_loss did not improve
Epoch 20/300
1s - loss: 1566.4287 - val_loss: 7258.1493
Epoch 00019: val_loss did not improve
Epoch 21/300
1s - loss: 1564.4202 - val_loss: 7282.2255
Epoch 00020: val_loss did not improve
Epoch 22/300
1s - loss: 1558.3739 - val_loss: 7384.4117
Epoch 00021: val_loss did not improve
Epoch 23/300
1s - loss: 1549.9029 - val_loss: 7226.9003
Epoch 00022: val_loss did not improve
Epoch 24/300
1s - loss: 1535.6417 - val_loss: 7390.0822
Epoch 00023: val_loss did not improve
Epoch 25/300
1s - loss: 1527.6684 - val_loss: 7092.0942
Epoch 00024: val_loss did not improve
Epoch 26/300
1s - loss: 1509.2513 - val_loss: 6975.6479
Epoch 00025: val_loss did not improve
Epoch 27/300
1s - loss: 1490.3049 - val_loss: 7183.6175
Epoch 00026: val_loss did not improve
Epoch 28/300
1s - loss: 1459.2003 - val_loss: 6827.3212
Epoch 00027: val_loss improved from 6923.55605 to 6827.32119, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1354.6534      0.87  0.20  0.71      0.88  0.17  0.75      0.89  0.14  0.78
beijing_tanh+hardsigmoid9 6827.3213      0.84  0.10  0.77      0.84  0.07  0.78      0.86  0.05  0.82
forget mean min: 0.909911 0.190837
incx.max(), incx.min(), incx.mean() 9.37028 -8.35683 4.90241
fgtx.max(), fgtx.min(), fgtx.mean() 2.68994 -2.64907 1.34431
abs_mean, abs_mean+, abs_mean-: 10.3251 6.55668 19.3066
U_c = [[-0.00669691]] U_f = [[ 0.]] b_c = [ 0.43887427] b_f = [ 1.10325706]
W_c max, min, mean, abs_mean: 0.458824 -0.459083 -5.2692e-05 0.45753
W_f max, min, mean, abs_mean: 0.139427 -0.140285 -0.000213046 0.137798
Epoch 29/300
1s - loss: 1430.4793 - val_loss: 6607.9512
Epoch 00028: val_loss improved from 6827.32119 to 6607.95116, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1340.9920      0.88  0.22  0.71      0.90  0.19  0.74      0.91  0.15  0.78
beijing_tanh+hardsigmoid9 6607.9512      0.85  0.10  0.78      0.85  0.07  0.80      0.87  0.05  0.84
forget mean min: 0.909716 0.172075
incx.max(), incx.min(), incx.mean() 9.63948 -8.53691 5.00046
fgtx.max(), fgtx.min(), fgtx.mean() 2.80055 -2.74082 1.38626
abs_mean, abs_mean+, abs_mean-: 10.5664 6.74204 19.9043
U_c = [[-0.00505122]] U_f = [[ 0.]] b_c = [ 0.45330527] b_f = [ 1.10118914]
W_c max, min, mean, abs_mean: 0.472694 -0.472986 -3.8895e-05 0.471329
W_f max, min, mean, abs_mean: 0.145454 -0.146581 -0.000281976 0.143692
Epoch 30/300
1s - loss: 1400.7902 - val_loss: 6987.8048
Epoch 00029: val_loss did not improve
Epoch 31/300
1s - loss: 1363.2081 - val_loss: 6627.2109
Epoch 00030: val_loss did not improve
Epoch 32/300
1s - loss: 1323.7807 - val_loss: 6462.5660
Epoch 00031: val_loss improved from 6607.95116 to 6462.56598, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1223.3352      0.88  0.20  0.72      0.89  0.17  0.76      0.90  0.14  0.79
beijing_tanh+hardsigmoid9 6462.5660      0.86  0.08  0.80      0.86  0.05  0.82      0.88  0.04  0.85
forget mean min: 0.898348 0.170659
incx.max(), incx.min(), incx.mean() 10.6854 -9.05997 5.0918
fgtx.max(), fgtx.min(), fgtx.mean() 2.90053 -2.72563 1.30674
abs_mean, abs_mean+, abs_mean-: 10.6036 7.09264 17.6273
U_c = [[-0.00515172]] U_f = [[ 0.]] b_c = [ 0.50572807] b_f = [ 1.07893014]
W_c max, min, mean, abs_mean: 0.528304 -0.528543 3.92914e-05 0.526177
W_f max, min, mean, abs_mean: 0.151958 -0.154103 -0.000483045 0.149926
Epoch 33/300
1s - loss: 1279.0348 - val_loss: 6839.3991
Epoch 00032: val_loss did not improve
Epoch 34/300
1s - loss: 1245.2673 - val_loss: 6687.5689
Epoch 00033: val_loss did not improve
Epoch 35/300
1s - loss: 1206.7643 - val_loss: 6048.4193
Epoch 00034: val_loss improved from 6462.56598 to 6048.41934, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1137.9133      0.88  0.19  0.73      0.90  0.16  0.76      0.90  0.13  0.79
beijing_tanh+hardsigmoid9 6048.4193      0.88  0.08  0.82      0.88  0.05  0.84      0.90  0.04  0.87
forget mean min: 0.893362 0.160608
incx.max(), incx.min(), incx.mean() 11.9803 -9.50419 5.36422
fgtx.max(), fgtx.min(), fgtx.mean() 3.12149 -2.75385 1.31227
abs_mean, abs_mean+, abs_mean-: 11.0332 7.53537 17.933
U_c = [[-0.00673239]] U_f = [[ 0.]] b_c = [ 0.56605989] b_f = [ 1.05688488]
W_c max, min, mean, abs_mean: 0.596813 -0.596934 0.000143707 0.593559
W_f max, min, mean, abs_mean: 0.164806 -0.167841 -0.000677101 0.162323
Epoch 36/300
1s - loss: 1176.0515 - val_loss: 6198.2320
Epoch 00035: val_loss did not improve
Epoch 37/300
1s - loss: 1158.0431 - val_loss: 5647.6483
Epoch 00036: val_loss improved from 6048.41934 to 5647.64832, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1100.8319      0.89  0.19  0.74      0.91  0.16  0.78      0.92  0.13  0.80
beijing_tanh+hardsigmoid9 5647.6483      0.89  0.08  0.83      0.90  0.05  0.86      0.92  0.04  0.89
forget mean min: 0.895139 0.136489
incx.max(), incx.min(), incx.mean() 12.732 -9.89491 5.61469
fgtx.max(), fgtx.min(), fgtx.mean() 3.30842 -2.86258 1.36744
abs_mean, abs_mean+, abs_mean-: 11.2714 7.81644 18.1896
U_c = [[-0.01113864]] U_f = [[ 0.]] b_c = [ 0.60140461] b_f = [ 1.04502702]
W_c max, min, mean, abs_mean: 0.638119 -0.638028 0.000239459 0.633916
W_f max, min, mean, abs_mean: 0.175985 -0.179209 -0.000802433 0.172892
Epoch 38/300
1s - loss: 1139.6561 - val_loss: 6357.7688
Epoch 00037: val_loss did not improve
Epoch 39/300
1s - loss: 1124.4515 - val_loss: 5632.9931
Epoch 00038: val_loss improved from 5647.64832 to 5632.99305, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1040.1767      0.89  0.17  0.75      0.91  0.13  0.79      0.91  0.10  0.82
beijing_tanh+hardsigmoid9 5632.9931      0.90  0.08  0.84      0.90  0.05  0.86      0.92  0.04  0.89
forget mean min: 0.896571 0.127469
incx.max(), incx.min(), incx.mean() 13.2181 -10.0416 5.72749
fgtx.max(), fgtx.min(), fgtx.mean() 3.42283 -2.89876 1.38717
abs_mean, abs_mean+, abs_mean-: 11.2556 7.89044 17.5697
U_c = [[-0.01239229]] U_f = [[ 0.]] b_c = [ 0.62439096] b_f = [ 1.03610289]
W_c max, min, mean, abs_mean: 0.664692 -0.664175 0.000348654 0.659592
W_f max, min, mean, abs_mean: 0.182892 -0.186207 -0.000907434 0.179272
Epoch 40/300
1s - loss: 1118.6771 - val_loss: 5627.5426
Epoch 00039: val_loss improved from 5632.99305 to 5627.54256, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1061.1240      0.90  0.18  0.75      0.91  0.15  0.79      0.92  0.11  0.82
beijing_tanh+hardsigmoid9 5627.5425      0.90  0.08  0.84      0.90  0.05  0.86      0.91  0.04  0.88
forget mean min: 0.898018 0.106535
incx.max(), incx.min(), incx.mean() 13.4534 -10.3147 5.8423
fgtx.max(), fgtx.min(), fgtx.mean() 3.51408 -3.00133 1.42787
abs_mean, abs_mean+, abs_mean-: 11.4046 7.97524 17.9547
U_c = [[-0.01180079]] U_f = [[ 0.]] b_c = [ 0.63439292] b_f = [ 1.03400111]
W_c max, min, mean, abs_mean: 0.676579 -0.675766 0.000412977 0.670985
W_f max, min, mean, abs_mean: 0.187584 -0.191023 -0.000934418 0.183939
Epoch 41/300
1s - loss: 1120.5142 - val_loss: 5543.3417
Epoch 00040: val_loss improved from 5627.54256 to 5543.34175, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1038.7803      0.88  0.17  0.75      0.90  0.14  0.79      0.91  0.11  0.82
beijing_tanh+hardsigmoid9 5543.3417      0.89  0.08  0.83      0.90  0.05  0.86      0.91  0.04  0.88
forget mean min: 0.892961 0.106957
incx.max(), incx.min(), incx.mean() 13.5391 -10.1833 5.72041
fgtx.max(), fgtx.min(), fgtx.mean() 3.57277 -2.99709 1.40758
abs_mean, abs_mean+, abs_mean-: 11.4094 7.98969 17.8833
U_c = [[-0.0146346]] U_f = [[ 0.]] b_c = [ 0.63897032] b_f = [ 1.03187561]
W_c max, min, mean, abs_mean: 0.68223 -0.681163 0.000473815 0.676147
W_f max, min, mean, abs_mean: 0.191533 -0.194735 -0.00101434 0.187265
Epoch 42/300
1s - loss: 1108.5695 - val_loss: 5449.5803
Epoch 00041: val_loss improved from 5543.34175 to 5449.58027, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1050.6317      0.90  0.19  0.74      0.93  0.15  0.79      0.93  0.12  0.83
beijing_tanh+hardsigmoid9 5449.5803      0.90  0.08  0.84      0.90  0.05  0.86      0.92  0.04  0.89
forget mean min: 0.895508 0.0942286
incx.max(), incx.min(), incx.mean() 13.7813 -10.3812 5.87034
fgtx.max(), fgtx.min(), fgtx.mean() 3.6419 -3.05943 1.44805
abs_mean, abs_mean+, abs_mean-: 11.5308 8.14693 17.9229
U_c = [[-0.01142915]] U_f = [[ 0.]] b_c = [ 0.65039808] b_f = [ 1.03056967]
W_c max, min, mean, abs_mean: 0.694672 -0.693149 0.00056195 0.687953
W_f max, min, mean, abs_mean: 0.195351 -0.198559 -0.00107255 0.190809
Epoch 43/300
1s - loss: 1102.4919 - val_loss: 5320.2518
Epoch 00042: val_loss improved from 5449.58027 to 5320.25178, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1019.6046      0.89  0.17  0.75      0.91  0.13  0.80      0.91  0.10  0.83
beijing_tanh+hardsigmoid9 5320.2518      0.90  0.08  0.84      0.91  0.05  0.87      0.92  0.04  0.89
forget mean min: 0.896813 0.0914646
incx.max(), incx.min(), incx.mean() 13.8383 -10.3369 5.88724
fgtx.max(), fgtx.min(), fgtx.mean() 3.68319 -3.07007 1.46233
abs_mean, abs_mean+, abs_mean-: 11.4997 8.09906 18.0338
U_c = [[-0.01467447]] U_f = [[ 0.]] b_c = [ 0.65377736] b_f = [ 1.02739584]
W_c max, min, mean, abs_mean: 0.699245 -0.697362 0.000633666 0.692023
W_f max, min, mean, abs_mean: 0.198147 -0.201422 -0.00114031 0.193324
Epoch 44/300
1s - loss: 1095.3228 - val_loss: 5561.2623
Epoch 00043: val_loss did not improve
Epoch 45/300
1s - loss: 1088.2102 - val_loss: 5094.6974
Epoch 00044: val_loss improved from 5320.25178 to 5094.69735, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1067.3403      0.90  0.18  0.75      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid9 5094.6972      0.91  0.08  0.84      0.91  0.05  0.87      0.93  0.04  0.90
forget mean min: 0.896053 0.0790004
incx.max(), incx.min(), incx.mean() 14.1051 -10.5541 5.99623
fgtx.max(), fgtx.min(), fgtx.mean() 3.74811 -3.13008 1.48664
abs_mean, abs_mean+, abs_mean-: 11.7417 8.22838 19.0159
U_c = [[-0.01302091]] U_f = [[ 0.]] b_c = [ 0.66824806] b_f = [ 1.02507997]
W_c max, min, mean, abs_mean: 0.715203 -0.712127 0.000843281 0.706634
W_f max, min, mean, abs_mean: 0.202913 -0.206182 -0.00134481 0.197112
Epoch 46/300
1s - loss: 1091.3601 - val_loss: 5300.9783
Epoch 00045: val_loss did not improve
Epoch 47/300
1s - loss: 1086.1335 - val_loss: 4947.7342
Epoch 00046: val_loss improved from 5094.69735 to 4947.73423, saving model to beijing_tanh+hardsigmoid9_weights.hdf5
beijing_tanh+hardsigmoid9 1081.0661      0.91  0.19  0.75      0.92  0.16  0.79      0.92  0.13  0.81
beijing_tanh+hardsigmoid9 4947.7342      0.91  0.08  0.85      0.92  0.05  0.87      0.94  0.04  0.90
forget mean min: 0.898729 0.0626383
incx.max(), incx.min(), incx.mean() 14.3187 -10.7149 6.1204
fgtx.max(), fgtx.min(), fgtx.mean() 3.84299 -3.21045 1.53354
abs_mean, abs_mean+, abs_mean-: 11.8526 8.30794 19.4667
U_c = [[-0.01505604]] U_f = [[ 0.]] b_c = [ 0.6801793] b_f = [ 1.02363694]
W_c max, min, mean, abs_mean: 0.72753 -0.723124 0.00106407 0.717609
W_f max, min, mean, abs_mean: 0.209078 -0.212263 -0.00157195 0.202203
Epoch 48/300
1s - loss: 1078.8278 - val_loss: 5058.7306
Epoch 00047: val_loss did not improve
Epoch 49/300
1s - loss: 1072.3696 - val_loss: 5458.6937
Epoch 00048: val_loss did not improve
Epoch 50/300
1s - loss: 1064.9308 - val_loss: 5389.5759
Epoch 00049: val_loss did not improve
Epoch 51/300
1s - loss: 1065.4047 - val_loss: 5501.8703
Epoch 00050: val_loss did not improve
Epoch 52/300
1s - loss: 1061.5102 - val_loss: 5474.3312
Epoch 00051: val_loss did not improve
Epoch 53/300
1s - loss: 1052.9135 - val_loss: 5060.3274
Epoch 00052: val_loss did not improve
Epoch 54/300
1s - loss: 1042.9387 - val_loss: 5639.2109
Epoch 00053: val_loss did not improve
Epoch 55/300
1s - loss: 1040.3942 - val_loss: 5637.4097
Epoch 00054: val_loss did not improve
Epoch 56/300
1s - loss: 1037.0657 - val_loss: 5491.0188
Epoch 00055: val_loss did not improve
Epoch 57/300
1s - loss: 1022.4414 - val_loss: 5424.2307
Epoch 00056: val_loss did not improve
Epoch 58/300
1s - loss: 1019.6848 - val_loss: 5291.7688
Epoch 00057: val_loss did not improve
Epoch 59/300
1s - loss: 1010.8524 - val_loss: 5473.3557
Epoch 00058: val_loss did not improve
Epoch 60/300
1s - loss: 1007.6812 - val_loss: 5726.4285
Epoch 00059: val_loss did not improve
Epoch 61/300
1s - loss: 995.4542 - val_loss: 7116.4444
Epoch 00060: val_loss did not improve
Epoch 62/300
1s - loss: 991.0932 - val_loss: 5935.5207
Epoch 00061: val_loss did not improve
Epoch 63/300
1s - loss: 990.2139 - val_loss: 6854.2445
Epoch 00062: val_loss did not improve
Epoch 64/300
1s - loss: 982.8231 - val_loss: 6245.5812
Epoch 00063: val_loss did not improve
Epoch 65/300
1s - loss: 969.9126 - val_loss: 5839.7825
Epoch 00064: val_loss did not improve
Epoch 66/300
1s - loss: 962.7323 - val_loss: 6850.7385
Epoch 00065: val_loss did not improve
Epoch 67/300
1s - loss: 959.0572 - val_loss: 6348.6998
Epoch 00066: val_loss did not improve
Epoch 68/300
1s - loss: 954.7150 - val_loss: 6133.6070
Epoch 00067: val_loss did not improve

beijing_tanh+hardsigmoid0
beijing_tanh+hardsigmoid0 1507.8198      0.88  0.23  0.70      0.89  0.20  0.73      0.90  0.17  0.75
beijing_tanh+hardsigmoid0 7076.9953      0.84  0.11  0.76      0.84  0.08  0.78      0.86  0.06  0.82
forget mean min: 0.908519 0.273913
incx.max(), incx.min(), incx.mean() 8.00087 -7.31164 4.60337
fgtx.max(), fgtx.min(), fgtx.mean() 2.20951 -2.21664 1.22746
abs_mean, abs_mean+, abs_mean-: 9.93869 6.05012 20.8213
U_c = [[-0.0091265]] U_f = [[ 0.]] b_c = [ 0.35694259] b_f = [ 1.08620012]
W_c max, min, mean, abs_mean: 0.388487 -0.388036 0.00123714 0.386521
W_f max, min, mean, abs_mean: 0.113407 -0.113614 0.000659654 0.111726

beijing_tanh+hardsigmoid1
beijing_tanh+hardsigmoid1 1141.8007      0.91  0.19  0.75      0.93  0.16  0.79      0.92  0.13  0.81
beijing_tanh+hardsigmoid1 5818.1990      0.91  0.08  0.84      0.91  0.05  0.86      0.92  0.04  0.89
forget mean min: 0.898504 0.0
incx.max(), incx.min(), incx.mean() 15.3705 -11.5905 6.34339
fgtx.max(), fgtx.min(), fgtx.mean() 4.16894 -3.5341 1.59027
abs_mean, abs_mean+, abs_mean-: 11.9131 8.54835 18.5714
U_c = [[-0.01504009]] U_f = [[ 0.]] b_c = [ 0.78129858] b_f = [ 0.97997832]
W_c max, min, mean, abs_mean: 0.808746 -0.813315 -0.000211734 0.791217
W_f max, min, mean, abs_mean: 0.240006 -0.247891 -0.00103771 0.226078

beijing_tanh+hardsigmoid2
beijing_tanh+hardsigmoid2  987.8508      0.90  0.17  0.76      0.92  0.14  0.80      0.92  0.11  0.83
beijing_tanh+hardsigmoid2 5259.6670      0.91  0.09  0.84      0.92  0.06  0.87      0.94  0.04  0.90
forget mean min: 0.902411 0.0810632
incx.max(), incx.min(), incx.mean() 14.629 -10.7114 6.28825
fgtx.max(), fgtx.min(), fgtx.mean() 3.81495 -3.12563 1.53257
abs_mean, abs_mean+, abs_mean-: 12.1074 8.48589 20.1734
U_c = [[-0.0133762]] U_f = [[ 0.]] b_c = [ 0.69940996] b_f = [ 1.03094757]
W_c max, min, mean, abs_mean: 0.735316 -0.745107 -0.148416 0.726839
W_f max, min, mean, abs_mean: 0.208831 -0.208374 -0.0406762 0.19907

beijing_tanh+hardsigmoid3
beijing_tanh+hardsigmoid3 1508.7489      0.88  0.23  0.70      0.89  0.20  0.73      0.89  0.17  0.75
beijing_tanh+hardsigmoid3 6931.4395      0.84  0.11  0.76      0.84  0.08  0.78      0.87  0.06  0.83
forget mean min: 0.907469 0.267745
incx.max(), incx.min(), incx.mean() 8.14567 -7.44364 4.63373
fgtx.max(), fgtx.min(), fgtx.mean() 2.2434 -2.25061 1.23099
abs_mean, abs_mean+, abs_mean-: 10.1031 6.13521 21.6122
U_c = [[-0.01082286]] U_f = [[ 0.]] b_c = [ 0.36352196] b_f = [ 1.08933437]
W_c max, min, mean, abs_mean: 0.394568 -0.394835 -0.000412104 0.393677
W_f max, min, mean, abs_mean: 0.11418 -0.11414 -0.000206615 0.113487

beijing_tanh+hardsigmoid4
beijing_tanh+hardsigmoid4 1058.8500      0.90  0.18  0.75      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid4 5552.7744      0.89  0.08  0.83      0.90  0.05  0.86      0.91  0.04  0.88
forget mean min: 0.895942 0.0726809
incx.max(), incx.min(), incx.mean() 14.3262 -10.9341 6.06567
fgtx.max(), fgtx.min(), fgtx.mean() 3.66951 -3.13315 1.44652
abs_mean, abs_mean+, abs_mean-: 11.5014 8.22944 17.3418
U_c = [[-0.0112675]] U_f = [[ 0.]] b_c = [ 0.70174557] b_f = [ 0.99655479]
W_c max, min, mean, abs_mean: 0.725722 -0.725964 0.359015 0.718418
W_f max, min, mean, abs_mean: 0.210772 -0.208692 0.0968048 0.193511

beijing_tanh+hardsigmoid5
beijing_tanh+hardsigmoid5 1528.0020      0.88  0.22  0.70      0.89  0.20  0.73      0.89  0.17  0.75
beijing_tanh+hardsigmoid5 6967.0094      0.85  0.11  0.76      0.85  0.09  0.79      0.88  0.06  0.83
forget mean min: 0.908718 0.25324
incx.max(), incx.min(), incx.mean() 8.04735 -7.36512 4.59776
fgtx.max(), fgtx.min(), fgtx.mean() 2.31498 -2.32676 1.27608
abs_mean, abs_mean+, abs_mean-: 10.1392 6.14132 22.273
U_c = [[-0.01017717]] U_f = [[ 0.]] b_c = [ 0.36066827] b_f = [ 1.09295869]
W_c max, min, mean, abs_mean: 0.390881 -0.391457 -0.15574 0.389639
W_f max, min, mean, abs_mean: 0.119136 -0.11914 -0.0470315 0.117347

beijing_tanh+hardsigmoid6
beijing_tanh+hardsigmoid6 1557.3817      0.89  0.24  0.70      0.90  0.22  0.72      0.90  0.19  0.75
beijing_tanh+hardsigmoid6 6929.4957      0.86  0.12  0.77      0.86  0.09  0.79      0.89  0.07  0.83
forget mean min: 0.91088 0.259964
incx.max(), incx.min(), incx.mean() 7.85241 -7.18515 4.67273
fgtx.max(), fgtx.min(), fgtx.mean() 2.27748 -2.28819 1.31207
abs_mean, abs_mean+, abs_mean-: 10.2604 6.21509 23.8095
U_c = [[-0.01464731]] U_f = [[ 0.]] b_c = [ 0.35126859] b_f = [ 1.08801162]
W_c max, min, mean, abs_mean: 0.381366 -0.381263 -0.0386402 0.379493
W_f max, min, mean, abs_mean: 0.116695 -0.11703 -0.0122949 0.115221

beijing_tanh+hardsigmoid7
beijing_tanh+hardsigmoid7 1521.8793      0.88  0.22  0.71      0.89  0.19  0.73      0.89  0.16  0.76
beijing_tanh+hardsigmoid7 6837.5710      0.85  0.12  0.76      0.85  0.09  0.79      0.87  0.06  0.83
forget mean min: 0.909148 0.25622
incx.max(), incx.min(), incx.mean() 8.08918 -7.39249 4.59009
fgtx.max(), fgtx.min(), fgtx.mean() 2.30257 -2.31134 1.25976
abs_mean, abs_mean+, abs_mean-: 10.0422 6.15436 21.2639
U_c = [[-0.01405468]] U_f = [[ 0.]] b_c = [ 0.36306381] b_f = [ 1.09243965]
W_c max, min, mean, abs_mean: 0.392387 -0.392767 -0.0779847 0.391649
W_f max, min, mean, abs_mean: 0.11692 -0.117858 -0.0233623 0.116721

beijing_tanh+hardsigmoid8
beijing_tanh+hardsigmoid8 1050.1207      0.90  0.18  0.75      0.92  0.15  0.79      0.92  0.12  0.82
beijing_tanh+hardsigmoid8 5372.0456      0.91  0.08  0.84      0.91  0.05  0.87      0.92  0.04  0.89
forget mean min: 0.903566 0.00898227
incx.max(), incx.min(), incx.mean() 14.1177 -11.4758 6.2023
fgtx.max(), fgtx.min(), fgtx.mean() 3.80325 -3.43911 1.56366
abs_mean, abs_mean+, abs_mean-: 11.7926 8.35807 18.3567
U_c = [[-0.01231514]] U_f = [[ 0.]] b_c = [ 0.67801321] b_f = [ 0.98401952]
W_c max, min, mean, abs_mean: 0.71647 -0.716321 -0.208548 0.703449
W_f max, min, mean, abs_mean: 0.208721 -0.22252 -0.065506 0.199089

beijing_tanh+hardsigmoid9
beijing_tanh+hardsigmoid9 1081.0661      0.91  0.19  0.75      0.92  0.16  0.79      0.92  0.13  0.81
beijing_tanh+hardsigmoid9 4947.7342      0.91  0.08  0.85      0.92  0.05  0.87      0.94  0.04  0.90
forget mean min: 0.898729 0.0626383
incx.max(), incx.min(), incx.mean() 14.3187 -10.7149 6.1204
fgtx.max(), fgtx.min(), fgtx.mean() 3.84299 -3.21045 1.53354
abs_mean, abs_mean+, abs_mean-: 11.8526 8.30794 19.4667
U_c = [[-0.01505604]] U_f = [[ 0.]] b_c = [ 0.6801793] b_f = [ 1.02363694]
W_c max, min, mean, abs_mean: 0.72753 -0.723124 0.00106407 0.717609
W_f max, min, mean, abs_mean: 0.209078 -0.212263 -0.00157195 0.202203
